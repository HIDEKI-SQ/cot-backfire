{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A3 Scaling Law Full Experiment: GPT Models (200 problems)\n",
    "\n",
    "**Purpose**: Measure Œª* (Backfire boundary) across GPT model family\n",
    "\n",
    "**Available Models**:\n",
    "| Model | API Name | Capability |\n",
    "|-------|----------|------------|\n",
    "| GPT-3.5 Turbo | gpt-3.5-turbo | Low |\n",
    "| GPT-4o-mini | gpt-4o-mini | Medium |\n",
    "| GPT-4o | gpt-4o | High |\n",
    "\n",
    "**Pilot Results (50 problems)**:\n",
    "- GPT-3.5: Baseline 46%, Œª* = 0.693\n",
    "- GPT-4o-mini: Baseline 44%, Œª* = 0.783\n",
    "- GPT-4o: Baseline 56.28%, Œª* = 0.865"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Google Drive Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "EXPERIMENT_DATE = datetime.now().strftime('%Y%m%d')\n",
    "SAVE_DIR = '/content/drive/MyDrive/CoT_Experiment'\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "print(f'Base directory: {SAVE_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets openai pandas tqdm matplotlib scipy -q\n",
    "print('Dependencies installed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ‚≠ê MODEL SELECTION ‚≠ê\n",
    "\n",
    "**„Åì„Åì„Åß„É¢„Éá„É´„ÇíÈÅ∏Êäû„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºÅ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Select GPT Model { run: \"auto\" }\n",
    "#@markdown **„É¢„Éá„É´„ÇíÈÅ∏Êäû„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºö**\n",
    "\n",
    "MODEL_CHOICE = \"GPT-3.5 Turbo\" #@param [\"GPT-3.5 Turbo\", \"GPT-4o-mini\", \"GPT-4o\"]\n",
    "\n",
    "# Model configuration mapping\n",
    "MODEL_CONFIG = {\n",
    "    \"GPT-3.5 Turbo\": {\n",
    "        \"api_name\": \"gpt-3.5-turbo\",\n",
    "        \"short_name\": \"gpt35\",\n",
    "        \"capability\": \"Low\",\n",
    "        \"cost_per_1k_input\": 0.0005,\n",
    "        \"cost_per_1k_output\": 0.0015\n",
    "    },\n",
    "    \"GPT-4o-mini\": {\n",
    "        \"api_name\": \"gpt-4o-mini\",\n",
    "        \"short_name\": \"gpt4omini\",\n",
    "        \"capability\": \"Medium\",\n",
    "        \"cost_per_1k_input\": 0.00015,\n",
    "        \"cost_per_1k_output\": 0.0006\n",
    "    },\n",
    "    \"GPT-4o\": {\n",
    "        \"api_name\": \"gpt-4o\",\n",
    "        \"short_name\": \"gpt4o\",\n",
    "        \"capability\": \"High\",\n",
    "        \"cost_per_1k_input\": 0.005,\n",
    "        \"cost_per_1k_output\": 0.015\n",
    "    }\n",
    "}\n",
    "\n",
    "# Set configuration\n",
    "config = MODEL_CONFIG[MODEL_CHOICE]\n",
    "API_MODEL = config[\"api_name\"]\n",
    "MODEL_SHORT = config[\"short_name\"]\n",
    "MODEL_CAPABILITY = config[\"capability\"]\n",
    "\n",
    "# Create experiment directory\n",
    "SAVE_DIR_EXP = f'{SAVE_DIR}/a3_gpt_{MODEL_SHORT}_{EXPERIMENT_DATE}'\n",
    "os.makedirs(SAVE_DIR_EXP, exist_ok=True)\n",
    "os.makedirs(f'{SAVE_DIR_EXP}/results', exist_ok=True)\n",
    "os.makedirs(f'{SAVE_DIR_EXP}/checkpoints', exist_ok=True)\n",
    "\n",
    "print('='*60)\n",
    "print(f'SELECTED MODEL: {MODEL_CHOICE}')\n",
    "print('='*60)\n",
    "print(f'  API Name: {API_MODEL}')\n",
    "print(f'  Capability: {MODEL_CAPABILITY}')\n",
    "print(f'  Save Directory: {SAVE_DIR_EXP}')\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Experiment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import hashlib\n",
    "from typing import List, Dict, Optional, Any\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# =============================================================================\n",
    "# Configuration - MUST MATCH ORIGINAL EXPERIMENTS\n",
    "# =============================================================================\n",
    "GLOBAL_SEED = 20251224  # Same as all other experiments\n",
    "N_PROBLEMS = 200  # Full experiment\n",
    "\n",
    "# Experimental conditions\n",
    "I_FIXED = 10  # Same trace depth\n",
    "LAMBDA_VALUES = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "\n",
    "# API settings\n",
    "API_MAX_TOKENS = 256\n",
    "API_RATE_LIMIT_DELAY = 0.3  # OpenAI is generally faster\n",
    "CHECKPOINT_EVERY = 50\n",
    "\n",
    "print('='*60)\n",
    "print('EXPERIMENT CONFIGURATION')\n",
    "print('='*60)\n",
    "print(f'  Model: {MODEL_CHOICE} ({API_MODEL})')\n",
    "print(f'  GLOBAL_SEED: {GLOBAL_SEED}')\n",
    "print(f'  N_PROBLEMS: {N_PROBLEMS}')\n",
    "print(f'  I (fixed): {I_FIXED}')\n",
    "print(f'  Œª values: {LAMBDA_VALUES}')\n",
    "print(f'  Total inferences: {N_PROBLEMS * (len(LAMBDA_VALUES) + 1)}')\n",
    "print('='*60)\n",
    "\n",
    "# Estimate cost\n",
    "est_tokens_per_call = 500\n",
    "total_calls = N_PROBLEMS * (len(LAMBDA_VALUES) + 1)\n",
    "est_cost = (total_calls * est_tokens_per_call / 1000) * (config['cost_per_1k_input'] + config['cost_per_1k_output'])\n",
    "print(f'\\nüí∞ Estimated cost: ${est_cost:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Structures & Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GSM8KProblem:\n",
    "    index: int\n",
    "    question: str\n",
    "    answer_text: str\n",
    "    final_answer: int\n",
    "\n",
    "def extract_final_answer(answer_text: str) -> int:\n",
    "    match = re.search(r'####\\s*([\\d,]+)', answer_text)\n",
    "    if match:\n",
    "        return int(match.group(1).replace(',', ''))\n",
    "    raise ValueError(f'Could not extract final answer')\n",
    "\n",
    "def save_json(data: Any, filepath: str):\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    print(f'Saved: {filepath}')\n",
    "\n",
    "def load_json(filepath: str) -> Any:\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def derive_seed(global_seed: int, problem_id: int, I: int, lam: float) -> int:\n",
    "    key = f\"{global_seed}|{problem_id}|I={I}|lam={lam}\"\n",
    "    h = hashlib.sha256(key.encode(\"utf-8\")).hexdigest()\n",
    "    return int(h[:8], 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load GSM8K and Select Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('gsm8k', 'main', split='test')\n",
    "print(f'GSM8K test set loaded: {len(dataset)} problems')\n",
    "\n",
    "def select_problems(dataset, n_problems: int, seed: int) -> List[int]:\n",
    "    rng = random.Random(seed)\n",
    "    indices = list(range(len(dataset)))\n",
    "    rng.shuffle(indices)\n",
    "    return sorted(indices[:n_problems])\n",
    "\n",
    "# Same seed ‚Üí Same problems as all other experiments\n",
    "selected_indices = select_problems(dataset, N_PROBLEMS, GLOBAL_SEED)\n",
    "print(f'Selected {len(selected_indices)} problems (identical across all experiments)')\n",
    "\n",
    "problems = []\n",
    "for idx in selected_indices:\n",
    "    item = dataset[idx]\n",
    "    try:\n",
    "        final_ans = extract_final_answer(item['answer'])\n",
    "        prob = GSM8KProblem(\n",
    "            index=idx,\n",
    "            question=item['question'],\n",
    "            answer_text=item['answer'],\n",
    "            final_answer=final_ans\n",
    "        )\n",
    "        problems.append(prob)\n",
    "    except ValueError as e:\n",
    "        print(f'Skipping problem {idx}: {e}')\n",
    "\n",
    "print(f'\\nLoaded {len(problems)} problems for experiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. OpenAI API Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import getpass\n",
    "\n",
    "# Get API key via input\n",
    "print(\"OpenAI API„Ç≠„Éº„ÇíÂÖ•Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºö\")\n",
    "OPENAI_API_KEY = getpass.getpass(\"API Key: \")\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "def call_openai(prompt: str, max_tokens: int = API_MAX_TOKENS) -> str:\n",
    "    \"\"\"Call OpenAI API with retry logic\"\"\"\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=API_MODEL,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=0  # Deterministic\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f'API error (attempt {attempt+1}): {e}')\n",
    "            time.sleep(2 ** attempt)\n",
    "    return \"\"\n",
    "\n",
    "# Test API\n",
    "print(f'\\nTesting {MODEL_CHOICE}...')\n",
    "test_response = call_openai(\"What is 2+2? Reply with just the number.\")\n",
    "print(f'API test: {test_response}')\n",
    "print(f'Model: {API_MODEL} ‚úì')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. CoT Generation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_clean_cot(problem: GSM8KProblem, I: int) -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate I clean reasoning steps from GSM8K answer.\n",
    "    \"\"\"\n",
    "    lines = problem.answer_text.split('\\n')\n",
    "    steps = [l.strip() for l in lines if l.strip() and not l.strip().startswith('####')]\n",
    "    \n",
    "    if len(steps) >= I:\n",
    "        return steps[:I]\n",
    "    else:\n",
    "        while len(steps) < I:\n",
    "            steps.append(f\"Step {len(steps)+1}: Continue calculation.\")\n",
    "        return steps\n",
    "\n",
    "def generate_corrupted_step(problem: GSM8KProblem, step_idx: int, rng: random.Random) -> str:\n",
    "    \"\"\"\n",
    "    Generate a corrupted reasoning step.\n",
    "    \"\"\"\n",
    "    corruption_templates = [\n",
    "        f\"Step {step_idx+1}: Let's multiply the values: {rng.randint(10, 100)} √ó {rng.randint(2, 10)} = {rng.randint(100, 1000)}.\",\n",
    "        f\"Step {step_idx+1}: Adding the totals: {rng.randint(50, 200)} + {rng.randint(50, 200)} = {rng.randint(100, 500)}.\",\n",
    "        f\"Step {step_idx+1}: The difference is: {rng.randint(100, 500)} - {rng.randint(20, 100)} = {rng.randint(50, 400)}.\",\n",
    "        f\"Step {step_idx+1}: Dividing gives us: {rng.randint(100, 1000)} √∑ {rng.randint(2, 10)} = {rng.randint(10, 200)}.\",\n",
    "        f\"Step {step_idx+1}: Converting: {rng.randint(1, 10)} √ó {rng.randint(10, 100)} = {rng.randint(10, 1000)}.\"\n",
    "    ]\n",
    "    return rng.choice(corruption_templates)\n",
    "\n",
    "def generate_mixed_cot(problem: GSM8KProblem, I: int, lam: float, seed: int) -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate CoT with Œª proportion of corrupted steps.\n",
    "    \"\"\"\n",
    "    rng = random.Random(seed)\n",
    "    clean_steps = generate_clean_cot(problem, I)\n",
    "    \n",
    "    n_corrupt = int(round(I * lam))\n",
    "    corrupt_indices = set(rng.sample(range(I), n_corrupt)) if n_corrupt > 0 else set()\n",
    "    \n",
    "    mixed_steps = []\n",
    "    for i in range(I):\n",
    "        if i in corrupt_indices:\n",
    "            mixed_steps.append(generate_corrupted_step(problem, i, rng))\n",
    "        else:\n",
    "            mixed_steps.append(clean_steps[i] if i < len(clean_steps) else f\"Step {i+1}: Continue.\")\n",
    "    \n",
    "    return mixed_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Prompt Creation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_direct_prompt(problem: GSM8KProblem) -> str:\n",
    "    return f\"\"\"Solve this math problem. Give ONLY the final numerical answer in JSON format.\n",
    "\n",
    "Problem: {problem.question}\n",
    "\n",
    "Reply with ONLY: {{\"final\": <number>}}\"\"\"\n",
    "\n",
    "def create_cot_prompt(problem: GSM8KProblem, cot_steps: List[str]) -> str:\n",
    "    steps_text = '\\n'.join(cot_steps)\n",
    "    return f\"\"\"Here is a math problem with a provided reasoning trace.\n",
    "Follow the reasoning and give the final numerical answer in JSON format.\n",
    "\n",
    "Problem: {problem.question}\n",
    "\n",
    "Reasoning trace:\n",
    "{steps_text}\n",
    "\n",
    "Based on this reasoning, what is the final answer?\n",
    "Reply with ONLY: {{\"final\": <number>}}\"\"\"\n",
    "\n",
    "def parse_answer(response: str) -> Optional[int]:\n",
    "    \"\"\"Extract numerical answer from model response\"\"\"\n",
    "    # Try JSON format\n",
    "    try:\n",
    "        match = re.search(r'\\{[^}]*\"final\"\\s*:\\s*(\\d+)[^}]*\\}', response)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Try plain number\n",
    "    try:\n",
    "        numbers = re.findall(r'\\b(\\d+)\\b', response)\n",
    "        if numbers:\n",
    "            return int(numbers[-1])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Run Direct Condition (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*60)\n",
    "print(f'PHASE 1: Direct Condition (Baseline) - {MODEL_CHOICE}')\n",
    "print('='*60)\n",
    "\n",
    "direct_results = []\n",
    "\n",
    "for i, problem in enumerate(tqdm(problems, desc='Direct')):\n",
    "    prompt = create_direct_prompt(problem)\n",
    "    response = call_openai(prompt)\n",
    "    \n",
    "    answer = parse_answer(response)\n",
    "    is_correct = (answer == problem.final_answer) if answer is not None else False\n",
    "    \n",
    "    result = {\n",
    "        'problem_index': problem.index,\n",
    "        'condition': 'direct',\n",
    "        'model': API_MODEL,\n",
    "        'model_answer': answer,\n",
    "        'correct_answer': problem.final_answer,\n",
    "        'is_correct': is_correct,\n",
    "        'raw_output': response,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    direct_results.append(result)\n",
    "    \n",
    "    time.sleep(API_RATE_LIMIT_DELAY)\n",
    "    \n",
    "    if (i + 1) % CHECKPOINT_EVERY == 0:\n",
    "        save_json(direct_results, f'{SAVE_DIR_EXP}/checkpoints/direct_checkpoint_{i+1}.json')\n",
    "\n",
    "# Save final direct results\n",
    "save_json(direct_results, f'{SAVE_DIR_EXP}/results/direct_results_{MODEL_SHORT}.json')\n",
    "\n",
    "# Calculate baseline accuracy\n",
    "baseline_acc = sum(r['is_correct'] for r in direct_results) / len(direct_results)\n",
    "print(f'\\n‚úì {MODEL_CHOICE} Direct (Baseline) Accuracy: {baseline_acc:.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Run CoT Conditions (Œª sweep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*60)\n",
    "print(f'PHASE 2: CoT Conditions (Œª sweep) - {MODEL_CHOICE}')\n",
    "print('='*60)\n",
    "\n",
    "cot_results = []\n",
    "total_trials = len(problems) * len(LAMBDA_VALUES)\n",
    "\n",
    "with tqdm(total=total_trials, desc='CoT sweep') as pbar:\n",
    "    for lam in LAMBDA_VALUES:\n",
    "        print(f'\\n--- Œª = {lam} ---')\n",
    "        \n",
    "        for problem in problems:\n",
    "            # Generate mixed CoT\n",
    "            seed = derive_seed(GLOBAL_SEED, problem.index, I_FIXED, lam)\n",
    "            cot_steps = generate_mixed_cot(problem, I_FIXED, lam, seed)\n",
    "            \n",
    "            # Query model\n",
    "            prompt = create_cot_prompt(problem, cot_steps)\n",
    "            response = call_openai(prompt)\n",
    "            \n",
    "            answer = parse_answer(response)\n",
    "            is_correct = (answer == problem.final_answer) if answer is not None else False\n",
    "            \n",
    "            result = {\n",
    "                'problem_index': problem.index,\n",
    "                'condition': 'cot',\n",
    "                'model': API_MODEL,\n",
    "                'I': I_FIXED,\n",
    "                'lam': lam,\n",
    "                'A_target': 1 - lam,\n",
    "                'model_answer': answer,\n",
    "                'correct_answer': problem.final_answer,\n",
    "                'is_correct': is_correct,\n",
    "                'raw_output': response,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "            cot_results.append(result)\n",
    "            \n",
    "            time.sleep(API_RATE_LIMIT_DELAY)\n",
    "            pbar.update(1)\n",
    "        \n",
    "        # Checkpoint after each Œª\n",
    "        save_json(cot_results, f'{SAVE_DIR_EXP}/checkpoints/cot_checkpoint_lam{lam}.json')\n",
    "\n",
    "# Save final CoT results\n",
    "save_json(cot_results, f'{SAVE_DIR_EXP}/results/cot_results_{MODEL_SHORT}.json')\n",
    "print('\\n‚úì CoT experiment complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert to DataFrame\n",
    "cot_df = pd.DataFrame(cot_results)\n",
    "\n",
    "# Calculate accuracy by Œª\n",
    "acc_by_lam = cot_df.groupby('lam')['is_correct'].mean().to_dict()\n",
    "\n",
    "print('='*60)\n",
    "print(f'{MODEL_CHOICE} RESULTS')\n",
    "print('='*60)\n",
    "print(f'\\nDirect (Baseline): {baseline_acc:.1%}')\n",
    "print('\\nCoT accuracy by Œª:')\n",
    "for lam, acc in sorted(acc_by_lam.items()):\n",
    "    marker = '‚Üê BACKFIRE' if acc < baseline_acc else ''\n",
    "    print(f'  Œª={lam:.1f}: {acc:.1%} {marker}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Estimate Œª*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "from scipy.optimize import brentq\n",
    "\n",
    "lam_points = np.array(sorted(acc_by_lam.keys()))\n",
    "acc_points = np.array([acc_by_lam[l] for l in lam_points])\n",
    "\n",
    "def estimate_lambda_crit(lam_arr, acc_arr, baseline):\n",
    "    \"\"\"Find Œª where CoT accuracy crosses baseline\"\"\"\n",
    "    f = interp1d(lam_arr, acc_arr - baseline, kind='linear', fill_value='extrapolate')\n",
    "    try:\n",
    "        for i in range(len(lam_arr) - 1):\n",
    "            if (acc_arr[i] - baseline) * (acc_arr[i+1] - baseline) < 0:\n",
    "                return brentq(f, lam_arr[i], lam_arr[i+1])\n",
    "    except:\n",
    "        pass\n",
    "    if acc_arr[-1] > baseline:\n",
    "        return 1.0\n",
    "    return None\n",
    "\n",
    "lambda_crit = estimate_lambda_crit(lam_points, acc_points, baseline_acc)\n",
    "\n",
    "print('='*60)\n",
    "print(f'Œª* ESTIMATION FOR {MODEL_CHOICE}')\n",
    "print('='*60)\n",
    "\n",
    "if lambda_crit is not None:\n",
    "    print(f'\\n  Œª* = {lambda_crit:.3f}')\n",
    "    print(f'  A* = {1 - lambda_crit:.3f}')\n",
    "else:\n",
    "    print('\\n  Could not estimate Œª* (no crossing detected)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot accuracy curve\n",
    "lams = list(acc_by_lam.keys())\n",
    "accs = [acc_by_lam[l] * 100 for l in lams]\n",
    "ax.plot(lams, accs, 'o-', color='#d62728', linewidth=2.5, markersize=10, label=f'{MODEL_CHOICE}')\n",
    "\n",
    "# Baseline\n",
    "ax.axhline(y=baseline_acc * 100, color='blue', linestyle='--', linewidth=2, label=f'Baseline ({baseline_acc:.1%})')\n",
    "\n",
    "# Mark Œª*\n",
    "if lambda_crit is not None and lambda_crit < 1.0:\n",
    "    ax.axvline(x=lambda_crit, color='green', linestyle=':', linewidth=2, alpha=0.7)\n",
    "    ax.annotate(f'Œª*={lambda_crit:.3f}', xy=(lambda_crit, baseline_acc*100), \n",
    "                xytext=(lambda_crit+0.1, baseline_acc*100+5),\n",
    "                fontsize=12, color='green',\n",
    "                arrowprops=dict(arrowstyle='->', color='green'))\n",
    "\n",
    "ax.set_xlabel('Corruption Rate (Œª)', fontsize=13)\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=13)\n",
    "ax.set_title(f'CoT Collapse Curve: {MODEL_CHOICE}', fontsize=14)\n",
    "ax.set_xlim(-0.05, 1.05)\n",
    "ax.set_ylim(20, 105)\n",
    "ax.legend(loc='lower left', fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{SAVE_DIR_EXP}/collapse_curve_{MODEL_SHORT}.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f'Saved: {SAVE_DIR_EXP}/collapse_curve_{MODEL_SHORT}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Save Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {\n",
    "    'experiment': 'A3_Scaling_Law_GPT',\n",
    "    'date': EXPERIMENT_DATE,\n",
    "    'model': {\n",
    "        'name': MODEL_CHOICE,\n",
    "        'api_name': API_MODEL,\n",
    "        'capability': MODEL_CAPABILITY\n",
    "    },\n",
    "    'n_problems': len(problems),\n",
    "    'baseline_accuracy': baseline_acc,\n",
    "    'accuracy_by_lambda': {str(k): v for k, v in acc_by_lam.items()},\n",
    "    'lambda_crit': lambda_crit,\n",
    "    'a_crit': 1 - lambda_crit if lambda_crit else None\n",
    "}\n",
    "\n",
    "save_json(summary, f'{SAVE_DIR_EXP}/results/summary_{MODEL_SHORT}.json')\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print(f'EXPERIMENT COMPLETE: {MODEL_CHOICE}')\n",
    "print('='*60)\n",
    "print(f'''\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ {MODEL_CHOICE:^38} ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ Baseline Accuracy: {baseline_acc:>17.1%} ‚îÇ\n",
    "‚îÇ Œª* (Backfire):     {lambda_crit if lambda_crit else \"N/A\":>17} ‚îÇ\n",
    "‚îÇ A* (Critical):     {1-lambda_crit if lambda_crit else \"N/A\":>17} ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "''')\n",
    "print(f'Results saved to: {SAVE_DIR_EXP}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Compare with All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All results so far (update as experiments complete)\n",
    "all_results = {\n",
    "    # Claude family\n",
    "    'Claude 3 Haiku': {'baseline': 0.322, 'lambda_crit': 0.826},\n",
    "    'Claude 3.5 Haiku': {'baseline': 0.467, 'lambda_crit': 1.0},\n",
    "    'Claude 4 Sonnet': {'baseline': 0.925, 'lambda_crit': None},  # TBD\n",
    "    # GPT family\n",
    "    'GPT-3.5': {'baseline': 0.46, 'lambda_crit': 0.693},  # Pilot\n",
    "    'GPT-4o-mini': {'baseline': 0.44, 'lambda_crit': 0.783},  # Pilot\n",
    "    'GPT-4o': {'baseline': 0.563, 'lambda_crit': 0.865},\n",
    "}\n",
    "\n",
    "# Add current result\n",
    "all_results[MODEL_CHOICE] = {'baseline': baseline_acc, 'lambda_crit': lambda_crit}\n",
    "\n",
    "print('='*60)\n",
    "print('ALL MODELS COMPARISON')\n",
    "print('='*60)\n",
    "print('\\n| Model | Baseline | Œª* |')\n",
    "print('|-------|----------|-----|')\n",
    "for model, data in sorted(all_results.items(), key=lambda x: x[1]['baseline'] or 0):\n",
    "    b = f\"{data['baseline']:.1%}\" if data['baseline'] else \"TBD\"\n",
    "    l = f\"{data['lambda_crit']:.3f}\" if data['lambda_crit'] else \"TBD\"\n",
    "    print(f'| {model} | {b} | {l} |')"
   ]
  }
 ]
}
