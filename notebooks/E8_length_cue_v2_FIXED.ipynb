{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E8: Length × Cue × Corruption Experiment (FIXED VERSION)\n",
    "\n",
    "**Paper**: A2 (Cue-Dominant Extraction Explains Length Effects)\n",
    "\n",
    "**FIXES**:\n",
    "1. Added L=15 (now L ∈ {5, 10, 15, 20})\n",
    "2. Fixed cue removal - replaces ENTIRE final step\n",
    "\n",
    "**Purpose**: Validate that length effect is CONDITIONAL on cue presence.\n",
    "\n",
    "**Predictions**:\n",
    "| Condition | L=5 | L=10 | L=15 | L=20 | Pattern |\n",
    "|-----------|-----|------|------|------|---------|\n",
    "| Cue-present | High | High | High | High | Flat (cue dominates) |\n",
    "| Cue-absent | Low | Med | Med+ | High | Increasing (redundancy) |\n",
    "\n",
    "**Expected inferences**: 188 × 4 × 2 × 2 = ~3,008\n",
    "\n",
    "**Date**: 2026-01-03\n",
    "**VERSION**: 2.0 (FIXED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Google Drive Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "EXPERIMENT_NAME = 'E8_length_cue_v2'\n",
    "EXPERIMENT_DATE = datetime.now().strftime('%Y%m%d')\n",
    "\n",
    "BASE_DIR = '/content/drive/MyDrive/CoT_Experiment'\n",
    "V3_DATA_DIR = f'{BASE_DIR}/full_experiment_v3_20251224'\n",
    "TRACES_DIR = f'{V3_DATA_DIR}/clean_traces'\n",
    "\n",
    "SAVE_DIR = f'{BASE_DIR}/{EXPERIMENT_NAME}_{EXPERIMENT_DATE}'\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(f'{SAVE_DIR}/results', exist_ok=True)\n",
    "\n",
    "print(f'Experiment: {EXPERIMENT_NAME}')\n",
    "print(f'Save directory: {SAVE_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets anthropic matplotlib pandas tqdm scipy -q\n",
    "print('Dependencies installed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import random\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from typing import List, Dict, Tuple, Optional, Any, Set\n",
    "from dataclasses import dataclass, asdict, field\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# =============================================================================\n",
    "# Global Configuration\n",
    "# =============================================================================\n",
    "GLOBAL_SEED = 20251224\n",
    "E8_SEED = 20260103\n",
    "\n",
    "# UPDATED: Now includes L=15\n",
    "LENGTHS = [5, 10, 15, 20]\n",
    "CORRUPTION_LEVELS = [0.4, 0.8]\n",
    "CUE_CONDITIONS = ['present', 'absent']\n",
    "\n",
    "CORRUPTION_RATIO = {'IRR': 1, 'LOC': 2, 'WRONG': 2}\n",
    "\n",
    "API_MAX_TOKENS_ANSWER = 256\n",
    "API_RETRY_DELAY = 1.0\n",
    "API_RATE_LIMIT_DELAY = 0.5\n",
    "\n",
    "print('='*70)\n",
    "print('E8: LENGTH × CUE × CORRUPTION (FIXED VERSION)')\n",
    "print('='*70)\n",
    "print(f'  Lengths: {LENGTHS} (now includes L=15)')\n",
    "print(f'  Corruption: {CORRUPTION_LEVELS}')\n",
    "print(f'  Cue: {CUE_CONDITIONS}')\n",
    "print(f'  Total conditions: {len(LENGTHS) * len(CORRUPTION_LEVELS) * len(CUE_CONDITIONS)}')\n",
    "print('='*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GSM8KProblem:\n",
    "    index: int\n",
    "    question: str\n",
    "    answer_text: str\n",
    "    final_answer: int\n",
    "\n",
    "@dataclass\n",
    "class CleanTrace:\n",
    "    problem_index: int\n",
    "    I: int\n",
    "    steps: List[str]\n",
    "    full_text: str\n",
    "\n",
    "@dataclass\n",
    "class CorruptedTrace:\n",
    "    problem_index: int\n",
    "    L: int\n",
    "    c: float\n",
    "    cue_condition: str\n",
    "    K_corrupt: int\n",
    "    corrupted_steps: List[int]\n",
    "    corruption_types: Dict[int, str]\n",
    "    steps: List[str]\n",
    "    full_text: str\n",
    "    seed: int\n",
    "\n",
    "@dataclass\n",
    "class ExperimentResult:\n",
    "    problem_index: int\n",
    "    L: int\n",
    "    c: float\n",
    "    cue_condition: str\n",
    "    K_corrupt: int\n",
    "    K_clean: int\n",
    "    model_answer: Optional[int]\n",
    "    correct_answer: int\n",
    "    is_correct: bool\n",
    "    raw_output: str\n",
    "    timestamp: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derive_seed(global_seed: int, problem_id: int, L: int, c: float, cue: str) -> int:\n",
    "    key = f\"{global_seed}|E8|{problem_id}|L={L}|c={c}|cue={cue}\"\n",
    "    h = hashlib.sha256(key.encode(\"utf-8\")).hexdigest()\n",
    "    return int(h[:8], 16)\n",
    "\n",
    "def save_json(data: Any, filepath: str):\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def load_json(filepath: str) -> Any:\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load problems\n",
    "problems_path = f'{V3_DATA_DIR}/problems_v3.json'\n",
    "problems_data = load_json(problems_path)\n",
    "problems = [GSM8KProblem(**p) for p in problems_data]\n",
    "prob_map = {p.index: p for p in problems}\n",
    "print(f'Loaded {len(problems)} problems')\n",
    "\n",
    "# Load traces for each L\n",
    "trace_maps = {}\n",
    "\n",
    "for L in LENGTHS:\n",
    "    trace_path = f'{TRACES_DIR}/clean_traces_I{L}_v3.json'\n",
    "    if os.path.exists(trace_path):\n",
    "        traces_data = load_json(trace_path)\n",
    "        traces = [CleanTrace(**t) for t in traces_data]\n",
    "        trace_maps[L] = {t.problem_index: t for t in traces}\n",
    "        print(f'Loaded {len(traces)} traces for L={L}')\n",
    "    else:\n",
    "        print(f'ERROR: Traces for L={L} not found!')\n",
    "        trace_maps[L] = {}\n",
    "\n",
    "# Find common problems\n",
    "common_indices = set.intersection(*[set(tm.keys()) for tm in trace_maps.values()])\n",
    "print(f'\\nCommon problems across L={LENGTHS}: {len(common_indices)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_problems = [p for p in problems if p.index in common_indices]\n",
    "print(f'Using {len(experiment_problems)} problems')\n",
    "print(f'Total inferences: {len(experiment_problems) * len(LENGTHS) * len(CORRUPTION_LEVELS) * len(CUE_CONDITIONS)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Corruption Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_corrupted_steps(L: int, c: float, seed: int) -> List[int]:\n",
    "    \"\"\"Select steps to corrupt (excluding final step)\"\"\"\n",
    "    K = int(round(c * (L - 1)))  # Exclude last step\n",
    "    if K == 0:\n",
    "        return []\n",
    "    steps = list(range(1, L))  # Steps 1 to L-1\n",
    "    rng = random.Random(seed)\n",
    "    rng.shuffle(steps)\n",
    "    return sorted(steps[:K])\n",
    "\n",
    "def assign_corruption_types(corrupted_steps: List[int], seed: int) -> Dict[int, str]:\n",
    "    K = len(corrupted_steps)\n",
    "    if K == 0:\n",
    "        return {}\n",
    "    n_irr = (K * 1) // 5\n",
    "    n_loc = (K * 2) // 5\n",
    "    n_wrong = K - n_irr - n_loc\n",
    "    if n_wrong == 0 and K > 0:\n",
    "        n_wrong = 1\n",
    "        if n_loc > 0:\n",
    "            n_loc -= 1\n",
    "        elif n_irr > 0:\n",
    "            n_irr -= 1\n",
    "    rng = random.Random(seed + 1)\n",
    "    perm = corrupted_steps[:]\n",
    "    rng.shuffle(perm)\n",
    "    type_map = {}\n",
    "    for s in perm[:n_irr]:\n",
    "        type_map[s] = \"IRR\"\n",
    "    for s in perm[n_irr:n_irr + n_loc]:\n",
    "        type_map[s] = \"LOC\"\n",
    "    for s in perm[n_irr + n_loc:]:\n",
    "        type_map[s] = \"WRONG\"\n",
    "    return type_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Corruption Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IRRELEVANT_TEMPLATES = [\n",
    "    \"Compute an auxiliary value: aux = {a} + {b} = {result}, but it will not be used later.\",\n",
    "    \"Compute a side quantity: aux = {a} * 2 = {result}, unrelated to the final result.\",\n",
    "    \"Note that we can also compute aux = {a} - {b} = {result}, though this is not needed.\",\n",
    "]\n",
    "\n",
    "WRONG_CONSTRAINT_TEMPLATES = [\n",
    "    \"Fix an intermediate condition: set {var} = {wrong_value} as a given constraint for the rest of the steps.\",\n",
    "    \"Assume the total is {var} = {wrong_value} and proceed using this fixed value.\",\n",
    "]\n",
    "\n",
    "def generate_irrelevant_step(step_num: int, seed: int) -> str:\n",
    "    rng = random.Random(seed)\n",
    "    a = rng.randint(2, 20)\n",
    "    b = rng.randint(2, 20)\n",
    "    template = rng.choice(IRRELEVANT_TEMPLATES)\n",
    "    if '+' in template:\n",
    "        result = a + b\n",
    "    elif '*' in template:\n",
    "        result = a * 2\n",
    "    else:\n",
    "        result = a - b\n",
    "    return template.format(a=a, b=b, result=result)\n",
    "\n",
    "def generate_local_error_step(original_step: str, seed: int) -> str:\n",
    "    rng = random.Random(seed)\n",
    "    numbers = re.findall(r'\\d+', original_step)\n",
    "    if not numbers:\n",
    "        return f\"Compute t = 10 * 3 = {rng.randint(28, 32)} (using the previous values).\"\n",
    "    original_result = int(numbers[-1])\n",
    "    offset = rng.choice([-3, -2, -1, 1, 2, 3])\n",
    "    wrong_result = max(0, original_result + offset)\n",
    "    modified = re.sub(r'= (\\d+)\\.$', f'= {wrong_result}.', original_step)\n",
    "    if modified == original_step:\n",
    "        modified = re.sub(r'(\\d+)\\.$', f'{wrong_result}.', original_step)\n",
    "    return modified\n",
    "\n",
    "def generate_wrong_constraint_step(step_num: int, seed: int) -> str:\n",
    "    rng = random.Random(seed)\n",
    "    var = rng.choice(['x', 'total', 'result', 'n'])\n",
    "    wrong_value = rng.randint(10, 100)\n",
    "    template = rng.choice(WRONG_CONSTRAINT_TEMPLATES)\n",
    "    return template.format(var=var, wrong_value=wrong_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. FIXED: Final Step Manipulation\n",
    "\n",
    "**BUG IN PREVIOUS VERSION**: Partial replacement left residual expressions.\n",
    "\n",
    "**FIX**: Replace the ENTIRE final step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_final_step_with_cue(answer: int) -> str:\n",
    "    \"\"\"\n",
    "    FIXED: Create a clean final step WITH the cue.\n",
    "    Replaces the ENTIRE step to avoid residual expressions.\n",
    "    \"\"\"\n",
    "    return f\"Therefore, the final answer is Final = {answer}.\"\n",
    "\n",
    "def create_final_step_without_cue() -> str:\n",
    "    \"\"\"\n",
    "    FIXED: Create a clean final step WITHOUT the cue.\n",
    "    Replaces the ENTIRE step to avoid residual expressions.\n",
    "    \"\"\"\n",
    "    return \"The reasoning steps above lead to the solution. The calculation is now complete.\"\n",
    "\n",
    "# Verify\n",
    "print(\"=== FIXED FINAL STEP TEMPLATES ===\")\n",
    "print(f\"Cue present: {create_final_step_with_cue(70000)}\")\n",
    "print(f\"Cue absent: {create_final_step_without_cue()}\")\n",
    "print(\"\\nNo residual expressions like '- 130000' will remain.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Trace Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corrupted_trace(\n",
    "    clean_trace: CleanTrace,\n",
    "    correct_answer: int,\n",
    "    c: float,\n",
    "    cue_condition: str,\n",
    "    seed: int\n",
    ") -> CorruptedTrace:\n",
    "    \"\"\"\n",
    "    Create a corrupted trace with specified corruption level and cue condition.\n",
    "    FIXED: Uses complete final step replacement.\n",
    "    \"\"\"\n",
    "    L = clean_trace.I\n",
    "    \n",
    "    # Pick steps to corrupt (excluding final step)\n",
    "    corrupted_steps = pick_corrupted_steps(L, c, seed)\n",
    "    corruption_types = assign_corruption_types(corrupted_steps, seed)\n",
    "    K_corrupt = len(corrupted_steps)\n",
    "    \n",
    "    # Apply corruption to steps 1 to L-1\n",
    "    new_steps = []\n",
    "    for i, step_content in enumerate(clean_trace.steps[:-1]):\n",
    "        step_num = i + 1\n",
    "        \n",
    "        if step_num in corrupted_steps:\n",
    "            ctype = corruption_types[step_num]\n",
    "            step_seed = seed + step_num * 1000\n",
    "            \n",
    "            if ctype == 'IRR':\n",
    "                new_content = generate_irrelevant_step(step_num, step_seed)\n",
    "            elif ctype == 'LOC':\n",
    "                new_content = generate_local_error_step(step_content, step_seed)\n",
    "            else:\n",
    "                new_content = generate_wrong_constraint_step(step_num, step_seed)\n",
    "            new_steps.append(new_content)\n",
    "        else:\n",
    "            new_steps.append(step_content)\n",
    "    \n",
    "    # FIXED: Handle final step with complete replacement\n",
    "    if cue_condition == 'present':\n",
    "        final_step = create_final_step_with_cue(correct_answer)\n",
    "    else:\n",
    "        final_step = create_final_step_without_cue()\n",
    "    new_steps.append(final_step)\n",
    "    \n",
    "    # Build full text\n",
    "    lines = ['[[COT_START]]']\n",
    "    for i, content in enumerate(new_steps):\n",
    "        lines.append(f'Step {i+1}: {content}')\n",
    "    lines.append('[[COT_END]]')\n",
    "    full_text = '\\n'.join(lines)\n",
    "    \n",
    "    return CorruptedTrace(\n",
    "        problem_index=clean_trace.problem_index,\n",
    "        L=L,\n",
    "        c=c,\n",
    "        cue_condition=cue_condition,\n",
    "        K_corrupt=K_corrupt,\n",
    "        corrupted_steps=corrupted_steps,\n",
    "        corruption_types=corruption_types,\n",
    "        steps=new_steps,\n",
    "        full_text=full_text,\n",
    "        seed=seed\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. API Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "ANTHROPIC_API_KEY = getpass('Enter Anthropic API Key: ')\n",
    "print('API Key set.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "\n",
    "client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n",
    "\n",
    "def call_claude(system_prompt: str, user_prompt: str, max_tokens: int = 1024, retries: int = 3) -> str:\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            message = client.messages.create(\n",
    "                model=\"claude-sonnet-4-20250514\",\n",
    "                max_tokens=max_tokens,\n",
    "                messages=[{\"role\": \"user\", \"content\": user_prompt}],\n",
    "                system=system_prompt,\n",
    "                temperature=0\n",
    "            )\n",
    "            time.sleep(API_RATE_LIMIT_DELAY)\n",
    "            return message.content[0].text\n",
    "        except Exception as e:\n",
    "            print(f'API error (attempt {attempt+1}): {e}')\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(API_RETRY_DELAY * (attempt + 1))\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "test_response = call_claude(\n",
    "    \"You output ONLY JSON.\",\n",
    "    'Respond with exactly: {\"test\": \"ok\"}',\n",
    "    max_tokens=50\n",
    ")\n",
    "print(f'API test: {test_response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Experiment Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_SYSTEM_PROMPT = \"\"\"You are a calculator that outputs ONLY JSON.\n",
    "\n",
    "CRITICAL RULES:\n",
    "1. Your output MUST start with the character '{'\n",
    "2. Your output MUST be exactly: {\"final\": <number>}\n",
    "3. Replace <number> with an integer (the numerical answer)\n",
    "4. Do NOT write ANY explanation, reasoning, or text before or after the JSON\n",
    "5. Do NOT write \"I need to\" or \"Let me\" or any other words\n",
    "6. ONLY output the JSON object, nothing else\n",
    "\n",
    "CORRECT OUTPUT EXAMPLE:\n",
    "{\"final\": 42}\n",
    "\"\"\"\n",
    "\n",
    "def create_experiment_prompt(problem: GSM8KProblem, cot_text: str) -> Tuple[str, str]:\n",
    "    user = f\"\"\"Problem: {problem.question}\n",
    "\n",
    "Reasoning trace (use these steps as given facts):\n",
    "{cot_text}\n",
    "\n",
    "Based on the trace above, compute the final numerical answer.\n",
    "OUTPUT ONLY: {{\"final\": <number>}}\n",
    "START YOUR RESPONSE WITH '{{'\"\"\"\n",
    "    return EXPERIMENT_SYSTEM_PROMPT, user\n",
    "\n",
    "def parse_model_answer(response: str) -> Optional[int]:\n",
    "    match = re.search(r'\\{\\s*\"final\"\\s*:\\s*(-?\\d+(?:\\.\\d+)?)\\s*\\}', response)\n",
    "    if match:\n",
    "        return int(round(float(match.group(1))))\n",
    "    match = re.search(r\"\\{\\s*[\\\"']final[\\\"']\\s*:\\s*(-?\\d+(?:\\.\\d+)?)\\s*\\}\", response)\n",
    "    if match:\n",
    "        return int(round(float(match.group(1))))\n",
    "    match = re.search(r'\"final\"\\s*:\\s*(-?\\d+(?:\\.\\d+)?)', response)\n",
    "    if match:\n",
    "        return int(round(float(match.group(1))))\n",
    "    matches = re.findall(r'(?:^|\\s)(-?\\d+(?:\\.\\d+)?)(?:\\s|$|\\.|,)', response)\n",
    "    if matches:\n",
    "        return int(round(float(matches[-1])))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_experiment(\n",
    "    problem: GSM8KProblem,\n",
    "    trace: CorruptedTrace\n",
    ") -> ExperimentResult:\n",
    "    sys_prompt, usr_prompt = create_experiment_prompt(problem, trace.full_text)\n",
    "    response = call_claude(sys_prompt, usr_prompt, max_tokens=API_MAX_TOKENS_ANSWER)\n",
    "    \n",
    "    model_answer = parse_model_answer(response)\n",
    "    is_correct = (model_answer == problem.final_answer) if model_answer is not None else False\n",
    "    \n",
    "    return ExperimentResult(\n",
    "        problem_index=problem.index,\n",
    "        L=trace.L,\n",
    "        c=trace.c,\n",
    "        cue_condition=trace.cue_condition,\n",
    "        K_corrupt=trace.K_corrupt,\n",
    "        K_clean=trace.L - trace.K_corrupt,\n",
    "        model_answer=model_answer,\n",
    "        correct_answer=problem.final_answer,\n",
    "        is_correct=is_correct,\n",
    "        raw_output=response,\n",
    "        timestamp=datetime.now().isoformat()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*70)\n",
    "print('E8: LENGTH × CUE × CORRUPTION (FIXED)')\n",
    "print('='*70)\n",
    "\n",
    "total_conditions = len(LENGTHS) * len(CORRUPTION_LEVELS) * len(CUE_CONDITIONS)\n",
    "total_inferences = len(experiment_problems) * total_conditions\n",
    "print(f'Problems: {len(experiment_problems)}')\n",
    "print(f'Conditions: {total_conditions}')\n",
    "print(f'Total inferences: {total_inferences}')\n",
    "print('='*70)\n",
    "\n",
    "all_results = []\n",
    "traces_log = []\n",
    "\n",
    "for L in LENGTHS:\n",
    "    for c in CORRUPTION_LEVELS:\n",
    "        for cue in CUE_CONDITIONS:\n",
    "            condition_name = f'L={L}_c={c}_cue={cue}'\n",
    "            print(f'\\nRunning: {condition_name}')\n",
    "            \n",
    "            condition_results = []\n",
    "            \n",
    "            for prob in tqdm(experiment_problems, desc=condition_name):\n",
    "                clean_trace = trace_maps[L].get(prob.index)\n",
    "                if clean_trace is None:\n",
    "                    continue\n",
    "                \n",
    "                seed = derive_seed(E8_SEED, prob.index, L, c, cue)\n",
    "                corrupted_trace = create_corrupted_trace(clean_trace, prob.final_answer, c, cue, seed)\n",
    "                result = run_single_experiment(prob, corrupted_trace)\n",
    "                \n",
    "                condition_results.append(result)\n",
    "                all_results.append(result)\n",
    "                \n",
    "                if len(condition_results) <= 3:\n",
    "                    traces_log.append({\n",
    "                        'condition': condition_name,\n",
    "                        'trace': asdict(corrupted_trace)\n",
    "                    })\n",
    "            \n",
    "            acc = sum(r.is_correct for r in condition_results) / len(condition_results) if condition_results else 0\n",
    "            print(f'  → Accuracy: {acc*100:.1f}% ({sum(r.is_correct for r in condition_results)}/{len(condition_results)})')\n",
    "\n",
    "print(f'\\n\\nTotal experiments completed: {len(all_results)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json([asdict(r) for r in all_results], f'{SAVE_DIR}/results/E8_length_cue_results.json')\n",
    "print(f'Results saved: {SAVE_DIR}/results/E8_length_cue_results.json')\n",
    "\n",
    "save_json(traces_log, f'{SAVE_DIR}/results/E8_traces_sample.json')\n",
    "print(f'Trace samples saved: {SAVE_DIR}/results/E8_traces_sample.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([asdict(r) for r in all_results])\n",
    "\n",
    "# Pivot table\n",
    "pivot = df.pivot_table(\n",
    "    values='is_correct',\n",
    "    index=['c', 'cue_condition'],\n",
    "    columns='L',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "\n",
    "print('='*70)\n",
    "print('E8 RESULTS: ACCURACY BY CONDITION')\n",
    "print('='*70)\n",
    "print((pivot * 100).round(1))\n",
    "print('='*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed table with L effect\n",
    "print('\\n' + '='*80)\n",
    "print('DETAILED RESULTS')\n",
    "print('='*80)\n",
    "print(f'{\"c\":>5} {\"Cue\":>10} {\"L=5\":>8} {\"L=10\":>8} {\"L=15\":>8} {\"L=20\":>8} {\"L Effect (20-5)\":>15}')\n",
    "print('-'*80)\n",
    "\n",
    "for c in CORRUPTION_LEVELS:\n",
    "    for cue in CUE_CONDITIONS:\n",
    "        accs = {}\n",
    "        for L in LENGTHS:\n",
    "            mask = (df['c'] == c) & (df['cue_condition'] == cue) & (df['L'] == L)\n",
    "            accs[L] = df[mask]['is_correct'].mean()\n",
    "        \n",
    "        l_effect = accs[20] - accs[5]\n",
    "        effect_str = f'{l_effect*100:+.1f}pp'\n",
    "        \n",
    "        print(f'{c:>5.1f} {cue:>10} {accs[5]*100:>7.1f}% {accs[10]*100:>7.1f}% {accs[15]*100:>7.1f}% {accs[20]*100:>7.1f}% {effect_str:>15}')\n",
    "    print('-'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key hypothesis tests\n",
    "print('\\n' + '='*70)\n",
    "print('HYPOTHESIS TESTS')\n",
    "print('='*70)\n",
    "\n",
    "# H1: Cue-present is L-insensitive\n",
    "print('\\n1. CUE-PRESENT: Is performance L-insensitive?')\n",
    "for c in CORRUPTION_LEVELS:\n",
    "    mask_present = (df['cue_condition'] == 'present') & (df['c'] == c)\n",
    "    acc_5 = df[(mask_present) & (df['L'] == 5)]['is_correct'].mean()\n",
    "    acc_20 = df[(mask_present) & (df['L'] == 20)]['is_correct'].mean()\n",
    "    diff = acc_20 - acc_5\n",
    "    \n",
    "    print(f'  c={c}: L=5 ({acc_5*100:.1f}%) → L=20 ({acc_20*100:.1f}%), diff = {diff*100:+.1f}pp')\n",
    "    if abs(diff) < 0.05:\n",
    "        print(f'    ✓ L-insensitive (cue dominates)')\n",
    "    else:\n",
    "        print(f'    △ Some L sensitivity detected')\n",
    "\n",
    "# H2: Cue-absent shows L effect (redundancy)\n",
    "print('\\n2. CUE-ABSENT: Does L matter (redundancy)?')\n",
    "for c in CORRUPTION_LEVELS:\n",
    "    mask_absent = (df['cue_condition'] == 'absent') & (df['c'] == c)\n",
    "    acc_5 = df[(mask_absent) & (df['L'] == 5)]['is_correct'].mean()\n",
    "    acc_20 = df[(mask_absent) & (df['L'] == 20)]['is_correct'].mean()\n",
    "    diff = acc_20 - acc_5\n",
    "    \n",
    "    print(f'  c={c}: L=5 ({acc_5*100:.1f}%) → L=20 ({acc_20*100:.1f}%), diff = {diff*100:+.1f}pp')\n",
    "    if diff > 0.1:\n",
    "        print(f'    ✓ Strong L effect (redundancy matters)')\n",
    "    elif diff > 0.05:\n",
    "        print(f'    △ Moderate L effect')\n",
    "    else:\n",
    "        print(f'    ✗ Weak or no L effect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for idx, c in enumerate(CORRUPTION_LEVELS):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    for cue in CUE_CONDITIONS:\n",
    "        accs = []\n",
    "        for L in LENGTHS:\n",
    "            mask = (df['c'] == c) & (df['cue_condition'] == cue) & (df['L'] == L)\n",
    "            accs.append(df[mask]['is_correct'].mean() * 100)\n",
    "        \n",
    "        marker = 'o' if cue == 'present' else 's'\n",
    "        linestyle = '-' if cue == 'present' else '--'\n",
    "        ax.plot(LENGTHS, accs, marker=marker, linestyle=linestyle, \n",
    "                label=f'Cue {cue}', linewidth=2, markersize=8)\n",
    "    \n",
    "    ax.set_xlabel('Trace Length (L)', fontsize=12)\n",
    "    ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "    ax.set_title(f'c = {c} (corruption level)', fontsize=14)\n",
    "    ax.set_xticks(LENGTHS)\n",
    "    ax.legend(loc='best')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(0, 100)\n",
    "\n",
    "plt.suptitle('E8: Length × Cue Interaction (L=5,10,15,20)', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{SAVE_DIR}/results/E8_length_cue_figure.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f'Figure saved: {SAVE_DIR}/results/E8_length_cue_figure.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute summary statistics\n",
    "summary_data = {}\n",
    "for c in CORRUPTION_LEVELS:\n",
    "    for cue in CUE_CONDITIONS:\n",
    "        for L in LENGTHS:\n",
    "            mask = (df['c'] == c) & (df['cue_condition'] == cue) & (df['L'] == L)\n",
    "            key = f'c{c}_cue{cue}_L{L}'\n",
    "            summary_data[key] = {\n",
    "                'accuracy': float(df[mask]['is_correct'].mean()),\n",
    "                'n_correct': int(df[mask]['is_correct'].sum()),\n",
    "                'n_total': int(mask.sum())\n",
    "            }\n",
    "\n",
    "# Calculate L effects\n",
    "l_effects = {}\n",
    "for c in CORRUPTION_LEVELS:\n",
    "    for cue in CUE_CONDITIONS:\n",
    "        key = f'c{c}_cue{cue}'\n",
    "        acc_5 = summary_data[f'c{c}_cue{cue}_L5']['accuracy']\n",
    "        acc_20 = summary_data[f'c{c}_cue{cue}_L20']['accuracy']\n",
    "        l_effects[key] = acc_20 - acc_5\n",
    "\n",
    "summary = {\n",
    "    'experiment': 'E8_length_cue_v2_FIXED',\n",
    "    'date': EXPERIMENT_DATE,\n",
    "    'n_problems': len(experiment_problems),\n",
    "    'total_inferences': len(all_results),\n",
    "    'conditions': {\n",
    "        'lengths': LENGTHS,\n",
    "        'corruption_levels': CORRUPTION_LEVELS,\n",
    "        'cue_conditions': CUE_CONDITIONS\n",
    "    },\n",
    "    'results': summary_data,\n",
    "    'l_effects': {k: float(v) for k, v in l_effects.items()},\n",
    "    'interpretation': {\n",
    "        'cue_present_l_insensitive': bool(all(abs(l_effects[f'c{c}_cuepresent']) < 0.1 for c in CORRUPTION_LEVELS)),\n",
    "        'cue_absent_l_sensitive': bool(all(l_effects[f'c{c}_cueabsent'] > 0.05 for c in CORRUPTION_LEVELS))\n",
    "    }\n",
    "}\n",
    "\n",
    "save_json(summary, f'{SAVE_DIR}/results/E8_summary.json')\n",
    "\n",
    "print('='*70)\n",
    "print('E8 EXPERIMENT COMPLETE (FIXED VERSION)')\n",
    "print('='*70)\n",
    "print(f'Date: {EXPERIMENT_DATE}')\n",
    "print(f'Total experiments: {len(all_results)}')\n",
    "print(f'\\nL Effects (L=20 - L=5):')\n",
    "for key, effect in l_effects.items():\n",
    "    print(f'  {key}: {effect*100:+.1f}pp')\n",
    "print(f'\\nInterpretation:')\n",
    "print(f'  Cue-present L-insensitive: {summary[\"interpretation\"][\"cue_present_l_insensitive\"]}')\n",
    "print(f'  Cue-absent L-sensitive: {summary[\"interpretation\"][\"cue_absent_l_sensitive\"]}')\n",
    "print(f'\\nFiles saved to: {SAVE_DIR}')\n",
    "print('='*70)"
   ]
  }
 ]
}
