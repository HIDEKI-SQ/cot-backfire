{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2 GPT-4o E6: Cross-Model Validation (FIXED VERSION)\n",
    "\n",
    "**Paper**: A2 (Cue-Dominant Extraction Explains Length Effects)\n",
    "\n",
    "**CRITICAL FIX**: Uses complete final step replacement (same as E6 v2).\n",
    "\n",
    "**Purpose**: Cross-model validation of E6 using GPT-4o.\n",
    "\n",
    "**Conditions** (A and B only for cost efficiency):\n",
    "| Condition | Step 1-9 | Step 10 (Cue) | Prediction |\n",
    "|-----------|----------|---------------|------------|\n",
    "| **A: Wrong Cue** | Clean | Wrong answer | WRONG (if cue-dominant) |\n",
    "| **B: Correct Cue** | Corrupted | Correct answer | CORRECT (if cue-dominant) |\n",
    "\n",
    "**Expected inference count**: 199 Ã— 2 = 398\n",
    "\n",
    "**Date**: 2026-01-03\n",
    "**VERSION**: 2.0 (FIXED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Google Drive Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "EXPERIMENT_NAME = 'A2_GPT4o_E6_v2'\n",
    "EXPERIMENT_DATE = datetime.now().strftime('%Y%m%d')\n",
    "\n",
    "BASE_DIR = '/content/drive/MyDrive/CoT_Experiment'\n",
    "V3_DATA_DIR = f'{BASE_DIR}/full_experiment_v3_20251224'\n",
    "\n",
    "SAVE_DIR = f'{BASE_DIR}/{EXPERIMENT_NAME}_{EXPERIMENT_DATE}'\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(f'{SAVE_DIR}/results', exist_ok=True)\n",
    "\n",
    "print(f'Experiment: {EXPERIMENT_NAME}')\n",
    "print(f'Save directory: {SAVE_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets openai matplotlib pandas tqdm scipy -q\n",
    "print('Dependencies installed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import random\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from typing import List, Dict, Tuple, Optional, Any\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "GLOBAL_SEED = 20251224\n",
    "E6_SEED = 20260103\n",
    "CORRUPTION_RATE = 0.8\n",
    "\n",
    "API_MAX_TOKENS_ANSWER = 256\n",
    "API_RETRY_DELAY = 1.0\n",
    "API_RATE_LIMIT_DELAY = 0.5\n",
    "\n",
    "print('='*70)\n",
    "print('GPT-4o E6: CROSS-MODEL VALIDATION (FIXED)')\n",
    "print('='*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GSM8KProblem:\n",
    "    index: int\n",
    "    question: str\n",
    "    answer_text: str\n",
    "    final_answer: int\n",
    "\n",
    "@dataclass\n",
    "class CleanTrace:\n",
    "    problem_index: int\n",
    "    I: int\n",
    "    steps: List[str]\n",
    "    full_text: str\n",
    "\n",
    "@dataclass\n",
    "class ManipulatedTrace:\n",
    "    problem_index: int\n",
    "    L: int\n",
    "    condition: str\n",
    "    reasoning_status: str\n",
    "    cue_status: str\n",
    "    cue_answer: int\n",
    "    correct_answer: int\n",
    "    steps: List[str]\n",
    "    full_text: str\n",
    "    seed: int\n",
    "\n",
    "@dataclass\n",
    "class ExperimentResult:\n",
    "    problem_index: int\n",
    "    model: str\n",
    "    condition: str\n",
    "    cue_answer: int\n",
    "    correct_answer: int\n",
    "    model_answer: Optional[int]\n",
    "    is_correct: bool\n",
    "    followed_cue: bool\n",
    "    raw_output: str\n",
    "    timestamp: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derive_seed(global_seed: int, problem_id: int, condition: str) -> int:\n",
    "    key = f\"{global_seed}|E6|{problem_id}|{condition}\"\n",
    "    h = hashlib.sha256(key.encode(\"utf-8\")).hexdigest()\n",
    "    return int(h[:8], 16)\n",
    "\n",
    "def save_json(data: Any, filepath: str):\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def load_json(filepath: str) -> Any:\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problems_path = f'{V3_DATA_DIR}/problems_v3.json'\n",
    "problems_data = load_json(problems_path)\n",
    "problems = [GSM8KProblem(**p) for p in problems_data]\n",
    "prob_map = {p.index: p for p in problems}\n",
    "print(f'Loaded {len(problems)} problems')\n",
    "\n",
    "traces_path = f'{V3_DATA_DIR}/clean_traces/clean_traces_I10_v3.json'\n",
    "traces_data = load_json(traces_path)\n",
    "clean_traces = [CleanTrace(**t) for t in traces_data]\n",
    "print(f'Loaded {len(clean_traces)} traces')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Corruption Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_corrupted_steps(L: int, c: float, seed: int) -> List[int]:\n",
    "    K = int(round(c * (L - 1)))\n",
    "    if K == 0:\n",
    "        return []\n",
    "    steps = list(range(1, L))\n",
    "    rng = random.Random(seed)\n",
    "    rng.shuffle(steps)\n",
    "    return sorted(steps[:K])\n",
    "\n",
    "def assign_corruption_types(corrupted_steps: List[int], seed: int) -> Dict[int, str]:\n",
    "    K = len(corrupted_steps)\n",
    "    if K == 0:\n",
    "        return {}\n",
    "    n_irr = (K * 1) // 5\n",
    "    n_loc = (K * 2) // 5\n",
    "    n_wrong = K - n_irr - n_loc\n",
    "    if n_wrong == 0 and K > 0:\n",
    "        n_wrong = 1\n",
    "        if n_loc > 0:\n",
    "            n_loc -= 1\n",
    "        elif n_irr > 0:\n",
    "            n_irr -= 1\n",
    "    rng = random.Random(seed + 1)\n",
    "    perm = corrupted_steps[:]\n",
    "    rng.shuffle(perm)\n",
    "    type_map = {}\n",
    "    for s in perm[:n_irr]:\n",
    "        type_map[s] = \"IRR\"\n",
    "    for s in perm[n_irr:n_irr + n_loc]:\n",
    "        type_map[s] = \"LOC\"\n",
    "    for s in perm[n_irr + n_loc:]:\n",
    "        type_map[s] = \"WRONG\"\n",
    "    return type_map\n",
    "\n",
    "IRRELEVANT_TEMPLATES = [\n",
    "    \"Compute an auxiliary value: aux = {a} + {b} = {result}, but it will not be used later.\",\n",
    "    \"Compute a side quantity: aux = {a} * 2 = {result}, unrelated to the final result.\",\n",
    "]\n",
    "\n",
    "WRONG_CONSTRAINT_TEMPLATES = [\n",
    "    \"Fix an intermediate condition: set {var} = {wrong_value} as a given constraint.\",\n",
    "    \"Assume the total is {var} = {wrong_value} and proceed using this fixed value.\",\n",
    "]\n",
    "\n",
    "def generate_irrelevant_step(step_num: int, seed: int) -> str:\n",
    "    rng = random.Random(seed)\n",
    "    a, b = rng.randint(2, 20), rng.randint(2, 20)\n",
    "    template = rng.choice(IRRELEVANT_TEMPLATES)\n",
    "    result = a + b if '+' in template else a * 2\n",
    "    return template.format(a=a, b=b, result=result)\n",
    "\n",
    "def generate_local_error_step(original_step: str, seed: int) -> str:\n",
    "    rng = random.Random(seed)\n",
    "    numbers = re.findall(r'\\d+', original_step)\n",
    "    if not numbers:\n",
    "        return f\"Compute t = 10 * 3 = {rng.randint(28, 32)}.\"\n",
    "    original_result = int(numbers[-1])\n",
    "    offset = rng.choice([-3, -2, -1, 1, 2, 3])\n",
    "    wrong_result = max(0, original_result + offset)\n",
    "    modified = re.sub(r'= (\\d+)\\.$', f'= {wrong_result}.', original_step)\n",
    "    if modified == original_step:\n",
    "        modified = re.sub(r'(\\d+)\\.$', f'{wrong_result}.', original_step)\n",
    "    return modified\n",
    "\n",
    "def generate_wrong_constraint_step(step_num: int, seed: int) -> str:\n",
    "    rng = random.Random(seed)\n",
    "    var = rng.choice(['x', 'total', 'result', 'n'])\n",
    "    wrong_value = rng.randint(10, 100)\n",
    "    template = rng.choice(WRONG_CONSTRAINT_TEMPLATES)\n",
    "    return template.format(var=var, wrong_value=wrong_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. FIXED: Final Step Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wrong_answer(correct_answer: int, seed: int) -> int:\n",
    "    rng = random.Random(seed)\n",
    "    magnitude = max(1, abs(correct_answer))\n",
    "    if magnitude < 10:\n",
    "        offset = rng.choice([-5, -4, -3, -2, 2, 3, 4, 5])\n",
    "    elif magnitude < 100:\n",
    "        offset = rng.choice([-20, -15, -10, 10, 15, 20])\n",
    "    elif magnitude < 1000:\n",
    "        offset = rng.choice([-100, -50, 50, 100, 150])\n",
    "    else:\n",
    "        pct = rng.choice([0.1, 0.15, 0.2, -0.1, -0.15, -0.2])\n",
    "        offset = int(correct_answer * pct)\n",
    "    wrong_answer = correct_answer + offset\n",
    "    if wrong_answer == correct_answer:\n",
    "        wrong_answer = correct_answer + 10\n",
    "    if correct_answer > 0 and wrong_answer <= 0:\n",
    "        wrong_answer = abs(wrong_answer) + 1\n",
    "    return wrong_answer\n",
    "\n",
    "def create_clean_final_step(answer: int) -> str:\n",
    "    \"\"\"FIXED: Replace ENTIRE final step with clean cue\"\"\"\n",
    "    return f\"Therefore, the final answer is Final = {answer}.\"\n",
    "\n",
    "print(\"FIXED: Final step is now completely replaced\")\n",
    "print(f\"Example: {create_clean_final_step(70000)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Trace Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_condition_a_trace(clean_trace: CleanTrace, correct_answer: int, seed: int) -> ManipulatedTrace:\n",
    "    \"\"\"Wrong Cue + Clean Reasoning\"\"\"\n",
    "    wrong_answer = generate_wrong_answer(correct_answer, seed)\n",
    "    new_steps = clean_trace.steps[:-1] + [create_clean_final_step(wrong_answer)]\n",
    "    \n",
    "    lines = ['[[COT_START]]']\n",
    "    for i, content in enumerate(new_steps):\n",
    "        lines.append(f'Step {i+1}: {content}')\n",
    "    lines.append('[[COT_END]]')\n",
    "    \n",
    "    return ManipulatedTrace(\n",
    "        problem_index=clean_trace.problem_index,\n",
    "        L=clean_trace.I,\n",
    "        condition='wrong_cue',\n",
    "        reasoning_status='clean',\n",
    "        cue_status='wrong',\n",
    "        cue_answer=wrong_answer,\n",
    "        correct_answer=correct_answer,\n",
    "        steps=new_steps,\n",
    "        full_text='\\n'.join(lines),\n",
    "        seed=seed\n",
    "    )\n",
    "\n",
    "def create_condition_b_trace(clean_trace: CleanTrace, correct_answer: int, seed: int) -> ManipulatedTrace:\n",
    "    \"\"\"Correct Cue + Corrupted Reasoning\"\"\"\n",
    "    L = clean_trace.I\n",
    "    corrupted_steps = pick_corrupted_steps(L, CORRUPTION_RATE, seed)\n",
    "    corruption_types = assign_corruption_types(corrupted_steps, seed)\n",
    "    \n",
    "    new_steps = []\n",
    "    for i, step_content in enumerate(clean_trace.steps[:-1]):\n",
    "        step_num = i + 1\n",
    "        if step_num in corrupted_steps:\n",
    "            ctype = corruption_types[step_num]\n",
    "            step_seed = seed + step_num * 1000\n",
    "            if ctype == 'IRR':\n",
    "                new_content = generate_irrelevant_step(step_num, step_seed)\n",
    "            elif ctype == 'LOC':\n",
    "                new_content = generate_local_error_step(step_content, step_seed)\n",
    "            else:\n",
    "                new_content = generate_wrong_constraint_step(step_num, step_seed)\n",
    "            new_steps.append(new_content)\n",
    "        else:\n",
    "            new_steps.append(step_content)\n",
    "    \n",
    "    new_steps.append(create_clean_final_step(correct_answer))\n",
    "    \n",
    "    lines = ['[[COT_START]]']\n",
    "    for i, content in enumerate(new_steps):\n",
    "        lines.append(f'Step {i+1}: {content}')\n",
    "    lines.append('[[COT_END]]')\n",
    "    \n",
    "    return ManipulatedTrace(\n",
    "        problem_index=clean_trace.problem_index,\n",
    "        L=L,\n",
    "        condition='correct_cue_only',\n",
    "        reasoning_status='corrupted',\n",
    "        cue_status='correct',\n",
    "        cue_answer=correct_answer,\n",
    "        correct_answer=correct_answer,\n",
    "        steps=new_steps,\n",
    "        full_text='\\n'.join(lines),\n",
    "        seed=seed\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. API Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "OPENAI_API_KEY = getpass('Enter OpenAI API Key: ')\n",
    "print('API Key set.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "MODEL = 'gpt-4o'\n",
    "\n",
    "def call_gpt4o(system_prompt: str, user_prompt: str, max_tokens: int = 1024, retries: int = 3) -> str:\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=MODEL, max_tokens=max_tokens,\n",
    "                messages=[{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": user_prompt}],\n",
    "                temperature=0\n",
    "            )\n",
    "            time.sleep(API_RATE_LIMIT_DELAY)\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f'API error (attempt {attempt+1}): {e}')\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(API_RETRY_DELAY * (attempt + 1))\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "test_response = call_gpt4o(\"You output ONLY JSON.\", 'Respond: {\"test\": \"ok\"}', max_tokens=50)\n",
    "print(f'API test: {test_response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a calculator that outputs ONLY JSON.\n",
    "Output exactly: {\"final\": <number>}\n",
    "Do NOT write any explanation.\n",
    "\"\"\"\n",
    "\n",
    "def create_prompt(problem: GSM8KProblem, trace_text: str) -> str:\n",
    "    return f\"\"\"Problem: {problem.question}\n",
    "\n",
    "Reasoning trace:\n",
    "{trace_text}\n",
    "\n",
    "OUTPUT ONLY: {{\"final\": <number>}}\"\"\"\n",
    "\n",
    "def parse_model_answer(response: str) -> Optional[int]:\n",
    "    match = re.search(r'\\{\\s*\"final\"\\s*:\\s*(-?\\d+(?:\\.\\d+)?)\\s*\\}', response)\n",
    "    if match:\n",
    "        return int(round(float(match.group(1))))\n",
    "    match = re.search(r'\"final\"\\s*:\\s*(-?\\d+(?:\\.\\d+)?)', response)\n",
    "    if match:\n",
    "        return int(round(float(match.group(1))))\n",
    "    matches = re.findall(r'(-?\\d+)', response)\n",
    "    if matches:\n",
    "        return int(matches[-1])\n",
    "    return None\n",
    "\n",
    "def run_single_experiment(problem: GSM8KProblem, trace: ManipulatedTrace) -> ExperimentResult:\n",
    "    user_prompt = create_prompt(problem, trace.full_text)\n",
    "    response = call_gpt4o(SYSTEM_PROMPT, user_prompt, max_tokens=API_MAX_TOKENS_ANSWER)\n",
    "    model_answer = parse_model_answer(response)\n",
    "    is_correct = (model_answer == trace.correct_answer) if model_answer is not None else False\n",
    "    followed_cue = (model_answer == trace.cue_answer) if model_answer is not None else False\n",
    "    \n",
    "    return ExperimentResult(\n",
    "        problem_index=problem.index, model=MODEL, condition=trace.condition,\n",
    "        cue_answer=trace.cue_answer, correct_answer=trace.correct_answer,\n",
    "        model_answer=model_answer, is_correct=is_correct, followed_cue=followed_cue,\n",
    "        raw_output=response, timestamp=datetime.now().isoformat()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*70)\n",
    "print('GPT-4o E6 (FIXED VERSION)')\n",
    "print('='*70)\n",
    "\n",
    "all_results = []\n",
    "all_traces = []\n",
    "\n",
    "for trace in tqdm(clean_traces, desc='Running GPT-4o E6'):\n",
    "    problem = prob_map.get(trace.problem_index)\n",
    "    if problem is None:\n",
    "        continue\n",
    "    \n",
    "    seed = derive_seed(E6_SEED, trace.problem_index, 'E6')\n",
    "    correct_answer = problem.final_answer\n",
    "    \n",
    "    trace_a = create_condition_a_trace(trace, correct_answer, seed)\n",
    "    trace_b = create_condition_b_trace(trace, correct_answer, seed)\n",
    "    \n",
    "    result_a = run_single_experiment(problem, trace_a)\n",
    "    result_b = run_single_experiment(problem, trace_b)\n",
    "    \n",
    "    all_results.extend([result_a, result_b])\n",
    "    all_traces.append({\n",
    "        'problem_index': trace.problem_index,\n",
    "        'condition_a': asdict(trace_a),\n",
    "        'condition_b': asdict(trace_b)\n",
    "    })\n",
    "\n",
    "print(f'\\nTotal experiments: {len(all_results)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json([asdict(r) for r in all_results], f'{SAVE_DIR}/results/A2_GPT4o_E6_results.json')\n",
    "save_json(all_traces, f'{SAVE_DIR}/results/A2_GPT4o_E6_traces.json')\n",
    "print(f'Results saved')\n",
    "\n",
    "df = pd.DataFrame([asdict(r) for r in all_results])\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('GPT-4o E6 RESULTS (FIXED)')\n",
    "print('='*70)\n",
    "\n",
    "for condition in ['wrong_cue', 'correct_cue_only']:\n",
    "    cond_df = df[df['condition'] == condition]\n",
    "    acc = cond_df['is_correct'].mean()\n",
    "    cue_follow = cond_df['followed_cue'].mean()\n",
    "    print(f'\\n{condition.upper()}:')\n",
    "    print(f'  Accuracy: {acc*100:.2f}%')\n",
    "    print(f'  Cue Following: {cue_follow*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_a = df[df['condition'] == 'wrong_cue']['is_correct'].mean()\n",
    "acc_b = df[df['condition'] == 'correct_cue_only']['is_correct'].mean()\n",
    "cue_follow_a = df[df['condition'] == 'wrong_cue']['followed_cue'].mean()\n",
    "cue_follow_b = df[df['condition'] == 'correct_cue_only']['followed_cue'].mean()\n",
    "\n",
    "summary = {\n",
    "    'experiment': 'A2_GPT4o_E6_v2_FIXED',\n",
    "    'model': MODEL,\n",
    "    'date': EXPERIMENT_DATE,\n",
    "    'n_problems': len(clean_traces),\n",
    "    'total_inferences': len(all_results),\n",
    "    'results': {\n",
    "        'condition_a': {\n",
    "            'description': 'Wrong Cue + Clean Reasoning',\n",
    "            'accuracy': acc_a,\n",
    "            'followed_cue': cue_follow_a,\n",
    "            'n_correct': int(df[df['condition'] == 'wrong_cue']['is_correct'].sum())\n",
    "        },\n",
    "        'condition_b': {\n",
    "            'description': 'Correct Cue + Corrupted Reasoning',\n",
    "            'accuracy': acc_b,\n",
    "            'followed_cue': cue_follow_b,\n",
    "            'n_correct': int(df[df['condition'] == 'correct_cue_only']['is_correct'].sum())\n",
    "        }\n",
    "    },\n",
    "    'interpretation': {\n",
    "        'cue_dominant': bool(cue_follow_a > 0.5),\n",
    "        'cue_rescues': bool(acc_b > 0.7)\n",
    "    }\n",
    "}\n",
    "\n",
    "save_json(summary, f'{SAVE_DIR}/results/A2_GPT4o_E6_summary.json')\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('COMPLETE')\n",
    "print('='*70)\n",
    "print(f'Condition A (Wrong Cue): {acc_a*100:.1f}% acc, {cue_follow_a*100:.1f}% cue follow')\n",
    "print(f'Condition B (Correct Cue): {acc_b*100:.1f}% acc')"
   ]
  }
 ]
}
