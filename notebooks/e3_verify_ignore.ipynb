{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E3: Verification Instruction Gating Experiment\n",
    "\n",
    "**Purpose**: Separate \"compliance\" from \"verification ability\"\n",
    "\n",
    "**Prompt Conditions**:\n",
    "1. **USE**: Follow the provided trace and answer\n",
    "2. **VERIFY**: Verify the trace, correct if wrong, then answer\n",
    "3. **IGNORE**: Ignore the trace and solve directly\n",
    "\n",
    "**Models**: Claude 4 Sonnet, GPT-4o, Claude 3.5 Haiku\n",
    "\n",
    "**λ condition**: 0.8 (most discriminative)\n",
    "\n",
    "**Expected outcome**:\n",
    "- If VERIFY helps: dual-route = verification pathway\n",
    "- If IGNORE helps: model can bypass contamination\n",
    "- If USE is best: model benefits from trace structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "EXPERIMENT_DATE = datetime.now().strftime('%Y%m%d')\n",
    "SAVE_DIR = '/content/drive/MyDrive/CoT_Experiment'\n",
    "SAVE_DIR_EXP = f'{SAVE_DIR}/e3_verify_ignore_{EXPERIMENT_DATE}'\n",
    "os.makedirs(SAVE_DIR_EXP, exist_ok=True)\n",
    "os.makedirs(f'{SAVE_DIR_EXP}/results', exist_ok=True)\n",
    "os.makedirs(f'{SAVE_DIR_EXP}/checkpoints', exist_ok=True)\n",
    "\n",
    "print(f'Save directory: {SAVE_DIR_EXP}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets openai anthropic pandas tqdm matplotlib -q\n",
    "print('Dependencies installed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import hashlib\n",
    "from typing import List, Dict, Optional, Any\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Configuration\n",
    "GLOBAL_SEED = 20251224\n",
    "N_PROBLEMS = 100  # Subset for efficiency\n",
    "I_FIXED = 10\n",
    "LAMBDA_FIXED = 0.8  # Fixed at most discriminative point\n",
    "INSTRUCTION_CONDITIONS = ['USE', 'VERIFY', 'IGNORE']\n",
    "\n",
    "# Models to test\n",
    "MODELS = {\n",
    "    'Claude 4 Sonnet': {\n",
    "        'provider': 'anthropic',\n",
    "        'api_name': 'claude-sonnet-4-20250514',\n",
    "        'short': 'sonnet4'\n",
    "    },\n",
    "    'GPT-4o': {\n",
    "        'provider': 'openai',\n",
    "        'api_name': 'gpt-4o',\n",
    "        'short': 'gpt4o'\n",
    "    },\n",
    "    'Claude 3.5 Haiku': {\n",
    "        'provider': 'anthropic',\n",
    "        'api_name': 'claude-3-5-haiku-latest',\n",
    "        'short': 'haiku35'\n",
    "    }\n",
    "}\n",
    "\n",
    "print('='*60)\n",
    "print('E3: VERIFY/IGNORE INSTRUCTION EXPERIMENT')\n",
    "print('='*60)\n",
    "print(f'Models: {list(MODELS.keys())}')\n",
    "print(f'λ (fixed): {LAMBDA_FIXED}')\n",
    "print(f'Instruction conditions: {INSTRUCTION_CONDITIONS}')\n",
    "print(f'Problems: {N_PROBLEMS}')\n",
    "print(f'Total inferences per model: {N_PROBLEMS * len(INSTRUCTION_CONDITIONS) + N_PROBLEMS}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. API Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "\n",
    "print(\"OpenAI APIキーを入力してください：\")\n",
    "OPENAI_API_KEY = getpass.getpass(\"OpenAI API Key: \")\n",
    "\n",
    "print(\"\\nAnthropic APIキーを入力してください：\")\n",
    "ANTHROPIC_API_KEY = getpass.getpass(\"Anthropic API Key: \")\n",
    "\n",
    "openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "anthropic_client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n",
    "\n",
    "def call_api(prompt: str, model_config: dict, max_tokens: int = 512) -> str:\n",
    "    \"\"\"Unified API call for both providers\"\"\"\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            if model_config['provider'] == 'openai':\n",
    "                response = openai_client.chat.completions.create(\n",
    "                    model=model_config['api_name'],\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    max_tokens=max_tokens,\n",
    "                    temperature=0\n",
    "                )\n",
    "                return response.choices[0].message.content\n",
    "            else:\n",
    "                response = anthropic_client.messages.create(\n",
    "                    model=model_config['api_name'],\n",
    "                    max_tokens=max_tokens,\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "                )\n",
    "                return response.content[0].text\n",
    "        except Exception as e:\n",
    "            print(f'API error (attempt {attempt+1}): {e}')\n",
    "            time.sleep(2 ** attempt)\n",
    "    return \"\"\n",
    "\n",
    "# Test APIs\n",
    "print('\\nTesting APIs...')\n",
    "for name, config in MODELS.items():\n",
    "    resp = call_api(\"What is 2+2? Reply with just the number.\", config)\n",
    "    print(f'{name}: {resp.strip()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load GSM8K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "@dataclass\n",
    "class GSM8KProblem:\n",
    "    index: int\n",
    "    question: str\n",
    "    answer_text: str\n",
    "    final_answer: int\n",
    "\n",
    "def extract_final_answer(answer_text: str) -> int:\n",
    "    match = re.search(r'####\\s*([\\d,]+)', answer_text)\n",
    "    if match:\n",
    "        return int(match.group(1).replace(',', ''))\n",
    "    raise ValueError('Could not extract final answer')\n",
    "\n",
    "def save_json(data, filepath):\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    print(f'Saved: {filepath}')\n",
    "\n",
    "dataset = load_dataset('gsm8k', 'main', split='test')\n",
    "print(f'GSM8K loaded: {len(dataset)} problems')\n",
    "\n",
    "# Select same seed, but only first N_PROBLEMS\n",
    "rng = random.Random(GLOBAL_SEED)\n",
    "indices = list(range(len(dataset)))\n",
    "rng.shuffle(indices)\n",
    "selected_indices = sorted(indices[:N_PROBLEMS])\n",
    "\n",
    "problems = []\n",
    "for idx in selected_indices:\n",
    "    item = dataset[idx]\n",
    "    try:\n",
    "        final_ans = extract_final_answer(item['answer'])\n",
    "        problems.append(GSM8KProblem(\n",
    "            index=idx,\n",
    "            question=item['question'],\n",
    "            answer_text=item['answer'],\n",
    "            final_answer=final_ans\n",
    "        ))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(f'Loaded {len(problems)} problems')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. CoT Generation (Same as A3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_clean_cot(problem: GSM8KProblem, I: int) -> List[str]:\n",
    "    lines = problem.answer_text.split('\\n')\n",
    "    steps = [l.strip() for l in lines if l.strip() and not l.strip().startswith('####')]\n",
    "    while len(steps) < I:\n",
    "        steps.append(f\"Step {len(steps)+1}: Continue calculation.\")\n",
    "    return steps[:I]\n",
    "\n",
    "def generate_corrupted_step(problem: GSM8KProblem, step_idx: int, rng: random.Random) -> str:\n",
    "    templates = [\n",
    "        f\"Step {step_idx+1}: Let's multiply: {rng.randint(10, 100)} × {rng.randint(2, 10)} = {rng.randint(100, 1000)}.\",\n",
    "        f\"Step {step_idx+1}: Adding totals: {rng.randint(50, 200)} + {rng.randint(50, 200)} = {rng.randint(100, 500)}.\",\n",
    "        f\"Step {step_idx+1}: The difference: {rng.randint(100, 500)} - {rng.randint(20, 100)} = {rng.randint(50, 400)}.\",\n",
    "    ]\n",
    "    return rng.choice(templates)\n",
    "\n",
    "def generate_mixed_cot(problem: GSM8KProblem, I: int, lam: float, seed: int) -> List[str]:\n",
    "    rng = random.Random(seed)\n",
    "    clean_steps = generate_clean_cot(problem, I)\n",
    "    \n",
    "    n_corrupt = int(round(I * lam))\n",
    "    corrupt_indices = set(rng.sample(range(I), n_corrupt)) if n_corrupt > 0 else set()\n",
    "    \n",
    "    mixed_steps = []\n",
    "    for i in range(I):\n",
    "        if i in corrupt_indices:\n",
    "            mixed_steps.append(generate_corrupted_step(problem, i, rng))\n",
    "        else:\n",
    "            mixed_steps.append(clean_steps[i] if i < len(clean_steps) else f\"Step {i+1}: Continue.\")\n",
    "    \n",
    "    return mixed_steps\n",
    "\n",
    "def derive_seed(global_seed: int, problem_id: int, lam: float) -> int:\n",
    "    key = f\"{global_seed}|{problem_id}|lam={lam}\"\n",
    "    h = hashlib.sha256(key.encode(\"utf-8\")).hexdigest()\n",
    "    return int(h[:8], 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Instruction-Specific Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(problem: GSM8KProblem, cot_steps: List[str], instruction: str) -> str:\n",
    "    \"\"\"\n",
    "    Create prompt based on instruction condition\n",
    "    \"\"\"\n",
    "    steps_text = '\\n'.join(cot_steps)\n",
    "    \n",
    "    if instruction == 'USE':\n",
    "        return f\"\"\"Here is a math problem with a provided reasoning trace.\n",
    "Follow the provided reasoning trace and give the final numerical answer.\n",
    "\n",
    "Problem: {problem.question}\n",
    "\n",
    "Reasoning trace:\n",
    "{steps_text}\n",
    "\n",
    "Based on this reasoning, what is the final answer?\n",
    "Reply with ONLY: {{\"final\": <number>}}\"\"\"\n",
    "\n",
    "    elif instruction == 'VERIFY':\n",
    "        return f\"\"\"Here is a math problem with a provided reasoning trace.\n",
    "CAREFULLY VERIFY the reasoning trace. If you find any errors, CORRECT them.\n",
    "Then give the final numerical answer based on your verified/corrected reasoning.\n",
    "\n",
    "Problem: {problem.question}\n",
    "\n",
    "Reasoning trace to verify:\n",
    "{steps_text}\n",
    "\n",
    "Verify the above reasoning, correct any errors, and give the final answer.\n",
    "Reply with ONLY: {{\"final\": <number>}}\"\"\"\n",
    "\n",
    "    elif instruction == 'IGNORE':\n",
    "        return f\"\"\"Here is a math problem with a provided reasoning trace.\n",
    "IGNORE the provided reasoning trace completely.\n",
    "Solve the problem yourself from scratch and give the final numerical answer.\n",
    "\n",
    "Problem: {problem.question}\n",
    "\n",
    "Reasoning trace (IGNORE THIS):\n",
    "{steps_text}\n",
    "\n",
    "Solve the problem yourself, ignoring the trace above.\n",
    "Reply with ONLY: {{\"final\": <number>}}\"\"\"\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown instruction: {instruction}\")\n",
    "\n",
    "def create_direct_prompt(problem: GSM8KProblem) -> str:\n",
    "    return f\"\"\"Solve this math problem. Give ONLY the final numerical answer in JSON format.\n",
    "\n",
    "Problem: {problem.question}\n",
    "\n",
    "Reply with ONLY: {{\"final\": <number>}}\"\"\"\n",
    "\n",
    "def parse_answer(response: str) -> Optional[int]:\n",
    "    try:\n",
    "        match = re.search(r'\\{[^}]*\"final\"\\s*:\\s*(\\d+)[^}]*\\}', response)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        numbers = re.findall(r'\\b(\\d+)\\b', response)\n",
    "        if numbers:\n",
    "            return int(numbers[-1])\n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "# Test prompts\n",
    "test_prob = problems[0]\n",
    "test_cot = generate_mixed_cot(test_prob, I_FIXED, LAMBDA_FIXED, 42)\n",
    "print('=== USE Prompt ===')\n",
    "print(create_prompt(test_prob, test_cot, 'USE')[:500])\n",
    "print('\\n=== VERIFY Prompt ===')\n",
    "print(create_prompt(test_prob, test_cot, 'VERIFY')[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select model\n",
    "#@title Select Model { run: \"auto\" }\n",
    "MODEL_CHOICE = \"Claude 4 Sonnet\" #@param [\"Claude 4 Sonnet\", \"GPT-4o\", \"Claude 3.5 Haiku\"]\n",
    "\n",
    "model_config = MODELS[MODEL_CHOICE]\n",
    "print(f'Running E3 for: {MODEL_CHOICE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*60)\n",
    "print(f'E3: VERIFY/IGNORE EXPERIMENT - {MODEL_CHOICE}')\n",
    "print('='*60)\n",
    "\n",
    "all_results = []\n",
    "\n",
    "# First: Direct condition (baseline)\n",
    "print('\\n--- DIRECT (Baseline) ---')\n",
    "for problem in tqdm(problems, desc='Direct'):\n",
    "    prompt = create_direct_prompt(problem)\n",
    "    response = call_api(prompt, model_config)\n",
    "    answer = parse_answer(response)\n",
    "    is_correct = (answer == problem.final_answer) if answer else False\n",
    "    \n",
    "    all_results.append({\n",
    "        'problem_index': problem.index,\n",
    "        'model': MODEL_CHOICE,\n",
    "        'condition': 'DIRECT',\n",
    "        'lam': None,\n",
    "        'model_answer': answer,\n",
    "        'correct_answer': problem.final_answer,\n",
    "        'is_correct': is_correct,\n",
    "        'raw_output': response,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    })\n",
    "    time.sleep(0.3)\n",
    "\n",
    "save_json(all_results, f\"{SAVE_DIR_EXP}/checkpoints/e3_{model_config['short']}_direct.json\")\n",
    "\n",
    "# Then: Each instruction condition with contaminated CoT\n",
    "for instruction in INSTRUCTION_CONDITIONS:\n",
    "    print(f'\\n--- {instruction} (λ={LAMBDA_FIXED}) ---')\n",
    "    \n",
    "    for problem in tqdm(problems, desc=instruction):\n",
    "        # Generate contaminated CoT (same for all conditions)\n",
    "        seed = derive_seed(GLOBAL_SEED, problem.index, LAMBDA_FIXED)\n",
    "        cot_steps = generate_mixed_cot(problem, I_FIXED, LAMBDA_FIXED, seed)\n",
    "        \n",
    "        # Create instruction-specific prompt\n",
    "        prompt = create_prompt(problem, cot_steps, instruction)\n",
    "        response = call_api(prompt, model_config)\n",
    "        \n",
    "        answer = parse_answer(response)\n",
    "        is_correct = (answer == problem.final_answer) if answer else False\n",
    "        \n",
    "        all_results.append({\n",
    "            'problem_index': problem.index,\n",
    "            'model': MODEL_CHOICE,\n",
    "            'condition': instruction,\n",
    "            'lam': LAMBDA_FIXED,\n",
    "            'model_answer': answer,\n",
    "            'correct_answer': problem.final_answer,\n",
    "            'is_correct': is_correct,\n",
    "            'raw_output': response,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        })\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    save_json(all_results, f\"{SAVE_DIR_EXP}/checkpoints/e3_{model_config['short']}_{instruction}.json\")\n",
    "\n",
    "# Save final\n",
    "save_json(all_results, f\"{SAVE_DIR_EXP}/results/e3_results_{model_config['short']}.json\")\n",
    "print('\\n✓ E3 experiment complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.DataFrame(all_results)\n",
    "\n",
    "# Calculate accuracy by condition\n",
    "acc_by_condition = df.groupby('condition')['is_correct'].mean()\n",
    "\n",
    "print('='*60)\n",
    "print(f'E3 RESULTS: {MODEL_CHOICE}')\n",
    "print('='*60)\n",
    "print('\\nAccuracy by Instruction Condition:')\n",
    "for cond in ['DIRECT', 'USE', 'VERIFY', 'IGNORE']:\n",
    "    if cond in acc_by_condition:\n",
    "        acc = acc_by_condition[cond]\n",
    "        print(f'  {cond}: {acc:.1%}')\n",
    "\n",
    "# Visualization\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "conditions = ['DIRECT', 'USE', 'VERIFY', 'IGNORE']\n",
    "colors = ['#333333', '#d62728', '#2ca02c', '#1f77b4']\n",
    "accs = [acc_by_condition.get(c, 0) * 100 for c in conditions]\n",
    "\n",
    "bars = ax.bar(conditions, accs, color=colors, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "# Add value labels\n",
    "for bar, acc in zip(bars, accs):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "            f'{acc:.1f}%', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=13)\n",
    "ax.set_xlabel('Instruction Condition', fontsize=13)\n",
    "ax.set_title(f'E3: Instruction Gating Analysis - {MODEL_CHOICE}\\n(λ={LAMBDA_FIXED})', fontsize=14)\n",
    "ax.set_ylim(0, 105)\n",
    "ax.axhline(y=acc_by_condition.get('DIRECT', 0) * 100, color='gray', linestyle='--', alpha=0.7, label='Direct baseline')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{SAVE_DIR_EXP}/e3_instruction_gating_{model_config['short']}.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary and interpretation\n",
    "direct_acc = acc_by_condition.get('DIRECT', 0)\n",
    "use_acc = acc_by_condition.get('USE', 0)\n",
    "verify_acc = acc_by_condition.get('VERIFY', 0)\n",
    "ignore_acc = acc_by_condition.get('IGNORE', 0)\n",
    "\n",
    "summary = {\n",
    "    'experiment': 'E3_Instruction_Gating',\n",
    "    'model': MODEL_CHOICE,\n",
    "    'date': EXPERIMENT_DATE,\n",
    "    'n_problems': len(problems),\n",
    "    'lambda': LAMBDA_FIXED,\n",
    "    'accuracy': {\n",
    "        'DIRECT': direct_acc,\n",
    "        'USE': use_acc,\n",
    "        'VERIFY': verify_acc,\n",
    "        'IGNORE': ignore_acc\n",
    "    },\n",
    "    'delta_vs_direct': {\n",
    "        'USE': use_acc - direct_acc,\n",
    "        'VERIFY': verify_acc - direct_acc,\n",
    "        'IGNORE': ignore_acc - direct_acc\n",
    "    }\n",
    "}\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('INTERPRETATION')\n",
    "print('='*60)\n",
    "\n",
    "print(f'\\nDelta vs Direct baseline:')\n",
    "print(f'  USE:    {use_acc - direct_acc:+.1%}')\n",
    "print(f'  VERIFY: {verify_acc - direct_acc:+.1%}')\n",
    "print(f'  IGNORE: {ignore_acc - direct_acc:+.1%}')\n",
    "\n",
    "best_condition = max(['USE', 'VERIFY', 'IGNORE'], key=lambda c: acc_by_condition.get(c, 0))\n",
    "print(f'\\nBest condition: {best_condition}')\n",
    "\n",
    "if verify_acc > use_acc:\n",
    "    print('→ VERIFY helps: Model can detect and correct errors (verification pathway)')\n",
    "if ignore_acc > use_acc:\n",
    "    print('→ IGNORE helps: Model can bypass contaminated trace')\n",
    "if use_acc >= verify_acc and use_acc >= ignore_acc:\n",
    "    print('→ USE is best: Model benefits from trace structure even with contamination')\n",
    "\n",
    "save_json(summary, f\"{SAVE_DIR_EXP}/results/e3_summary_{model_config['short']}.json\")"
   ]
  }
 ]
}
