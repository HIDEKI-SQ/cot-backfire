{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E2: Contamination Type Analysis\n",
    "\n",
    "**Purpose**: Test if dual-route models resist specific contamination types\n",
    "\n",
    "**Hypothesis**:\n",
    "- If dual-route = \"verification\": WRONG drops, but IRR/LOC resist\n",
    "- If dual-route = \"reference\": All types resist equally\n",
    "\n",
    "**Contamination Types**:\n",
    "1. **WRONG**: Conclusively misleading (wrong final calculation)\n",
    "2. **LOC**: Local errors (intermediate step errors)\n",
    "3. **IRR**: Irrelevant insertions (unrelated steps)\n",
    "\n",
    "**Models**: Claude 4 Sonnet, GPT-4o, Claude 3.5 Haiku\n",
    "\n",
    "**λ conditions**: 0.4, 0.8, 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "EXPERIMENT_DATE = datetime.now().strftime('%Y%m%d')\n",
    "SAVE_DIR = '/content/drive/MyDrive/CoT_Experiment'\n",
    "SAVE_DIR_EXP = f'{SAVE_DIR}/e2_contamination_type_{EXPERIMENT_DATE}'\n",
    "os.makedirs(SAVE_DIR_EXP, exist_ok=True)\n",
    "os.makedirs(f'{SAVE_DIR_EXP}/results', exist_ok=True)\n",
    "os.makedirs(f'{SAVE_DIR_EXP}/checkpoints', exist_ok=True)\n",
    "\n",
    "print(f'Save directory: {SAVE_DIR_EXP}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets openai anthropic pandas tqdm matplotlib -q\n",
    "print('Dependencies installed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import hashlib\n",
    "from typing import List, Dict, Optional, Any\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Configuration\n",
    "GLOBAL_SEED = 20251224\n",
    "N_PROBLEMS = 199  # Same as main experiment\n",
    "I_FIXED = 10\n",
    "LAMBDA_VALUES = [0.4, 0.8, 1.0]  # 3 points\n",
    "CONTAMINATION_TYPES = ['WRONG', 'LOC', 'IRR']\n",
    "\n",
    "# Models to test\n",
    "MODELS = {\n",
    "    'Claude 4 Sonnet': {\n",
    "        'provider': 'anthropic',\n",
    "        'api_name': 'claude-sonnet-4-20250514',\n",
    "        'short': 'sonnet4'\n",
    "    },\n",
    "    'GPT-4o': {\n",
    "        'provider': 'openai',\n",
    "        'api_name': 'gpt-4o',\n",
    "        'short': 'gpt4o'\n",
    "    },\n",
    "    'Claude 3.5 Haiku': {\n",
    "        'provider': 'anthropic',\n",
    "        'api_name': 'claude-3-5-haiku-latest',\n",
    "        'short': 'haiku35'\n",
    "    }\n",
    "}\n",
    "\n",
    "print('='*60)\n",
    "print('E2: CONTAMINATION TYPE EXPERIMENT')\n",
    "print('='*60)\n",
    "print(f'Models: {list(MODELS.keys())}')\n",
    "print(f'λ values: {LAMBDA_VALUES}')\n",
    "print(f'Contamination types: {CONTAMINATION_TYPES}')\n",
    "print(f'Total conditions per model: {len(LAMBDA_VALUES) * len(CONTAMINATION_TYPES)}')\n",
    "print(f'Total inferences per model: {N_PROBLEMS * len(LAMBDA_VALUES) * len(CONTAMINATION_TYPES)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. API Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "\n",
    "print(\"OpenAI APIキーを入力してください：\")\n",
    "OPENAI_API_KEY = getpass.getpass(\"OpenAI API Key: \")\n",
    "\n",
    "print(\"\\nAnthropic APIキーを入力してください：\")\n",
    "ANTHROPIC_API_KEY = getpass.getpass(\"Anthropic API Key: \")\n",
    "\n",
    "openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "anthropic_client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n",
    "\n",
    "def call_api(prompt: str, model_config: dict, max_tokens: int = 256) -> str:\n",
    "    \"\"\"Unified API call for both providers\"\"\"\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            if model_config['provider'] == 'openai':\n",
    "                response = openai_client.chat.completions.create(\n",
    "                    model=model_config['api_name'],\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    max_tokens=max_tokens,\n",
    "                    temperature=0\n",
    "                )\n",
    "                return response.choices[0].message.content\n",
    "            else:  # anthropic\n",
    "                response = anthropic_client.messages.create(\n",
    "                    model=model_config['api_name'],\n",
    "                    max_tokens=max_tokens,\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "                )\n",
    "                return response.content[0].text\n",
    "        except Exception as e:\n",
    "            print(f'API error (attempt {attempt+1}): {e}')\n",
    "            time.sleep(2 ** attempt)\n",
    "    return \"\"\n",
    "\n",
    "# Test APIs\n",
    "print('\\nTesting APIs...')\n",
    "for name, config in MODELS.items():\n",
    "    resp = call_api(\"What is 2+2? Reply with just the number.\", config)\n",
    "    print(f'{name}: {resp.strip()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load GSM8K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "@dataclass\n",
    "class GSM8KProblem:\n",
    "    index: int\n",
    "    question: str\n",
    "    answer_text: str\n",
    "    final_answer: int\n",
    "\n",
    "def extract_final_answer(answer_text: str) -> int:\n",
    "    match = re.search(r'####\\s*([\\d,]+)', answer_text)\n",
    "    if match:\n",
    "        return int(match.group(1).replace(',', ''))\n",
    "    raise ValueError('Could not extract final answer')\n",
    "\n",
    "def save_json(data, filepath):\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    print(f'Saved: {filepath}')\n",
    "\n",
    "dataset = load_dataset('gsm8k', 'main', split='test')\n",
    "print(f'GSM8K loaded: {len(dataset)} problems')\n",
    "\n",
    "# Select same problems as main experiment\n",
    "rng = random.Random(GLOBAL_SEED)\n",
    "indices = list(range(len(dataset)))\n",
    "rng.shuffle(indices)\n",
    "selected_indices = sorted(indices[:N_PROBLEMS])\n",
    "\n",
    "problems = []\n",
    "for idx in selected_indices:\n",
    "    item = dataset[idx]\n",
    "    try:\n",
    "        final_ans = extract_final_answer(item['answer'])\n",
    "        problems.append(GSM8KProblem(\n",
    "            index=idx,\n",
    "            question=item['question'],\n",
    "            answer_text=item['answer'],\n",
    "            final_answer=final_ans\n",
    "        ))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(f'Loaded {len(problems)} problems')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Contamination Type Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_clean_cot(problem: GSM8KProblem, I: int) -> List[str]:\n",
    "    \"\"\"Extract clean reasoning steps from GSM8K answer\"\"\"\n",
    "    lines = problem.answer_text.split('\\n')\n",
    "    steps = [l.strip() for l in lines if l.strip() and not l.strip().startswith('####')]\n",
    "    while len(steps) < I:\n",
    "        steps.append(f\"Step {len(steps)+1}: Continue calculation.\")\n",
    "    return steps[:I]\n",
    "\n",
    "def generate_WRONG_step(problem: GSM8KProblem, step_idx: int, rng: random.Random) -> str:\n",
    "    \"\"\"\n",
    "    WRONG: Generate step that leads to definitively WRONG conclusion\n",
    "    - Deliberately miscalculates to produce incorrect final answer\n",
    "    \"\"\"\n",
    "    wrong_answer = problem.final_answer + rng.randint(10, 100) * rng.choice([1, -1])\n",
    "    if wrong_answer < 0:\n",
    "        wrong_answer = abs(wrong_answer)\n",
    "    \n",
    "    templates = [\n",
    "        f\"Step {step_idx+1}: Therefore, the total is {wrong_answer}.\",\n",
    "        f\"Step {step_idx+1}: Adding everything together gives us {wrong_answer}.\",\n",
    "        f\"Step {step_idx+1}: The final calculation shows {wrong_answer}.\",\n",
    "        f\"Step {step_idx+1}: So the answer must be {wrong_answer}.\",\n",
    "    ]\n",
    "    return rng.choice(templates)\n",
    "\n",
    "def generate_LOC_step(problem: GSM8KProblem, step_idx: int, rng: random.Random) -> str:\n",
    "    \"\"\"\n",
    "    LOC: Generate step with LOCAL arithmetic error\n",
    "    - Intermediate calculation is wrong, but doesn't explicitly state final answer\n",
    "    \"\"\"\n",
    "    a = rng.randint(10, 100)\n",
    "    b = rng.randint(2, 20)\n",
    "    wrong_result = a * b + rng.randint(-20, 20)  # Deliberately wrong\n",
    "    \n",
    "    templates = [\n",
    "        f\"Step {step_idx+1}: Calculating {a} × {b} = {wrong_result}.\",\n",
    "        f\"Step {step_idx+1}: If we add {a} + {b} = {a + b + rng.randint(-5, 5)}.\",\n",
    "        f\"Step {step_idx+1}: The difference {a} - {b} = {a - b + rng.randint(-5, 5)}.\",\n",
    "        f\"Step {step_idx+1}: Dividing {a * b} by {b} gives {a + rng.randint(-3, 3)}.\",\n",
    "    ]\n",
    "    return rng.choice(templates)\n",
    "\n",
    "def generate_IRR_step(problem: GSM8KProblem, step_idx: int, rng: random.Random) -> str:\n",
    "    \"\"\"\n",
    "    IRR: Generate IRRELEVANT step\n",
    "    - Completely unrelated to the problem, doesn't affect calculation\n",
    "    \"\"\"\n",
    "    templates = [\n",
    "        f\"Step {step_idx+1}: Let's consider the weather today.\",\n",
    "        f\"Step {step_idx+1}: This reminds me of a similar problem about colors.\",\n",
    "        f\"Step {step_idx+1}: Interestingly, the number 7 is considered lucky.\",\n",
    "        f\"Step {step_idx+1}: Before continuing, note that math is useful in daily life.\",\n",
    "        f\"Step {step_idx+1}: This type of problem appears frequently in textbooks.\",\n",
    "        f\"Step {step_idx+1}: Let's take a moment to review our approach.\",\n",
    "        f\"Step {step_idx+1}: Mathematics has a long and rich history.\",\n",
    "    ]\n",
    "    return rng.choice(templates)\n",
    "\n",
    "def generate_contaminated_cot(problem: GSM8KProblem, I: int, lam: float, \n",
    "                               contamination_type: str, seed: int) -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate CoT with specific contamination type\n",
    "    \"\"\"\n",
    "    rng = random.Random(seed)\n",
    "    clean_steps = generate_clean_cot(problem, I)\n",
    "    \n",
    "    n_contaminate = int(round(I * lam))\n",
    "    contaminate_indices = set(rng.sample(range(I), n_contaminate)) if n_contaminate > 0 else set()\n",
    "    \n",
    "    # Select generator based on type\n",
    "    generators = {\n",
    "        'WRONG': generate_WRONG_step,\n",
    "        'LOC': generate_LOC_step,\n",
    "        'IRR': generate_IRR_step\n",
    "    }\n",
    "    generator = generators[contamination_type]\n",
    "    \n",
    "    mixed_steps = []\n",
    "    for i in range(I):\n",
    "        if i in contaminate_indices:\n",
    "            mixed_steps.append(generator(problem, i, rng))\n",
    "        else:\n",
    "            mixed_steps.append(clean_steps[i] if i < len(clean_steps) else f\"Step {i+1}: Continue.\")\n",
    "    \n",
    "    return mixed_steps\n",
    "\n",
    "# Test\n",
    "test_prob = problems[0]\n",
    "print('Test WRONG:', generate_WRONG_step(test_prob, 0, random.Random(42)))\n",
    "print('Test LOC:', generate_LOC_step(test_prob, 0, random.Random(42)))\n",
    "print('Test IRR:', generate_IRR_step(test_prob, 0, random.Random(42)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prompts and Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cot_prompt(problem: GSM8KProblem, cot_steps: List[str]) -> str:\n",
    "    steps_text = '\\n'.join(cot_steps)\n",
    "    return f\"\"\"Here is a math problem with a provided reasoning trace.\n",
    "Follow the reasoning and give the final numerical answer in JSON format.\n",
    "\n",
    "Problem: {problem.question}\n",
    "\n",
    "Reasoning trace:\n",
    "{steps_text}\n",
    "\n",
    "Based on this reasoning, what is the final answer?\n",
    "Reply with ONLY: {{\"final\": <number>}}\"\"\"\n",
    "\n",
    "def parse_answer(response: str) -> Optional[int]:\n",
    "    try:\n",
    "        match = re.search(r'\\{[^}]*\"final\"\\s*:\\s*(\\d+)[^}]*\\}', response)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        numbers = re.findall(r'\\b(\\d+)\\b', response)\n",
    "        if numbers:\n",
    "            return int(numbers[-1])\n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def derive_seed(global_seed: int, problem_id: int, lam: float, ctype: str) -> int:\n",
    "    key = f\"{global_seed}|{problem_id}|lam={lam}|type={ctype}\"\n",
    "    h = hashlib.sha256(key.encode(\"utf-8\")).hexdigest()\n",
    "    return int(h[:8], 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select model to run\n",
    "#@title Select Model { run: \"auto\" }\n",
    "MODEL_CHOICE = \"Claude 4 Sonnet\" #@param [\"Claude 4 Sonnet\", \"GPT-4o\", \"Claude 3.5 Haiku\"]\n",
    "\n",
    "model_config = MODELS[MODEL_CHOICE]\n",
    "print(f'Running E2 for: {MODEL_CHOICE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*60)\n",
    "print(f'E2: CONTAMINATION TYPE EXPERIMENT - {MODEL_CHOICE}')\n",
    "print('='*60)\n",
    "\n",
    "all_results = []\n",
    "total_trials = len(problems) * len(LAMBDA_VALUES) * len(CONTAMINATION_TYPES)\n",
    "\n",
    "with tqdm(total=total_trials, desc='E2 Experiment') as pbar:\n",
    "    for ctype in CONTAMINATION_TYPES:\n",
    "        for lam in LAMBDA_VALUES:\n",
    "            print(f'\\n--- {ctype} @ λ={lam} ---')\n",
    "            \n",
    "            for problem in problems:\n",
    "                # Generate contaminated CoT\n",
    "                seed = derive_seed(GLOBAL_SEED, problem.index, lam, ctype)\n",
    "                cot_steps = generate_contaminated_cot(problem, I_FIXED, lam, ctype, seed)\n",
    "                \n",
    "                # Query model\n",
    "                prompt = create_cot_prompt(problem, cot_steps)\n",
    "                response = call_api(prompt, model_config)\n",
    "                \n",
    "                answer = parse_answer(response)\n",
    "                is_correct = (answer == problem.final_answer) if answer else False\n",
    "                \n",
    "                result = {\n",
    "                    'problem_index': problem.index,\n",
    "                    'model': MODEL_CHOICE,\n",
    "                    'contamination_type': ctype,\n",
    "                    'lam': lam,\n",
    "                    'model_answer': answer,\n",
    "                    'correct_answer': problem.final_answer,\n",
    "                    'is_correct': is_correct,\n",
    "                    'raw_output': response,\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                }\n",
    "                all_results.append(result)\n",
    "                \n",
    "                time.sleep(0.5)\n",
    "                pbar.update(1)\n",
    "            \n",
    "            # Checkpoint\n",
    "            save_json(all_results, f\"{SAVE_DIR_EXP}/checkpoints/e2_{model_config['short']}_{ctype}_lam{lam}.json\")\n",
    "\n",
    "# Save final results\n",
    "save_json(all_results, f\"{SAVE_DIR_EXP}/results/e2_results_{model_config['short']}.json\")\n",
    "print('\\n✓ E2 experiment complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.DataFrame(all_results)\n",
    "\n",
    "# Calculate accuracy by type and lambda\n",
    "acc_table = df.groupby(['contamination_type', 'lam'])['is_correct'].mean().unstack()\n",
    "\n",
    "print('='*60)\n",
    "print(f'E2 RESULTS: {MODEL_CHOICE}')\n",
    "print('='*60)\n",
    "print('\\nAccuracy by Contamination Type and λ:')\n",
    "print(acc_table.round(3))\n",
    "\n",
    "# Visualization\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "colors = {'WRONG': '#d62728', 'LOC': '#ff7f0e', 'IRR': '#2ca02c'}\n",
    "markers = {'WRONG': 'o', 'LOC': 's', 'IRR': '^'}\n",
    "\n",
    "for ctype in CONTAMINATION_TYPES:\n",
    "    data = df[df['contamination_type'] == ctype].groupby('lam')['is_correct'].mean()\n",
    "    ax.plot(data.index, data.values * 100, \n",
    "            marker=markers[ctype], color=colors[ctype],\n",
    "            linewidth=2.5, markersize=10, label=ctype)\n",
    "\n",
    "ax.set_xlabel('Corruption Rate (λ)', fontsize=13)\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=13)\n",
    "ax.set_title(f'E2: Contamination Type Analysis - {MODEL_CHOICE}', fontsize=14)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(0, 105)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{SAVE_DIR_EXP}/e2_contamination_types_{model_config['short']}.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "summary = {\n",
    "    'experiment': 'E2_Contamination_Type',\n",
    "    'model': MODEL_CHOICE,\n",
    "    'date': EXPERIMENT_DATE,\n",
    "    'n_problems': len(problems),\n",
    "    'accuracy_by_type_lambda': acc_table.to_dict()\n",
    "}\n",
    "\n",
    "# Interpretation\n",
    "print('\\n' + '='*60)\n",
    "print('INTERPRETATION')\n",
    "print('='*60)\n",
    "\n",
    "# Check if WRONG drops more than others\n",
    "wrong_drop = acc_table.loc['WRONG', 1.0] - acc_table.loc['WRONG', 0.4]\n",
    "loc_drop = acc_table.loc['LOC', 1.0] - acc_table.loc['LOC', 0.4]\n",
    "irr_drop = acc_table.loc['IRR', 1.0] - acc_table.loc['IRR', 0.4]\n",
    "\n",
    "print(f'\\nAccuracy drop (λ=0.4 → λ=1.0):')\n",
    "print(f'  WRONG: {wrong_drop:+.1%}')\n",
    "print(f'  LOC:   {loc_drop:+.1%}')\n",
    "print(f'  IRR:   {irr_drop:+.1%}')\n",
    "\n",
    "if abs(wrong_drop) > abs(loc_drop) and abs(wrong_drop) > abs(irr_drop):\n",
    "    print('\\n→ WRONG shows largest drop: dual-route may be \"verification\" type')\n",
    "elif abs(wrong_drop) < abs(irr_drop):\n",
    "    print('\\n→ IRR shows larger drop: unexpected pattern')\n",
    "else:\n",
    "    print('\\n→ Similar drops across types: dual-route may be \"reference\" type')\n",
    "\n",
    "save_json(summary, f\"{SAVE_DIR_EXP}/results/e2_summary_{model_config['short']}.json\")"
   ]
  }
 ]
}
