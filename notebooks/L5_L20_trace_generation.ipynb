{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Trace Generation for L=5 and L=20\n",
    "\n",
    "**Paper**: A2 (Cue-Dominant Extraction)\n",
    "\n",
    "**Purpose**: Generate clean reasoning traces with L=5 and L=20 steps to enable\n",
    "the Length × Cue experiment (E8) that validates the paper's title \"Length Effects\".\n",
    "\n",
    "**Design Principles** (from Sofia's guidance):\n",
    "- Use same temperature (0) and model for all traces\n",
    "- Same template structure across all L values\n",
    "- Verify final answer matches ground truth\n",
    "- Ensure \"quality difference\" doesn't confound length effect\n",
    "\n",
    "**Output**:\n",
    "- `clean_traces_I5_v3.json` (199 traces, L=5)\n",
    "- `clean_traces_I20_v3.json` (199 traces, L=20)\n",
    "\n",
    "**Expected API calls**: 199 × 2 = 398 (plus retries for answer mismatches)\n",
    "\n",
    "**Date**: 2026-01-03\n",
    "**GLOBAL_SEED**: 20251224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Google Drive Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "EXPERIMENT_NAME = 'trace_generation_L5_L20'\n",
    "EXPERIMENT_DATE = datetime.now().strftime('%Y%m%d')\n",
    "\n",
    "BASE_DIR = '/content/drive/MyDrive/CoT_Experiment'\n",
    "V3_DATA_DIR = f'{BASE_DIR}/full_experiment_v3_20251224'\n",
    "\n",
    "# Output directory for new traces\n",
    "TRACES_OUTPUT_DIR = f'{V3_DATA_DIR}/clean_traces'\n",
    "os.makedirs(TRACES_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Log directory\n",
    "LOG_DIR = f'{BASE_DIR}/{EXPERIMENT_NAME}_{EXPERIMENT_DATE}'\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "print(f'Experiment: {EXPERIMENT_NAME}')\n",
    "print(f'V3 data directory: {V3_DATA_DIR}')\n",
    "print(f'Traces output: {TRACES_OUTPUT_DIR}')\n",
    "print(f'Log directory: {LOG_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets anthropic matplotlib pandas tqdm -q\n",
    "print('Dependencies installed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import random\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from typing import List, Dict, Tuple, Optional, Any\n",
    "from dataclasses import dataclass, asdict, field\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# =============================================================================\n",
    "# Global Configuration\n",
    "# =============================================================================\n",
    "GLOBAL_SEED = 20251224\n",
    "TRACE_GEN_SEED = 20260103  # Seed for trace generation\n",
    "\n",
    "# Target lengths\n",
    "TARGET_LENGTHS = [5, 20]\n",
    "\n",
    "# API settings\n",
    "API_MAX_TOKENS_TRACE = 2048  # Longer for L=20\n",
    "API_RETRY_DELAY = 1.0\n",
    "API_RATE_LIMIT_DELAY = 0.5\n",
    "MAX_RETRIES_PER_PROBLEM = 3  # Retry if answer doesn't match\n",
    "\n",
    "print('='*70)\n",
    "print('CLEAN TRACE GENERATION: L=5 and L=20')\n",
    "print('='*70)\n",
    "print(f'  GLOBAL_SEED: {GLOBAL_SEED}')\n",
    "print(f'  TRACE_GEN_SEED: {TRACE_GEN_SEED}')\n",
    "print(f'  Target lengths: {TARGET_LENGTHS}')\n",
    "print('='*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GSM8KProblem:\n",
    "    index: int\n",
    "    question: str\n",
    "    answer_text: str\n",
    "    final_answer: int\n",
    "\n",
    "@dataclass\n",
    "class CleanTrace:\n",
    "    \"\"\"Clean reasoning trace - same structure as existing traces\"\"\"\n",
    "    problem_index: int\n",
    "    I: int  # Number of steps (L)\n",
    "    steps: List[str]  # List of step contents\n",
    "    full_text: str  # Complete trace text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derive_seed(global_seed: int, problem_id: int, L: int) -> int:\n",
    "    \"\"\"Generate deterministic seed for trace generation\"\"\"\n",
    "    key = f\"{global_seed}|trace_gen|{problem_id}|L={L}\"\n",
    "    h = hashlib.sha256(key.encode(\"utf-8\")).hexdigest()\n",
    "    return int(h[:8], 16)\n",
    "\n",
    "def save_json(data: Any, filepath: str):\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def load_json(filepath: str) -> Any:\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load problems\n",
    "problems_path = f'{V3_DATA_DIR}/problems_v3.json'\n",
    "problems_data = load_json(problems_path)\n",
    "problems = [GSM8KProblem(**p) for p in problems_data]\n",
    "print(f'Loaded {len(problems)} problems')\n",
    "\n",
    "# Verify existing L=10 traces for reference\n",
    "traces_10_path = f'{V3_DATA_DIR}/clean_traces/clean_traces_I10_v3.json'\n",
    "if os.path.exists(traces_10_path):\n",
    "    traces_10_data = load_json(traces_10_path)\n",
    "    print(f'Reference: {len(traces_10_data)} existing L=10 traces')\n",
    "else:\n",
    "    print('Warning: L=10 traces not found for reference')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. API Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "ANTHROPIC_API_KEY = getpass('Enter Anthropic API Key: ')\n",
    "print('API Key set.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "\n",
    "client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n",
    "\n",
    "def call_claude(system_prompt: str, user_prompt: str, max_tokens: int = 2048, retries: int = 3) -> str:\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            message = client.messages.create(\n",
    "                model=\"claude-sonnet-4-20250514\",\n",
    "                max_tokens=max_tokens,\n",
    "                messages=[{\"role\": \"user\", \"content\": user_prompt}],\n",
    "                system=system_prompt,\n",
    "                temperature=0  # Deterministic\n",
    "            )\n",
    "            time.sleep(API_RATE_LIMIT_DELAY)\n",
    "            return message.content[0].text\n",
    "        except Exception as e:\n",
    "            print(f'API error (attempt {attempt+1}): {e}')\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(API_RETRY_DELAY * (attempt + 1))\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "# Test API\n",
    "test_response = call_claude(\n",
    "    \"You are a helpful assistant.\",\n",
    "    'Say \"API OK\" and nothing else.',\n",
    "    max_tokens=50\n",
    ")\n",
    "print(f'API test: {test_response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Trace Generation Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRACE_SYSTEM_PROMPT = \"\"\"You are a math tutor who solves problems step by step.\n",
    "\n",
    "CRITICAL FORMAT REQUIREMENTS:\n",
    "1. Start with [[COT_START]]\n",
    "2. Write EXACTLY {L} steps, numbered as \"Step 1:\", \"Step 2:\", etc.\n",
    "3. Each step should contain ONE clear calculation or reasoning\n",
    "4. The FINAL step (Step {L}) MUST end with \"Final = X\" where X is the integer answer\n",
    "5. End with [[COT_END]]\n",
    "\n",
    "EXAMPLE FORMAT (for L=3):\n",
    "[[COT_START]]\n",
    "Step 1: First, we identify that there are 5 apples at $2 each.\n",
    "Step 2: Calculate the total: 5 × 2 = 10 dollars.\n",
    "Step 3: Therefore, the total cost is Final = 10.\n",
    "[[COT_END]]\n",
    "\n",
    "IMPORTANT:\n",
    "- You MUST have exactly {L} steps\n",
    "- The final step MUST contain \"Final = \" followed by the integer answer\n",
    "- Do NOT add any text after [[COT_END]]\n",
    "\"\"\"\n",
    "\n",
    "def create_trace_generation_prompt(problem: GSM8KProblem, L: int) -> Tuple[str, str]:\n",
    "    \"\"\"Create prompt for generating a trace with L steps\"\"\"\n",
    "    system = TRACE_SYSTEM_PROMPT.format(L=L)\n",
    "    user = f\"\"\"Solve the following math problem in EXACTLY {L} steps.\n",
    "\n",
    "Problem: {problem.question}\n",
    "\n",
    "Remember:\n",
    "- Use EXACTLY {L} steps (Step 1 through Step {L})\n",
    "- Final step must end with \"Final = <answer>\"\n",
    "- The correct answer is an integer\n",
    "\n",
    "Start your response with [[COT_START]]\"\"\"\n",
    "    return system, user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Trace Parsing and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_trace_response(response: str, expected_L: int) -> Tuple[Optional[List[str]], Optional[int], str]:\n",
    "    \"\"\"\n",
    "    Parse the generated trace response.\n",
    "    \n",
    "    Returns:\n",
    "        (steps, extracted_answer, error_message)\n",
    "        - steps: List of step contents (or None if parsing failed)\n",
    "        - extracted_answer: The final answer from the trace (or None)\n",
    "        - error_message: Description of any error (empty if success)\n",
    "    \"\"\"\n",
    "    # Extract content between markers\n",
    "    match = re.search(r'\\[\\[COT_START\\]\\](.*)\\[\\[COT_END\\]\\]', response, re.DOTALL)\n",
    "    if not match:\n",
    "        # Try without end marker\n",
    "        match = re.search(r'\\[\\[COT_START\\]\\](.*)', response, re.DOTALL)\n",
    "        if not match:\n",
    "            return None, None, \"Missing [[COT_START]] marker\"\n",
    "    \n",
    "    content = match.group(1).strip()\n",
    "    \n",
    "    # Parse steps\n",
    "    steps = []\n",
    "    for i in range(1, expected_L + 1):\n",
    "        # Pattern: \"Step N:\" followed by content until next \"Step\" or end\n",
    "        if i < expected_L:\n",
    "            pattern = rf'Step\\s*{i}\\s*:\\s*(.+?)(?=Step\\s*{i+1}\\s*:)'\n",
    "        else:\n",
    "            pattern = rf'Step\\s*{i}\\s*:\\s*(.+?)(?=\\[\\[COT_END\\]\\]|$)'\n",
    "        \n",
    "        step_match = re.search(pattern, content, re.DOTALL | re.IGNORECASE)\n",
    "        if step_match:\n",
    "            step_content = step_match.group(1).strip()\n",
    "            # Clean up newlines within step\n",
    "            step_content = ' '.join(step_content.split())\n",
    "            steps.append(step_content)\n",
    "        else:\n",
    "            return None, None, f\"Missing Step {i}\"\n",
    "    \n",
    "    if len(steps) != expected_L:\n",
    "        return None, None, f\"Expected {expected_L} steps, got {len(steps)}\"\n",
    "    \n",
    "    # Extract final answer from last step\n",
    "    final_step = steps[-1]\n",
    "    answer_match = re.search(r'Final\\s*=\\s*(-?\\d+)', final_step, re.IGNORECASE)\n",
    "    if not answer_match:\n",
    "        # Try alternative patterns\n",
    "        answer_match = re.search(r'=\\s*(-?\\d+)\\s*\\.?\\s*$', final_step)\n",
    "    \n",
    "    if answer_match:\n",
    "        extracted_answer = int(answer_match.group(1))\n",
    "    else:\n",
    "        # Try to find any number at the end\n",
    "        numbers = re.findall(r'(-?\\d+)', final_step)\n",
    "        if numbers:\n",
    "            extracted_answer = int(numbers[-1])\n",
    "        else:\n",
    "            return steps, None, \"Could not extract final answer\"\n",
    "    \n",
    "    return steps, extracted_answer, \"\"\n",
    "\n",
    "def validate_trace(steps: List[str], extracted_answer: int, correct_answer: int, L: int) -> Tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Validate a generated trace.\n",
    "    \n",
    "    Returns:\n",
    "        (is_valid, error_message)\n",
    "    \"\"\"\n",
    "    if len(steps) != L:\n",
    "        return False, f\"Wrong number of steps: {len(steps)} vs {L}\"\n",
    "    \n",
    "    if extracted_answer != correct_answer:\n",
    "        return False, f\"Answer mismatch: {extracted_answer} vs {correct_answer}\"\n",
    "    \n",
    "    # Check that final step has proper format\n",
    "    if 'final' not in steps[-1].lower() and '=' not in steps[-1]:\n",
    "        return False, \"Final step missing 'Final =' format\"\n",
    "    \n",
    "    return True, \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test parsing with a sample\n",
    "test_response = \"\"\"[[COT_START]]\n",
    "Step 1: The problem states there are 5 boxes with 3 apples each.\n",
    "Step 2: Calculate total apples: 5 × 3 = 15.\n",
    "Step 3: Each apple costs $2, so total = 15 × 2 = 30.\n",
    "Step 4: Add tax of $5: 30 + 5 = 35.\n",
    "Step 5: Therefore, the answer is Final = 35.\n",
    "[[COT_END]]\"\"\"\n",
    "\n",
    "steps, answer, error = parse_trace_response(test_response, 5)\n",
    "print(f'Parsed steps: {len(steps) if steps else 0}')\n",
    "print(f'Extracted answer: {answer}')\n",
    "print(f'Error: {error}')\n",
    "if steps:\n",
    "    for i, s in enumerate(steps):\n",
    "        print(f'  Step {i+1}: {s[:60]}...' if len(s) > 60 else f'  Step {i+1}: {s}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Trace Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_clean_trace(\n",
    "    problem: GSM8KProblem,\n",
    "    L: int,\n",
    "    max_attempts: int = MAX_RETRIES_PER_PROBLEM\n",
    ") -> Tuple[Optional[CleanTrace], Dict]:\n",
    "    \"\"\"\n",
    "    Generate a clean trace for a problem with L steps.\n",
    "    \n",
    "    Returns:\n",
    "        (trace, log_info)\n",
    "    \"\"\"\n",
    "    log_info = {\n",
    "        'problem_index': problem.index,\n",
    "        'L': L,\n",
    "        'attempts': 0,\n",
    "        'success': False,\n",
    "        'errors': []\n",
    "    }\n",
    "    \n",
    "    for attempt in range(max_attempts):\n",
    "        log_info['attempts'] = attempt + 1\n",
    "        \n",
    "        try:\n",
    "            # Generate trace\n",
    "            system, user = create_trace_generation_prompt(problem, L)\n",
    "            response = call_claude(system, user, max_tokens=API_MAX_TOKENS_TRACE)\n",
    "            \n",
    "            # Parse response\n",
    "            steps, extracted_answer, parse_error = parse_trace_response(response, L)\n",
    "            \n",
    "            if parse_error:\n",
    "                log_info['errors'].append(f\"Attempt {attempt+1}: Parse error - {parse_error}\")\n",
    "                continue\n",
    "            \n",
    "            # Validate\n",
    "            is_valid, valid_error = validate_trace(steps, extracted_answer, problem.final_answer, L)\n",
    "            \n",
    "            if not is_valid:\n",
    "                log_info['errors'].append(f\"Attempt {attempt+1}: Validation error - {valid_error}\")\n",
    "                continue\n",
    "            \n",
    "            # Success! Build the trace\n",
    "            lines = ['[[COT_START]]']\n",
    "            for i, step_content in enumerate(steps):\n",
    "                lines.append(f'Step {i+1}: {step_content}')\n",
    "            lines.append('[[COT_END]]')\n",
    "            full_text = '\\n'.join(lines)\n",
    "            \n",
    "            trace = CleanTrace(\n",
    "                problem_index=problem.index,\n",
    "                I=L,\n",
    "                steps=steps,\n",
    "                full_text=full_text\n",
    "            )\n",
    "            \n",
    "            log_info['success'] = True\n",
    "            return trace, log_info\n",
    "            \n",
    "        except Exception as e:\n",
    "            log_info['errors'].append(f\"Attempt {attempt+1}: Exception - {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return None, log_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Generate L=5 Traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*70)\n",
    "print('GENERATING L=5 TRACES')\n",
    "print('='*70)\n",
    "\n",
    "L5_traces = []\n",
    "L5_logs = []\n",
    "L5_failures = []\n",
    "\n",
    "for prob in tqdm(problems, desc='L=5 traces'):\n",
    "    trace, log_info = generate_clean_trace(prob, L=5)\n",
    "    L5_logs.append(log_info)\n",
    "    \n",
    "    if trace:\n",
    "        L5_traces.append(trace)\n",
    "    else:\n",
    "        L5_failures.append(prob.index)\n",
    "        print(f'  Failed: Problem {prob.index}')\n",
    "\n",
    "print(f'\\nL=5 Generation Complete:')\n",
    "print(f'  Success: {len(L5_traces)}/{len(problems)}')\n",
    "print(f'  Failed: {len(L5_failures)}')\n",
    "if L5_failures:\n",
    "    print(f'  Failed indices: {L5_failures[:10]}...' if len(L5_failures) > 10 else f'  Failed indices: {L5_failures}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save L=5 traces\n",
    "if L5_traces:\n",
    "    L5_output_path = f'{TRACES_OUTPUT_DIR}/clean_traces_I5_v3.json'\n",
    "    save_json([asdict(t) for t in L5_traces], L5_output_path)\n",
    "    print(f'L=5 traces saved: {L5_output_path}')\n",
    "    \n",
    "    # Save log\n",
    "    save_json(L5_logs, f'{LOG_DIR}/L5_generation_log.json')\n",
    "    print(f'L=5 log saved: {LOG_DIR}/L5_generation_log.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Generate L=20 Traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*70)\n",
    "print('GENERATING L=20 TRACES')\n",
    "print('='*70)\n",
    "\n",
    "L20_traces = []\n",
    "L20_logs = []\n",
    "L20_failures = []\n",
    "\n",
    "for prob in tqdm(problems, desc='L=20 traces'):\n",
    "    trace, log_info = generate_clean_trace(prob, L=20)\n",
    "    L20_logs.append(log_info)\n",
    "    \n",
    "    if trace:\n",
    "        L20_traces.append(trace)\n",
    "    else:\n",
    "        L20_failures.append(prob.index)\n",
    "        print(f'  Failed: Problem {prob.index}')\n",
    "\n",
    "print(f'\\nL=20 Generation Complete:')\n",
    "print(f'  Success: {len(L20_traces)}/{len(problems)}')\n",
    "print(f'  Failed: {len(L20_failures)}')\n",
    "if L20_failures:\n",
    "    print(f'  Failed indices: {L20_failures[:10]}...' if len(L20_failures) > 10 else f'  Failed indices: {L20_failures}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save L=20 traces\n",
    "if L20_traces:\n",
    "    L20_output_path = f'{TRACES_OUTPUT_DIR}/clean_traces_I20_v3.json'\n",
    "    save_json([asdict(t) for t in L20_traces], L20_output_path)\n",
    "    print(f'L=20 traces saved: {L20_output_path}')\n",
    "    \n",
    "    # Save log\n",
    "    save_json(L20_logs, f'{LOG_DIR}/L20_generation_log.json')\n",
    "    print(f'L=20 log saved: {LOG_DIR}/L20_generation_log.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*70)\n",
    "print('QUALITY CHECK')\n",
    "print('='*70)\n",
    "\n",
    "# Load existing L=10 for comparison\n",
    "if os.path.exists(traces_10_path):\n",
    "    L10_traces = [CleanTrace(**t) for t in load_json(traces_10_path)]\n",
    "    L10_indices = {t.problem_index for t in L10_traces}\n",
    "else:\n",
    "    L10_traces = []\n",
    "    L10_indices = set()\n",
    "\n",
    "L5_indices = {t.problem_index for t in L5_traces}\n",
    "L20_indices = {t.problem_index for t in L20_traces}\n",
    "\n",
    "# Find common problems across all three lengths\n",
    "common_indices = L5_indices & L10_indices & L20_indices\n",
    "\n",
    "print(f'\\nCoverage:')\n",
    "print(f'  L=5:  {len(L5_traces)} traces')\n",
    "print(f'  L=10: {len(L10_traces)} traces')\n",
    "print(f'  L=20: {len(L20_traces)} traces')\n",
    "print(f'  Common across all: {len(common_indices)} problems')\n",
    "\n",
    "# Sample check: Show a few traces\n",
    "print('\\n' + '-'*70)\n",
    "print('SAMPLE TRACES (first common problem)')\n",
    "print('-'*70)\n",
    "\n",
    "if common_indices:\n",
    "    sample_idx = min(common_indices)\n",
    "    \n",
    "    for traces, L in [(L5_traces, 5), (L10_traces, 10), (L20_traces, 20)]:\n",
    "        for t in traces:\n",
    "            if t.problem_index == sample_idx:\n",
    "                print(f'\\nL={L} (Problem {sample_idx}):')\n",
    "                print(f'  Steps: {len(t.steps)}')\n",
    "                print(f'  Step 1: {t.steps[0][:60]}...')\n",
    "                print(f'  Final step: {t.steps[-1][:60]}...')\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify answers match\n",
    "print('\\n' + '-'*70)\n",
    "print('ANSWER VERIFICATION')\n",
    "print('-'*70)\n",
    "\n",
    "prob_map = {p.index: p for p in problems}\n",
    "\n",
    "def extract_final_answer(trace: CleanTrace) -> Optional[int]:\n",
    "    final_step = trace.steps[-1]\n",
    "    match = re.search(r'Final\\s*=\\s*(-?\\d+)', final_step, re.IGNORECASE)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    match = re.search(r'=\\s*(-?\\d+)\\s*\\.?\\s*$', final_step)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    numbers = re.findall(r'(-?\\d+)', final_step)\n",
    "    if numbers:\n",
    "        return int(numbers[-1])\n",
    "    return None\n",
    "\n",
    "for traces, L in [(L5_traces, 5), (L20_traces, 20)]:\n",
    "    mismatches = 0\n",
    "    for t in traces:\n",
    "        extracted = extract_final_answer(t)\n",
    "        correct = prob_map[t.problem_index].final_answer\n",
    "        if extracted != correct:\n",
    "            mismatches += 1\n",
    "            if mismatches <= 3:\n",
    "                print(f'  L={L} Problem {t.problem_index}: extracted={extracted}, correct={correct}')\n",
    "    print(f'L={L}: {mismatches} answer mismatches out of {len(traces)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {\n",
    "    'experiment': 'trace_generation_L5_L20',\n",
    "    'date': EXPERIMENT_DATE,\n",
    "    'seeds': {\n",
    "        'global': GLOBAL_SEED,\n",
    "        'trace_gen': TRACE_GEN_SEED\n",
    "    },\n",
    "    'results': {\n",
    "        'L5': {\n",
    "            'total_problems': len(problems),\n",
    "            'successful_traces': len(L5_traces),\n",
    "            'failed_indices': L5_failures\n",
    "        },\n",
    "        'L20': {\n",
    "            'total_problems': len(problems),\n",
    "            'successful_traces': len(L20_traces),\n",
    "            'failed_indices': L20_failures\n",
    "        }\n",
    "    },\n",
    "    'common_problems': len(common_indices),\n",
    "    'output_files': {\n",
    "        'L5': f'{TRACES_OUTPUT_DIR}/clean_traces_I5_v3.json',\n",
    "        'L20': f'{TRACES_OUTPUT_DIR}/clean_traces_I20_v3.json'\n",
    "    }\n",
    "}\n",
    "\n",
    "save_json(summary, f'{LOG_DIR}/generation_summary.json')\n",
    "\n",
    "print('='*70)\n",
    "print('TRACE GENERATION COMPLETE')\n",
    "print('='*70)\n",
    "print(f'Date: {EXPERIMENT_DATE}')\n",
    "print(f'\\nResults:')\n",
    "print(f'  L=5:  {len(L5_traces)} traces generated')\n",
    "print(f'  L=20: {len(L20_traces)} traces generated')\n",
    "print(f'  Common problems: {len(common_indices)}')\n",
    "print(f'\\nOutput files:')\n",
    "print(f'  {TRACES_OUTPUT_DIR}/clean_traces_I5_v3.json')\n",
    "print(f'  {TRACES_OUTPUT_DIR}/clean_traces_I20_v3.json')\n",
    "print(f'\\nLogs: {LOG_DIR}')\n",
    "print('='*70)\n",
    "print('\\nNext: Run E8_length_cue_experiment.ipynb')"
   ]
  }
 ]
}
