{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2 GPT-4o E7: Trace-Only Extraction Test\n",
    "\n",
    "**Paper**: A2 (Cue-Dominant Extraction Explains Length Effects)\n",
    "\n",
    "**Purpose**: Test whether GPT-4o exhibits extraction behavior like Claude.\n",
    "\n",
    "**Motivation**: \n",
    "- GPT-4o E8' showed unexpected L effect even with cue present (+13.8pp)\n",
    "- GPT-4o E6 showed lower cue dependence (75.9% vs Claude's 99.5%)\n",
    "- We need to determine: Does GPT-4o extract or reason?\n",
    "\n",
    "**Conditions** (same as Claude E7):\n",
    "| Condition | Problem Statement | Final Step | Prediction |\n",
    "|-----------|-------------------|------------|------------|\n",
    "| **A: Trace-Only + Cue** | ABSENT | Has `Final = X` | High accuracy → extraction |\n",
    "| **B: Trace-Only + No Cue** | ABSENT | Cue REMOVED | Lower accuracy → fallback |\n",
    "| **C: Full Context** | Present | Has `Final = X` | High (control) |\n",
    "\n",
    "**Expected inference count**: 199 × 3 = 597\n",
    "\n",
    "**Date**: 2026-01-03\n",
    "**VERSION**: 1.0 (uses FIXED final step manipulation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Google Drive Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "EXPERIMENT_NAME = 'A2_GPT4o_E7_trace_only'\n",
    "EXPERIMENT_DATE = datetime.now().strftime('%Y%m%d')\n",
    "\n",
    "BASE_DIR = '/content/drive/MyDrive/CoT_Experiment'\n",
    "V3_DATA_DIR = f'{BASE_DIR}/full_experiment_v3_20251224'\n",
    "\n",
    "SAVE_DIR = f'{BASE_DIR}/{EXPERIMENT_NAME}_{EXPERIMENT_DATE}'\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(f'{SAVE_DIR}/results', exist_ok=True)\n",
    "\n",
    "print(f'Experiment: {EXPERIMENT_NAME}')\n",
    "print(f'V3 data directory: {V3_DATA_DIR}')\n",
    "print(f'Save directory: {SAVE_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets openai matplotlib pandas tqdm scipy -q\n",
    "print('Dependencies installed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import random\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from typing import List, Dict, Tuple, Optional, Any\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# =============================================================================\n",
    "# Global Configuration\n",
    "# =============================================================================\n",
    "GLOBAL_SEED = 20251224\n",
    "E7_SEED = 20260103\n",
    "\n",
    "# API settings\n",
    "API_MAX_TOKENS_ANSWER = 256\n",
    "API_RETRY_DELAY = 1.0\n",
    "API_RATE_LIMIT_DELAY = 0.5\n",
    "\n",
    "print('='*70)\n",
    "print('GPT-4o E7: TRACE-ONLY EXTRACTION TEST')\n",
    "print('='*70)\n",
    "print(f'  GLOBAL_SEED: {GLOBAL_SEED}')\n",
    "print(f'  E7_SEED: {E7_SEED}')\n",
    "print('='*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GSM8KProblem:\n",
    "    index: int\n",
    "    question: str\n",
    "    answer_text: str\n",
    "    final_answer: int\n",
    "\n",
    "@dataclass\n",
    "class CleanTrace:\n",
    "    problem_index: int\n",
    "    I: int\n",
    "    steps: List[str]\n",
    "    full_text: str\n",
    "\n",
    "@dataclass\n",
    "class E7Trace:\n",
    "    \"\"\"Trace for E7 experiment\"\"\"\n",
    "    problem_index: int\n",
    "    L: int\n",
    "    condition: str  # 'trace_only_cue', 'trace_only_no_cue', 'control'\n",
    "    has_problem: bool\n",
    "    has_cue: bool\n",
    "    correct_answer: int\n",
    "    steps: List[str]\n",
    "    full_text: str\n",
    "\n",
    "@dataclass\n",
    "class ExperimentResult:\n",
    "    problem_index: int\n",
    "    model: str\n",
    "    condition: str\n",
    "    has_problem: bool\n",
    "    has_cue: bool\n",
    "    correct_answer: int\n",
    "    model_answer: Optional[int]\n",
    "    is_correct: bool\n",
    "    raw_output: str\n",
    "    timestamp: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derive_seed(global_seed: int, problem_id: int, condition: str) -> int:\n",
    "    key = f\"{global_seed}|E7|{problem_id}|{condition}\"\n",
    "    h = hashlib.sha256(key.encode(\"utf-8\")).hexdigest()\n",
    "    return int(h[:8], 16)\n",
    "\n",
    "def save_json(data: Any, filepath: str):\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def load_json(filepath: str) -> Any:\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load problems\n",
    "problems_path = f'{V3_DATA_DIR}/problems_v3.json'\n",
    "problems_data = load_json(problems_path)\n",
    "problems = [GSM8KProblem(**p) for p in problems_data]\n",
    "prob_map = {p.index: p for p in problems}\n",
    "print(f'Loaded {len(problems)} problems')\n",
    "\n",
    "# Load clean traces (L=10)\n",
    "traces_path = f'{V3_DATA_DIR}/clean_traces/clean_traces_I10_v3.json'\n",
    "traces_data = load_json(traces_path)\n",
    "clean_traces = [CleanTrace(**t) for t in traces_data]\n",
    "trace_map = {t.problem_index: t for t in clean_traces}\n",
    "print(f'Loaded {len(clean_traces)} clean traces (L=10)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Final Step Manipulation (FIXED version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_final_step_with_cue(answer: int) -> str:\n",
    "    \"\"\"\n",
    "    Create a clean final step WITH the cue.\n",
    "    Replaces the ENTIRE step to avoid residual expressions.\n",
    "    \"\"\"\n",
    "    return f\"Therefore, the final answer is Final = {answer}.\"\n",
    "\n",
    "def create_final_step_without_cue() -> str:\n",
    "    \"\"\"\n",
    "    Create a clean final step WITHOUT the cue.\n",
    "    Replaces the ENTIRE step to avoid residual expressions.\n",
    "    \"\"\"\n",
    "    return \"The reasoning steps above lead to the solution. The calculation is now complete.\"\n",
    "\n",
    "# Test\n",
    "print(\"=== FINAL STEP TEMPLATES ===\")\n",
    "print(f\"With cue: {create_final_step_with_cue(70000)}\")\n",
    "print(f\"Without cue: {create_final_step_without_cue()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Trace Creation for Each Condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_condition_a_trace(clean_trace: CleanTrace, correct_answer: int) -> E7Trace:\n",
    "    \"\"\"\n",
    "    Condition A: Trace-Only + Cue Present\n",
    "    - No problem statement\n",
    "    - Trace with clean Final = X cue\n",
    "    \"\"\"\n",
    "    # Replace final step entirely\n",
    "    new_steps = clean_trace.steps[:-1] + [create_final_step_with_cue(correct_answer)]\n",
    "    \n",
    "    lines = ['[[COT_START]]']\n",
    "    for i, content in enumerate(new_steps):\n",
    "        lines.append(f'Step {i+1}: {content}')\n",
    "    lines.append('[[COT_END]]')\n",
    "    full_text = '\\n'.join(lines)\n",
    "    \n",
    "    return E7Trace(\n",
    "        problem_index=clean_trace.problem_index,\n",
    "        L=clean_trace.I,\n",
    "        condition='trace_only_cue',\n",
    "        has_problem=False,\n",
    "        has_cue=True,\n",
    "        correct_answer=correct_answer,\n",
    "        steps=new_steps,\n",
    "        full_text=full_text\n",
    "    )\n",
    "\n",
    "def create_condition_b_trace(clean_trace: CleanTrace, correct_answer: int) -> E7Trace:\n",
    "    \"\"\"\n",
    "    Condition B: Trace-Only + Cue Absent\n",
    "    - No problem statement\n",
    "    - Trace with cue COMPLETELY removed\n",
    "    \"\"\"\n",
    "    # Replace final step entirely with no-cue version\n",
    "    new_steps = clean_trace.steps[:-1] + [create_final_step_without_cue()]\n",
    "    \n",
    "    lines = ['[[COT_START]]']\n",
    "    for i, content in enumerate(new_steps):\n",
    "        lines.append(f'Step {i+1}: {content}')\n",
    "    lines.append('[[COT_END]]')\n",
    "    full_text = '\\n'.join(lines)\n",
    "    \n",
    "    return E7Trace(\n",
    "        problem_index=clean_trace.problem_index,\n",
    "        L=clean_trace.I,\n",
    "        condition='trace_only_no_cue',\n",
    "        has_problem=False,\n",
    "        has_cue=False,\n",
    "        correct_answer=correct_answer,\n",
    "        steps=new_steps,\n",
    "        full_text=full_text\n",
    "    )\n",
    "\n",
    "def create_condition_c_trace(clean_trace: CleanTrace, correct_answer: int) -> E7Trace:\n",
    "    \"\"\"\n",
    "    Condition C: Full Context (Control)\n",
    "    - Has problem statement\n",
    "    - Trace with clean Final = X cue\n",
    "    \"\"\"\n",
    "    # Replace final step entirely for consistency\n",
    "    new_steps = clean_trace.steps[:-1] + [create_final_step_with_cue(correct_answer)]\n",
    "    \n",
    "    lines = ['[[COT_START]]']\n",
    "    for i, content in enumerate(new_steps):\n",
    "        lines.append(f'Step {i+1}: {content}')\n",
    "    lines.append('[[COT_END]]')\n",
    "    full_text = '\\n'.join(lines)\n",
    "    \n",
    "    return E7Trace(\n",
    "        problem_index=clean_trace.problem_index,\n",
    "        L=clean_trace.I,\n",
    "        condition='control',\n",
    "        has_problem=True,\n",
    "        has_cue=True,\n",
    "        correct_answer=correct_answer,\n",
    "        steps=new_steps,\n",
    "        full_text=full_text\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify with a sample\n",
    "sample_trace = clean_traces[0]\n",
    "sample_problem = prob_map[sample_trace.problem_index]\n",
    "\n",
    "print(\"=== VERIFICATION ===\")\n",
    "print(f\"Original Step 10: {sample_trace.steps[-1]}\")\n",
    "print()\n",
    "\n",
    "cond_a = create_condition_a_trace(sample_trace, sample_problem.final_answer)\n",
    "print(f\"Condition A (cue present): {cond_a.steps[-1]}\")\n",
    "print(f\"  has_problem={cond_a.has_problem}, has_cue={cond_a.has_cue}\")\n",
    "print()\n",
    "\n",
    "cond_b = create_condition_b_trace(sample_trace, sample_problem.final_answer)\n",
    "print(f\"Condition B (cue absent): {cond_b.steps[-1]}\")\n",
    "print(f\"  has_problem={cond_b.has_problem}, has_cue={cond_b.has_cue}\")\n",
    "print()\n",
    "\n",
    "cond_c = create_condition_c_trace(sample_trace, sample_problem.final_answer)\n",
    "print(f\"Condition C (control): {cond_c.steps[-1]}\")\n",
    "print(f\"  has_problem={cond_c.has_problem}, has_cue={cond_c.has_cue}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. API Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "OPENAI_API_KEY = getpass('Enter OpenAI API Key: ')\n",
    "print('API Key set.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "MODEL = 'gpt-4o'\n",
    "\n",
    "def call_gpt4o(system_prompt: str, user_prompt: str, max_tokens: int = 1024, retries: int = 3) -> str:\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=MODEL,\n",
    "                max_tokens=max_tokens,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt}\n",
    "                ],\n",
    "                temperature=0\n",
    "            )\n",
    "            time.sleep(API_RATE_LIMIT_DELAY)\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f'API error (attempt {attempt+1}): {e}')\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(API_RETRY_DELAY * (attempt + 1))\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "# Test API\n",
    "test_response = call_gpt4o(\n",
    "    \"You output ONLY JSON.\",\n",
    "    'Respond with exactly: {\"test\": \"ok\"}',\n",
    "    max_tokens=50\n",
    ")\n",
    "print(f'API test: {test_response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Experiment Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a calculator that outputs ONLY JSON.\n",
    "\n",
    "CRITICAL RULES:\n",
    "1. Your output MUST start with '{'\n",
    "2. Your output MUST be exactly: {\"final\": <number>}\n",
    "3. Replace <number> with an integer\n",
    "4. Do NOT write ANY explanation\n",
    "5. ONLY output the JSON object\n",
    "\"\"\"\n",
    "\n",
    "def create_prompt_with_problem(problem: GSM8KProblem, trace_text: str) -> str:\n",
    "    \"\"\"Condition C: Full context with problem statement\"\"\"\n",
    "    return f\"\"\"Problem: {problem.question}\n",
    "\n",
    "Reasoning trace:\n",
    "{trace_text}\n",
    "\n",
    "Based on the trace, what is the final numerical answer?\n",
    "OUTPUT ONLY: {{\"final\": <number>}}\"\"\"\n",
    "\n",
    "def create_prompt_trace_only(trace_text: str) -> str:\n",
    "    \"\"\"Conditions A & B: Trace only, no problem statement\"\"\"\n",
    "    return f\"\"\"The following is a mathematical reasoning trace:\n",
    "\n",
    "{trace_text}\n",
    "\n",
    "Extract the final numerical answer from this trace.\n",
    "OUTPUT ONLY: {{\"final\": <number>}}\"\"\"\n",
    "\n",
    "def parse_model_answer(response: str) -> Optional[int]:\n",
    "    # Try standard JSON format\n",
    "    match = re.search(r'\\{\\s*\"final\"\\s*:\\s*(-?\\d+(?:\\.\\d+)?)\\s*\\}', response)\n",
    "    if match:\n",
    "        return int(round(float(match.group(1))))\n",
    "    # Try with single quotes\n",
    "    match = re.search(r\"\\{\\s*[\\\"']final[\\\"']\\s*:\\s*(-?\\d+(?:\\.\\d+)?)\\s*\\}\", response)\n",
    "    if match:\n",
    "        return int(round(float(match.group(1))))\n",
    "    # Try just the key-value\n",
    "    match = re.search(r'\"final\"\\s*:\\s*(-?\\d+(?:\\.\\d+)?)', response)\n",
    "    if match:\n",
    "        return int(round(float(match.group(1))))\n",
    "    # Fallback: find last number\n",
    "    matches = re.findall(r'(-?\\d+)', response)\n",
    "    if matches:\n",
    "        return int(matches[-1])\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_experiment(problem: GSM8KProblem, trace: E7Trace) -> ExperimentResult:\n",
    "    if trace.has_problem:\n",
    "        user_prompt = create_prompt_with_problem(problem, trace.full_text)\n",
    "    else:\n",
    "        user_prompt = create_prompt_trace_only(trace.full_text)\n",
    "    \n",
    "    response = call_gpt4o(SYSTEM_PROMPT, user_prompt, max_tokens=API_MAX_TOKENS_ANSWER)\n",
    "    model_answer = parse_model_answer(response)\n",
    "    is_correct = (model_answer == trace.correct_answer) if model_answer is not None else False\n",
    "    \n",
    "    return ExperimentResult(\n",
    "        problem_index=problem.index,\n",
    "        model=MODEL,\n",
    "        condition=trace.condition,\n",
    "        has_problem=trace.has_problem,\n",
    "        has_cue=trace.has_cue,\n",
    "        correct_answer=trace.correct_answer,\n",
    "        model_answer=model_answer,\n",
    "        is_correct=is_correct,\n",
    "        raw_output=response,\n",
    "        timestamp=datetime.now().isoformat()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*70)\n",
    "print('GPT-4o E7: TRACE-ONLY EXTRACTION TEST')\n",
    "print('='*70)\n",
    "\n",
    "all_results = []\n",
    "all_traces = []\n",
    "\n",
    "for trace in tqdm(clean_traces, desc='Running GPT-4o E7'):\n",
    "    problem = prob_map.get(trace.problem_index)\n",
    "    if problem is None:\n",
    "        continue\n",
    "    \n",
    "    correct_answer = problem.final_answer\n",
    "    \n",
    "    # Create traces\n",
    "    trace_a = create_condition_a_trace(trace, correct_answer)\n",
    "    trace_b = create_condition_b_trace(trace, correct_answer)\n",
    "    trace_c = create_condition_c_trace(trace, correct_answer)\n",
    "    \n",
    "    # Run experiments\n",
    "    result_a = run_single_experiment(problem, trace_a)\n",
    "    result_b = run_single_experiment(problem, trace_b)\n",
    "    result_c = run_single_experiment(problem, trace_c)\n",
    "    \n",
    "    all_results.extend([result_a, result_b, result_c])\n",
    "    all_traces.append({\n",
    "        'problem_index': trace.problem_index,\n",
    "        'condition_a': asdict(trace_a),\n",
    "        'condition_b': asdict(trace_b),\n",
    "        'condition_c': asdict(trace_c)\n",
    "    })\n",
    "\n",
    "print(f'\\nTotal experiments: {len(all_results)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json([asdict(r) for r in all_results], f'{SAVE_DIR}/results/A2_GPT4o_E7_results.json')\n",
    "save_json(all_traces, f'{SAVE_DIR}/results/A2_GPT4o_E7_traces.json')\n",
    "print(f'Results saved to {SAVE_DIR}/results/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([asdict(r) for r in all_results])\n",
    "\n",
    "print('='*70)\n",
    "print('GPT-4o E7 RESULTS BY CONDITION')\n",
    "print('='*70)\n",
    "\n",
    "for condition in ['trace_only_cue', 'trace_only_no_cue', 'control']:\n",
    "    cond_df = df[df['condition'] == condition]\n",
    "    n = len(cond_df)\n",
    "    acc = cond_df['is_correct'].mean()\n",
    "    has_problem = cond_df['has_problem'].iloc[0]\n",
    "    has_cue = cond_df['has_cue'].iloc[0]\n",
    "    \n",
    "    print(f'\\n{condition.upper()}:')\n",
    "    print(f'  N = {n}')\n",
    "    print(f'  Has Problem: {has_problem}, Has Cue: {has_cue}')\n",
    "    print(f'  Accuracy: {acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contingency: A vs B\n",
    "print('\\n' + '='*70)\n",
    "print('CONTINGENCY: Condition A (cue) vs B (no cue)')\n",
    "print('='*70)\n",
    "\n",
    "a_results = {r.problem_index: r.is_correct for r in all_results if r.condition == 'trace_only_cue'}\n",
    "b_results = {r.problem_index: r.is_correct for r in all_results if r.condition == 'trace_only_no_cue'}\n",
    "\n",
    "both_correct = sum(1 for idx in a_results if a_results[idx] and b_results.get(idx, False))\n",
    "only_a_correct = sum(1 for idx in a_results if a_results[idx] and not b_results.get(idx, True))\n",
    "only_b_correct = sum(1 for idx in a_results if not a_results[idx] and b_results.get(idx, False))\n",
    "both_wrong = sum(1 for idx in a_results if not a_results[idx] and not b_results.get(idx, True))\n",
    "\n",
    "print(f'Both correct: {both_correct}')\n",
    "print(f'Only A (cue) correct: {only_a_correct}')\n",
    "print(f'Only B (no cue) correct: {only_b_correct}')\n",
    "print(f'Both wrong: {both_wrong}')\n",
    "print(f'\\nAsymmetry: {only_a_correct}:{only_b_correct}')\n",
    "\n",
    "# Effect sizes\n",
    "acc_a = df[df['condition'] == 'trace_only_cue']['is_correct'].mean()\n",
    "acc_b = df[df['condition'] == 'trace_only_no_cue']['is_correct'].mean()\n",
    "acc_c = df[df['condition'] == 'control']['is_correct'].mean()\n",
    "\n",
    "print(f'\\n*** KEY EFFECTS ***')\n",
    "print(f'Cue Effect (A - B): {(acc_a - acc_b)*100:+.2f}pp')\n",
    "print(f'Problem Effect (C - A): {(acc_c - acc_a)*100:+.2f}pp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison with Claude\n",
    "print('\\n' + '='*70)\n",
    "print('COMPARISON: GPT-4o vs Claude (E7)')\n",
    "print('='*70)\n",
    "\n",
    "# Claude results (from E7 v2 FIXED)\n",
    "claude_e7 = {\n",
    "    'A': 0.9849,  # Trace-only + cue\n",
    "    'B': 0.8744,  # Trace-only + no cue\n",
    "    'C': 0.9749   # Full context\n",
    "}\n",
    "\n",
    "gpt_e7 = {\n",
    "    'A': acc_a,\n",
    "    'B': acc_b,\n",
    "    'C': acc_c\n",
    "}\n",
    "\n",
    "print(f'\\n{\"Condition\":<30} {\"Claude\":>10} {\"GPT-4o\":>10} {\"Diff\":>10}')\n",
    "print('-'*65)\n",
    "print(f'{\"A: Trace-only + Cue\":<30} {claude_e7[\"A\"]*100:>9.1f}% {gpt_e7[\"A\"]*100:>9.1f}% {(gpt_e7[\"A\"]-claude_e7[\"A\"])*100:>+9.1f}pp')\n",
    "print(f'{\"B: Trace-only + No Cue\":<30} {claude_e7[\"B\"]*100:>9.1f}% {gpt_e7[\"B\"]*100:>9.1f}% {(gpt_e7[\"B\"]-claude_e7[\"B\"])*100:>+9.1f}pp')\n",
    "print(f'{\"C: Full Context\":<30} {claude_e7[\"C\"]*100:>9.1f}% {gpt_e7[\"C\"]*100:>9.1f}% {(gpt_e7[\"C\"]-claude_e7[\"C\"])*100:>+9.1f}pp')\n",
    "print('-'*65)\n",
    "print(f'{\"Cue Effect (A-B)\":<30} {(claude_e7[\"A\"]-claude_e7[\"B\"])*100:>+9.1f}pp {(gpt_e7[\"A\"]-gpt_e7[\"B\"])*100:>+9.1f}pp')\n",
    "print(f'{\"Problem Effect (C-A)\":<30} {(claude_e7[\"C\"]-claude_e7[\"A\"])*100:>+9.1f}pp {(gpt_e7[\"C\"]-gpt_e7[\"A\"])*100:>+9.1f}pp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {\n",
    "    'experiment': 'A2_GPT4o_E7_trace_only',\n",
    "    'model': MODEL,\n",
    "    'date': EXPERIMENT_DATE,\n",
    "    'n_problems': len(clean_traces),\n",
    "    'total_inferences': len(all_results),\n",
    "    'results': {\n",
    "        'condition_a': {\n",
    "            'description': 'Trace-Only + Cue Present',\n",
    "            'has_problem': False,\n",
    "            'has_cue': True,\n",
    "            'accuracy': float(acc_a),\n",
    "            'n_correct': int(df[df['condition'] == 'trace_only_cue']['is_correct'].sum())\n",
    "        },\n",
    "        'condition_b': {\n",
    "            'description': 'Trace-Only + Cue Absent',\n",
    "            'has_problem': False,\n",
    "            'has_cue': False,\n",
    "            'accuracy': float(acc_b),\n",
    "            'n_correct': int(df[df['condition'] == 'trace_only_no_cue']['is_correct'].sum())\n",
    "        },\n",
    "        'condition_c': {\n",
    "            'description': 'Full Context (Control)',\n",
    "            'has_problem': True,\n",
    "            'has_cue': True,\n",
    "            'accuracy': float(acc_c),\n",
    "            'n_correct': int(df[df['condition'] == 'control']['is_correct'].sum())\n",
    "        }\n",
    "    },\n",
    "    'contingency_a_vs_b': {\n",
    "        'both_correct': int(both_correct),\n",
    "        'only_a_correct': int(only_a_correct),\n",
    "        'only_b_correct': int(only_b_correct),\n",
    "        'both_wrong': int(both_wrong),\n",
    "        'asymmetry': f'{only_a_correct}:{only_b_correct}'\n",
    "    },\n",
    "    'effects': {\n",
    "        'cue_effect': float(acc_a - acc_b),\n",
    "        'problem_effect': float(acc_c - acc_a)\n",
    "    },\n",
    "    'interpretation': {\n",
    "        'extraction_confirmed': bool(acc_a > 0.85),\n",
    "        'cue_critical': bool((acc_a - acc_b) > 0.10)\n",
    "    }\n",
    "}\n",
    "\n",
    "save_json(summary, f'{SAVE_DIR}/results/A2_GPT4o_E7_summary.json')\n",
    "\n",
    "print('='*70)\n",
    "print('GPT-4o E7 EXPERIMENT COMPLETE')\n",
    "print('='*70)\n",
    "print(f'Model: {MODEL}')\n",
    "print(f'Date: {EXPERIMENT_DATE}')\n",
    "print(f'Total experiments: {len(all_results)}')\n",
    "print(f'\\nCondition A (Trace + Cue): {acc_a*100:.1f}%')\n",
    "print(f'Condition B (Trace only): {acc_b*100:.1f}%')\n",
    "print(f'Condition C (Full): {acc_c*100:.1f}%')\n",
    "print(f'\\nCue Effect: {(acc_a - acc_b)*100:+.1f}pp')\n",
    "print(f'Problem Effect: {(acc_c - acc_a)*100:+.1f}pp')\n",
    "print(f'\\nFiles saved to: {SAVE_DIR}')\n",
    "print('='*70)"
   ]
  }
 ]
}
