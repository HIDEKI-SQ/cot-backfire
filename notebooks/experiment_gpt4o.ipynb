{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CoT Phase Transition Experiment - ChatGPT Replication\n",
    "\n",
    "**Version**: 1.0 (2024-12-25)\n",
    "\n",
    "**Purpose**: Validate generalizability of A* threshold across models\n",
    "\n",
    "**Design**:\n",
    "- Same 200 problems as Claude experiment (GLOBAL_SEED=20251224)\n",
    "- Same clean traces (I=10)\n",
    "- Same corruption protocol\n",
    "- λ ∈ {0.0, 0.2, 0.4, 0.6, 0.8, 1.0} + Direct condition\n",
    "\n",
    "**Goal**: Estimate A* for GPT-4 and compare with Claude's A* ≈ 0.55"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Google Drive Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "EXPERIMENT_VERSION = 'chatgpt_v1'\n",
    "EXPERIMENT_DATE = datetime.now().strftime('%Y%m%d')\n",
    "\n",
    "SAVE_DIR = '/content/drive/MyDrive/CoT_Experiment'\n",
    "SAVE_DIR_GPT = f'{SAVE_DIR}/chatgpt_experiment_{EXPERIMENT_DATE}'\n",
    "\n",
    "os.makedirs(SAVE_DIR_GPT, exist_ok=True)\n",
    "os.makedirs(f'{SAVE_DIR_GPT}/results', exist_ok=True)\n",
    "os.makedirs(f'{SAVE_DIR_GPT}/checkpoints', exist_ok=True)\n",
    "\n",
    "print(f'ChatGPT experiment save directory: {SAVE_DIR_GPT}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets openai pandas tqdm matplotlib -q\n",
    "print('Dependencies installed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import hashlib\n",
    "from typing import List, Dict, Optional, Any\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# =============================================================================\n",
    "# Configuration - MUST MATCH CLAUDE EXPERIMENT\n",
    "# =============================================================================\n",
    "GLOBAL_SEED = 20251224  # Same as Claude experiment\n",
    "N_PROBLEMS = 200\n",
    "\n",
    "# Experimental conditions\n",
    "I_FIXED = 10  # Same trace depth as Claude\n",
    "LAMBDA_VALUES = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "\n",
    "# API settings\n",
    "API_MODEL = 'gpt-4o'  # or 'gpt-4-turbo'\n",
    "API_MAX_TOKENS = 256\n",
    "API_RATE_LIMIT_DELAY = 0.3  # OpenAI is generally faster\n",
    "CHECKPOINT_EVERY = 50\n",
    "\n",
    "print('='*60)\n",
    "print('CHATGPT REPLICATION EXPERIMENT')\n",
    "print('='*60)\n",
    "print(f'  GLOBAL_SEED: {GLOBAL_SEED}')\n",
    "print(f'  N_PROBLEMS: {N_PROBLEMS}')\n",
    "print(f'  I (fixed): {I_FIXED}')\n",
    "print(f'  λ values: {LAMBDA_VALUES}')\n",
    "print(f'  Model: {API_MODEL}')\n",
    "print(f'  Total inferences: {N_PROBLEMS * (len(LAMBDA_VALUES) + 1)}')\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Structures & Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GSM8KProblem:\n",
    "    index: int\n",
    "    question: str\n",
    "    answer_text: str\n",
    "    final_answer: int\n",
    "\n",
    "@dataclass\n",
    "class ExperimentResult:\n",
    "    problem_index: int\n",
    "    condition: str  # 'direct' or 'cot'\n",
    "    I: Optional[int]\n",
    "    lam: Optional[float]\n",
    "    A_target: Optional[float]\n",
    "    model_answer: Optional[int]\n",
    "    correct_answer: int\n",
    "    is_correct: bool\n",
    "    raw_output: str\n",
    "    timestamp: str\n",
    "    model: str\n",
    "\n",
    "def extract_final_answer(answer_text: str) -> int:\n",
    "    match = re.search(r'####\\s*([\\d,]+)', answer_text)\n",
    "    if match:\n",
    "        return int(match.group(1).replace(',', ''))\n",
    "    raise ValueError(f'Could not extract final answer')\n",
    "\n",
    "def save_json(data: Any, filepath: str):\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    print(f'Saved: {filepath}')\n",
    "\n",
    "def load_json(filepath: str) -> Any:\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def derive_seed(global_seed: int, problem_id: int, I: int, lam: float) -> int:\n",
    "    key = f\"{global_seed}|{problem_id}|I={I}|lam={lam}\"\n",
    "    h = hashlib.sha256(key.encode(\"utf-8\")).hexdigest()\n",
    "    return int(h[:8], 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load GSM8K and Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('gsm8k', 'main', split='test')\n",
    "print(f'GSM8K test set loaded: {len(dataset)} problems')\n",
    "\n",
    "def select_problems(dataset, n_problems: int, seed: int) -> List[int]:\n",
    "    rng = random.Random(seed)\n",
    "    indices = list(range(len(dataset)))\n",
    "    rng.shuffle(indices)\n",
    "    return sorted(indices[:n_problems])\n",
    "\n",
    "# Same seed → Same problems as Claude experiment\n",
    "selected_indices = select_problems(dataset, N_PROBLEMS, GLOBAL_SEED)\n",
    "print(f'Selected {len(selected_indices)} problems (identical to Claude experiment)')\n",
    "\n",
    "problems = []\n",
    "for idx in selected_indices:\n",
    "    item = dataset[idx]\n",
    "    try:\n",
    "        final_ans = extract_final_answer(item['answer'])\n",
    "        prob = GSM8KProblem(\n",
    "            index=idx,\n",
    "            question=item['question'],\n",
    "            answer_text=item['answer'],\n",
    "            final_answer=final_ans\n",
    "        )\n",
    "        problems.append(prob)\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "print(f'Prepared {len(problems)} problems')\n",
    "prob_map = {p.index: p for p in problems}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Clean Traces from Claude Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Claude's clean traces\n",
    "possible_paths = [\n",
    "    f'{SAVE_DIR}/full_experiment_v3_20251224/clean_traces/clean_traces_I10_v3.json',\n",
    "    f'{SAVE_DIR}/clean_traces_I10_v3.json',\n",
    "    f'{SAVE_DIR}/pilot_v2/clean_traces_I10.json',\n",
    "]\n",
    "\n",
    "clean_traces_path = None\n",
    "for path in possible_paths:\n",
    "    if os.path.exists(path):\n",
    "        clean_traces_path = path\n",
    "        break\n",
    "\n",
    "# If not found, search recursively\n",
    "if clean_traces_path is None:\n",
    "    for root, dirs, files in os.walk(SAVE_DIR):\n",
    "        for f in files:\n",
    "            if 'clean_traces_I10' in f and f.endswith('.json'):\n",
    "                clean_traces_path = os.path.join(root, f)\n",
    "                break\n",
    "        if clean_traces_path:\n",
    "            break\n",
    "\n",
    "if clean_traces_path is None:\n",
    "    raise FileNotFoundError('Could not find clean traces from Claude experiment!')\n",
    "\n",
    "print(f'Loading clean traces from: {clean_traces_path}')\n",
    "clean_traces_data = load_json(clean_traces_path)\n",
    "print(f'Loaded {len(clean_traces_data)} clean traces')\n",
    "\n",
    "traces_dict = {t['problem_index']: t for t in clean_traces_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Corruption Logic (Same as Claude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_corrupted_steps(I: int, lam: float, seed: int) -> List[int]:\n",
    "    K = int(round(lam * I))\n",
    "    if K == 0:\n",
    "        return []\n",
    "    steps = list(range(1, I + 1))\n",
    "    rng = random.Random(seed)\n",
    "    rng.shuffle(steps)\n",
    "    return sorted(steps[:K])\n",
    "\n",
    "def assign_corruption_types(corrupted_steps: List[int], seed: int) -> Dict[int, str]:\n",
    "    K = len(corrupted_steps)\n",
    "    if K == 0:\n",
    "        return {}\n",
    "    n_irr = (K * 1) // 5\n",
    "    n_loc = (K * 2) // 5\n",
    "    n_wrong = K - n_irr - n_loc\n",
    "    if n_wrong == 0 and K > 0:\n",
    "        n_wrong = 1\n",
    "        if n_loc > 0:\n",
    "            n_loc -= 1\n",
    "        elif n_irr > 0:\n",
    "            n_irr -= 1\n",
    "    rng = random.Random(seed + 1)\n",
    "    perm = corrupted_steps[:]\n",
    "    rng.shuffle(perm)\n",
    "    type_map = {}\n",
    "    for s in perm[:n_irr]:\n",
    "        type_map[s] = \"IRR\"\n",
    "    for s in perm[n_irr:n_irr + n_loc]:\n",
    "        type_map[s] = \"LOC\"\n",
    "    for s in perm[n_irr + n_loc:]:\n",
    "        type_map[s] = \"WRONG\"\n",
    "    return type_map\n",
    "\n",
    "IRRELEVANT_TEMPLATES = [\n",
    "    \"Compute an auxiliary value: aux = {a} + {b} = {result}, but it will not be used later.\",\n",
    "]\n",
    "WRONG_CONSTRAINT_TEMPLATES = [\n",
    "    \"Fix an intermediate condition: set {var} = {wrong_value} as a given constraint for the rest of the steps.\",\n",
    "]\n",
    "\n",
    "def generate_irrelevant_step(step_num: int, seed: int) -> str:\n",
    "    rng = random.Random(seed)\n",
    "    a, b = rng.randint(2, 20), rng.randint(2, 20)\n",
    "    return IRRELEVANT_TEMPLATES[0].format(a=a, b=b, result=a+b)\n",
    "\n",
    "def generate_local_error_step(original_step: str, seed: int) -> str:\n",
    "    rng = random.Random(seed)\n",
    "    numbers = re.findall(r'\\d+', original_step)\n",
    "    if not numbers:\n",
    "        return f\"Compute t = 10 * 3 = {rng.randint(28, 32)} (using the previous values).\"\n",
    "    original_result = int(numbers[-1])\n",
    "    offset = rng.choice([-3, -2, -1, 1, 2, 3])\n",
    "    wrong_result = max(0, original_result + offset)\n",
    "    modified = re.sub(r'= (\\d+)\\.$', f'= {wrong_result}.', original_step)\n",
    "    if modified == original_step:\n",
    "        modified = re.sub(r'(\\d+)\\.$', f'{wrong_result}.', original_step)\n",
    "    return modified\n",
    "\n",
    "def generate_wrong_constraint_step(step_num: int, seed: int) -> str:\n",
    "    rng = random.Random(seed)\n",
    "    var = rng.choice(['x', 'total', 'result', 'n'])\n",
    "    wrong_value = rng.randint(10, 100)\n",
    "    return WRONG_CONSTRAINT_TEMPLATES[0].format(var=var, wrong_value=wrong_value)\n",
    "\n",
    "def apply_corruption(trace_data: dict, lam: float, seed: int) -> str:\n",
    "    steps = trace_data['steps'][:]\n",
    "    I = len(steps)\n",
    "    corrupted_steps = pick_corrupted_steps(I, lam, seed)\n",
    "    corruption_types = assign_corruption_types(corrupted_steps, seed)\n",
    "    new_steps = []\n",
    "    for i, step_content in enumerate(steps):\n",
    "        step_num = i + 1\n",
    "        if step_num in corruption_types:\n",
    "            ctype = corruption_types[step_num]\n",
    "            step_seed = seed + step_num * 1000\n",
    "            if ctype == 'IRR':\n",
    "                new_content = generate_irrelevant_step(step_num, step_seed)\n",
    "            elif ctype == 'LOC':\n",
    "                new_content = generate_local_error_step(step_content, step_seed)\n",
    "            else:\n",
    "                new_content = generate_wrong_constraint_step(step_num, step_seed)\n",
    "            new_steps.append(new_content)\n",
    "        else:\n",
    "            new_steps.append(step_content)\n",
    "    lines = ['[[COT_START]]']\n",
    "    for i, content in enumerate(new_steps):\n",
    "        lines.append(f'Step {i+1}: {content}')\n",
    "    lines.append('[[COT_END]]')\n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "print('Corruption logic defined (identical to Claude experiment).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. OpenAI API Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "OPENAI_API_KEY = getpass('Enter OpenAI API Key: ')\n",
    "print('API Key set.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "def call_gpt(system_prompt: str, user_prompt: str, max_tokens: int = 256, retries: int = 3) -> str:\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=API_MODEL,\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=0,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt}\n",
    "                ]\n",
    "            )\n",
    "            time.sleep(API_RATE_LIMIT_DELAY)\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f'API error (attempt {attempt+1}): {e}')\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(2.0 * (attempt + 1))\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "# Test API connection\n",
    "test_response = call_gpt(\n",
    "    \"You output ONLY JSON.\",\n",
    "    'Respond with exactly: {\"test\": \"ok\"}',\n",
    "    max_tokens=50\n",
    ")\n",
    "print(f'API test: {test_response}')\n",
    "print(f'Model: {API_MODEL}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Experiment Prompts (Same as Claude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same prompts as Claude experiment for fair comparison\n",
    "EXPERIMENT_SYSTEM_PROMPT = \"\"\"You are a calculator that outputs ONLY JSON.\n",
    "\n",
    "CRITICAL RULES:\n",
    "1. Your output MUST start with the character '{'\n",
    "2. Your output MUST be exactly: {\"final\": <number>}\n",
    "3. Replace <number> with an integer (the numerical answer)\n",
    "4. Do NOT write ANY explanation, reasoning, or text before or after the JSON\n",
    "5. ONLY output the JSON object, nothing else\n",
    "\n",
    "CORRECT OUTPUT EXAMPLE:\n",
    "{\"final\": 42}\n",
    "\"\"\"\n",
    "\n",
    "DIRECT_SYSTEM_PROMPT = \"\"\"You are a calculator that outputs ONLY JSON.\n",
    "\n",
    "CRITICAL RULES:\n",
    "1. Your output MUST start with the character '{'\n",
    "2. Your output MUST be exactly: {\"final\": <number>}\n",
    "3. Replace <number> with an integer (the numerical answer)\n",
    "4. Do NOT write ANY explanation, reasoning, or text before or after the JSON\n",
    "5. ONLY output the JSON object, nothing else\n",
    "\n",
    "CORRECT OUTPUT EXAMPLE:\n",
    "{\"final\": 42}\n",
    "\"\"\"\n",
    "\n",
    "def create_cot_prompt(problem: GSM8KProblem, cot_text: str) -> tuple:\n",
    "    user = f\"\"\"Problem: {problem.question}\n",
    "\n",
    "Reasoning trace (use these steps as given facts):\n",
    "{cot_text}\n",
    "\n",
    "Based on the trace above, compute the final numerical answer.\n",
    "OUTPUT ONLY: {{\"final\": <number>}}\n",
    "START YOUR RESPONSE WITH '{{'\"\"\"\n",
    "    return EXPERIMENT_SYSTEM_PROMPT, user\n",
    "\n",
    "def create_direct_prompt(problem: GSM8KProblem) -> tuple:\n",
    "    user = f\"\"\"Problem: {problem.question}\n",
    "\n",
    "Solve this problem and give the final numerical answer.\n",
    "OUTPUT ONLY: {{\"final\": <number>}}\n",
    "START YOUR RESPONSE WITH '{{'\"\"\"\n",
    "    return DIRECT_SYSTEM_PROMPT, user\n",
    "\n",
    "def parse_model_answer(response: str) -> Optional[int]:\n",
    "    match = re.search(r'\\{\\s*\"final\"\\s*:\\s*(-?\\d+(?:\\.\\d+)?)\\s*\\}', response)\n",
    "    if match:\n",
    "        return int(round(float(match.group(1))))\n",
    "    match = re.search(r\"\\{\\s*[\\\"']final[\\\"']\\s*:\\s*(-?\\d+(?:\\.\\d+)?)\\s*\\}\", response)\n",
    "    if match:\n",
    "        return int(round(float(match.group(1))))\n",
    "    match = re.search(r'\"final\"\\s*:\\s*(-?\\d+(?:\\.\\d+)?)', response)\n",
    "    if match:\n",
    "        return int(round(float(match.group(1))))\n",
    "    matches = re.findall(r'(?:^|\\s)(-?\\d+(?:\\.\\d+)?)(?:\\s|$|\\.|,)', response)\n",
    "    if matches:\n",
    "        return int(round(float(matches[-1])))\n",
    "    return None\n",
    "\n",
    "print('Prompts defined (identical to Claude experiment).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Run Direct Condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*60)\n",
    "print('RUNNING DIRECT CONDITION (No CoT)')\n",
    "print('='*60)\n",
    "\n",
    "direct_results = []\n",
    "\n",
    "for prob in tqdm(problems, desc='Direct'):\n",
    "    sys_prompt, usr_prompt = create_direct_prompt(prob)\n",
    "    response = call_gpt(sys_prompt, usr_prompt, max_tokens=API_MAX_TOKENS)\n",
    "    \n",
    "    model_answer = parse_model_answer(response)\n",
    "    is_correct = (model_answer == prob.final_answer) if model_answer is not None else False\n",
    "    \n",
    "    result = ExperimentResult(\n",
    "        problem_index=prob.index,\n",
    "        condition='direct',\n",
    "        I=None,\n",
    "        lam=None,\n",
    "        A_target=None,\n",
    "        model_answer=model_answer,\n",
    "        correct_answer=prob.final_answer,\n",
    "        is_correct=is_correct,\n",
    "        raw_output=response,\n",
    "        timestamp=datetime.now().isoformat(),\n",
    "        model=API_MODEL\n",
    "    )\n",
    "    direct_results.append(result)\n",
    "\n",
    "# Save direct results\n",
    "save_json([asdict(r) for r in direct_results], \n",
    "         f'{SAVE_DIR_GPT}/results/direct_results_gpt.json')\n",
    "\n",
    "# Report\n",
    "direct_acc = sum(r.is_correct for r in direct_results) / len(direct_results)\n",
    "print(f'\\nDirect condition accuracy: {direct_acc:.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Run CoT Conditions (λ grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*60)\n",
    "print('RUNNING COT CONDITIONS (I=10, λ grid)')\n",
    "print('='*60)\n",
    "\n",
    "cot_results = []\n",
    "\n",
    "for lam in LAMBDA_VALUES:\n",
    "    print(f'\\nλ = {lam} (A = {1-lam})')\n",
    "    \n",
    "    lam_results = []\n",
    "    for prob in tqdm(problems, desc=f'λ={lam}'):\n",
    "        if prob.index not in traces_dict:\n",
    "            continue\n",
    "        \n",
    "        trace_data = traces_dict[prob.index]\n",
    "        \n",
    "        # Apply corruption (same seed as Claude)\n",
    "        seed = derive_seed(GLOBAL_SEED, prob.index, I=I_FIXED, lam=lam)\n",
    "        corrupted_cot = apply_corruption(trace_data, lam, seed)\n",
    "        \n",
    "        # Create prompt and call API\n",
    "        sys_prompt, usr_prompt = create_cot_prompt(prob, corrupted_cot)\n",
    "        response = call_gpt(sys_prompt, usr_prompt, max_tokens=API_MAX_TOKENS)\n",
    "        \n",
    "        model_answer = parse_model_answer(response)\n",
    "        is_correct = (model_answer == prob.final_answer) if model_answer is not None else False\n",
    "        \n",
    "        result = ExperimentResult(\n",
    "            problem_index=prob.index,\n",
    "            condition='cot',\n",
    "            I=I_FIXED,\n",
    "            lam=lam,\n",
    "            A_target=1.0 - lam,\n",
    "            model_answer=model_answer,\n",
    "            correct_answer=prob.final_answer,\n",
    "            is_correct=is_correct,\n",
    "            raw_output=response,\n",
    "            timestamp=datetime.now().isoformat(),\n",
    "            model=API_MODEL\n",
    "        )\n",
    "        lam_results.append(result)\n",
    "        cot_results.append(result)\n",
    "        \n",
    "        # Checkpoint\n",
    "        if len(cot_results) % CHECKPOINT_EVERY == 0:\n",
    "            save_json([asdict(r) for r in cot_results], \n",
    "                     f'{SAVE_DIR_GPT}/checkpoints/cot_checkpoint_gpt.json')\n",
    "    \n",
    "    # Report accuracy for this λ\n",
    "    acc = sum(r.is_correct for r in lam_results) / len(lam_results) if lam_results else 0\n",
    "    print(f'  Accuracy: {acc:.1%} ({sum(r.is_correct for r in lam_results)}/{len(lam_results)})')\n",
    "\n",
    "# Save all CoT results\n",
    "save_json([asdict(r) for r in cot_results], \n",
    "         f'{SAVE_DIR_GPT}/results/cot_results_gpt.json')\n",
    "\n",
    "print(f'\\nTotal CoT results: {len(cot_results)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Analysis & Comparison with Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Combine results\n",
    "all_results = direct_results + cot_results\n",
    "\n",
    "# Create DataFrames\n",
    "direct_df = pd.DataFrame([asdict(r) for r in direct_results])\n",
    "cot_df = pd.DataFrame([asdict(r) for r in cot_results])\n",
    "\n",
    "gpt_direct_acc = direct_df['is_correct'].mean()\n",
    "\n",
    "print('='*60)\n",
    "print(f'CHATGPT ({API_MODEL}) RESULTS')\n",
    "print('='*60)\n",
    "print(f'\\nDirect (no CoT): {gpt_direct_acc:.1%}')\n",
    "print(f'\\nCoT accuracy by λ:')\n",
    "print(f'{\"λ\":>6} {\"A\":>6} {\"Accuracy\":>10} {\"vs Direct\":>12}')\n",
    "print('-' * 40)\n",
    "\n",
    "gpt_acc_by_lam = {}\n",
    "for lam in LAMBDA_VALUES:\n",
    "    lam_data = cot_df[cot_df['lam'] == lam]\n",
    "    acc = lam_data['is_correct'].mean()\n",
    "    gpt_acc_by_lam[lam] = acc\n",
    "    diff = acc - gpt_direct_acc\n",
    "    sign = '+' if diff > 0 else ''\n",
    "    print(f'{lam:>6.1f} {1-lam:>6.1f} {acc:>10.1%} {sign}{diff:>11.1%}')\n",
    "\n",
    "print('-' * 40)\n",
    "print(f'{\"Direct\":>6} {\"-\":>6} {gpt_direct_acc:>10.1%} {\"baseline\":>12}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Estimate A* for ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "from scipy.optimize import brentq\n",
    "\n",
    "# Point estimate of λ*\n",
    "lam_points = np.array(list(gpt_acc_by_lam.keys()))\n",
    "acc_points = np.array(list(gpt_acc_by_lam.values()))\n",
    "\n",
    "def estimate_lambda_crit(lam_arr, acc_arr, baseline):\n",
    "    f = interp1d(lam_arr, acc_arr - baseline, kind='linear', fill_value='extrapolate')\n",
    "    try:\n",
    "        for i in range(len(lam_arr) - 1):\n",
    "            if (acc_arr[i] - baseline) * (acc_arr[i+1] - baseline) < 0:\n",
    "                return brentq(f, lam_arr[i], lam_arr[i+1])\n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "gpt_lam_crit = estimate_lambda_crit(lam_points, acc_points, gpt_direct_acc)\n",
    "\n",
    "print('='*60)\n",
    "print('A* ESTIMATION FOR CHATGPT')\n",
    "print('='*60)\n",
    "\n",
    "if gpt_lam_crit is not None:\n",
    "    gpt_a_crit = 1 - gpt_lam_crit\n",
    "    print(f'\\nPoint estimate:')\n",
    "    print(f'  λ* = {gpt_lam_crit:.3f}')\n",
    "    print(f'  A* = {gpt_a_crit:.3f}')\n",
    "    \n",
    "    # Bootstrap CI\n",
    "    print('\\nComputing bootstrap CI...')\n",
    "    np.random.seed(GLOBAL_SEED)\n",
    "    n_bootstrap = 1000\n",
    "    lam_crit_samples = []\n",
    "    \n",
    "    problem_ids = cot_df['problem_index'].unique()\n",
    "    n_problems = len(problem_ids)\n",
    "    \n",
    "    for b in range(n_bootstrap):\n",
    "        boot_problems = np.random.choice(problem_ids, size=n_problems, replace=True)\n",
    "        boot_cot = cot_df[cot_df['problem_index'].isin(boot_problems)]\n",
    "        boot_acc = boot_cot.groupby('lam')['is_correct'].mean()\n",
    "        \n",
    "        boot_lam_crit = estimate_lambda_crit(\n",
    "            np.array(boot_acc.index),\n",
    "            np.array(boot_acc.values),\n",
    "            gpt_direct_acc\n",
    "        )\n",
    "        if boot_lam_crit is not None:\n",
    "            lam_crit_samples.append(boot_lam_crit)\n",
    "    \n",
    "    if lam_crit_samples:\n",
    "        ci_lower = np.percentile(lam_crit_samples, 2.5)\n",
    "        ci_upper = np.percentile(lam_crit_samples, 97.5)\n",
    "        print(f'\\nBootstrap results ({len(lam_crit_samples)}/{n_bootstrap} samples):')\n",
    "        print(f'  λ* = {np.mean(lam_crit_samples):.3f} [95% CI: {ci_lower:.3f}, {ci_upper:.3f}]')\n",
    "        print(f'  A* = {1-np.mean(lam_crit_samples):.3f} [95% CI: {1-ci_upper:.3f}, {1-ci_lower:.3f}]')\n",
    "else:\n",
    "    print('\\nCould not estimate λ* (no crossing detected)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Comparison Plot: Claude vs ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude results (hardcoded from previous experiment)\n",
    "claude_acc_by_lam = {\n",
    "    0.0: 0.970,\n",
    "    0.2: 0.960,\n",
    "    0.4: 0.839,\n",
    "    0.6: 0.739,\n",
    "    0.8: 0.563,\n",
    "    1.0: 0.347\n",
    "}\n",
    "claude_direct_acc = 0.759\n",
    "claude_lam_crit = 0.449\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Claude curve\n",
    "claude_lams = list(claude_acc_by_lam.keys())\n",
    "claude_accs = [claude_acc_by_lam[l] * 100 for l in claude_lams]\n",
    "ax.plot(claude_lams, claude_accs, 'o-', color='#2166ac', linewidth=2.5, \n",
    "        markersize=10, label='Claude Sonnet')\n",
    "ax.axhline(y=claude_direct_acc * 100, color='#2166ac', linestyle='--', \n",
    "          linewidth=1.5, alpha=0.7)\n",
    "\n",
    "# ChatGPT curve\n",
    "gpt_lams = list(gpt_acc_by_lam.keys())\n",
    "gpt_accs = [gpt_acc_by_lam[l] * 100 for l in gpt_lams]\n",
    "ax.plot(gpt_lams, gpt_accs, 's-', color='#d62728', linewidth=2.5,\n",
    "        markersize=10, label=f'ChatGPT ({API_MODEL})')\n",
    "ax.axhline(y=gpt_direct_acc * 100, color='#d62728', linestyle='--',\n",
    "          linewidth=1.5, alpha=0.7)\n",
    "\n",
    "# Mark A* points\n",
    "if gpt_lam_crit is not None:\n",
    "    ax.axvline(x=claude_lam_crit, color='#2166ac', linestyle=':', linewidth=2, alpha=0.7)\n",
    "    ax.axvline(x=gpt_lam_crit, color='#d62728', linestyle=':', linewidth=2, alpha=0.7)\n",
    "    \n",
    "    # Annotate\n",
    "    ax.annotate(f'Claude λ*={claude_lam_crit:.2f}', \n",
    "               xy=(claude_lam_crit, 50), fontsize=10, color='#2166ac',\n",
    "               rotation=90, va='bottom', ha='right')\n",
    "    ax.annotate(f'GPT λ*={gpt_lam_crit:.2f}',\n",
    "               xy=(gpt_lam_crit, 50), fontsize=10, color='#d62728',\n",
    "               rotation=90, va='bottom', ha='left')\n",
    "\n",
    "ax.set_xlabel('Corruption Rate (λ)', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=13, fontweight='bold')\n",
    "ax.set_title('CoT Collapse: Claude vs ChatGPT', fontsize=14, fontweight='bold')\n",
    "ax.set_xlim(-0.05, 1.05)\n",
    "ax.set_ylim(25, 105)\n",
    "ax.legend(loc='lower left', fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add A axis\n",
    "ax2 = ax.twiny()\n",
    "ax2.set_xlim(ax.get_xlim())\n",
    "ax2.set_xticks([0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "ax2.set_xticklabels(['1.0', '0.8', '0.6', '0.4', '0.2', '0.0'])\n",
    "ax2.set_xlabel('Alignment (A = 1 - λ)', fontsize=13, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{SAVE_DIR_GPT}/comparison_claude_gpt.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f'Saved: {SAVE_DIR_GPT}/comparison_claude_gpt.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*70)\n",
    "print('EXPERIMENT SUMMARY: CLAUDE vs CHATGPT')\n",
    "print('='*70)\n",
    "\n",
    "print(f\"\"\"\n",
    "┌────────────────────────────────────────────────────────────────────┐\n",
    "│                     MODEL COMPARISON                               │\n",
    "├────────────────────────────────────────────────────────────────────┤\n",
    "│ Metric              │ Claude Sonnet    │ ChatGPT ({API_MODEL:10})  │\n",
    "├────────────────────────────────────────────────────────────────────┤\n",
    "│ Direct accuracy     │ {claude_direct_acc:>10.1%}       │ {gpt_direct_acc:>10.1%}            │\n",
    "│ Clean CoT (λ=0)     │ {claude_acc_by_lam[0.0]:>10.1%}       │ {gpt_acc_by_lam[0.0]:>10.1%}            │\n",
    "│ λ=0.4               │ {claude_acc_by_lam[0.4]:>10.1%}       │ {gpt_acc_by_lam[0.4]:>10.1%}            │\n",
    "│ λ=0.8               │ {claude_acc_by_lam[0.8]:>10.1%}       │ {gpt_acc_by_lam[0.8]:>10.1%}            │\n",
    "│ Full corrupt (λ=1)  │ {claude_acc_by_lam[1.0]:>10.1%}       │ {gpt_acc_by_lam[1.0]:>10.1%}            │\n",
    "├────────────────────────────────────────────────────────────────────┤\n",
    "│ λ* (critical)       │ {claude_lam_crit:>10.3f}       │ {gpt_lam_crit if gpt_lam_crit else 'N/A':>10}            │\n",
    "│ A* (critical)       │ {1-claude_lam_crit:>10.3f}       │ {1-gpt_lam_crit if gpt_lam_crit else 'N/A':>10}            │\n",
    "└────────────────────────────────────────────────────────────────────┘\n",
    "\"\"\")\n",
    "\n",
    "if gpt_lam_crit is not None:\n",
    "    delta = abs(gpt_lam_crit - claude_lam_crit)\n",
    "    print(f'\\nΔλ* = {delta:.3f} (difference between models)')\n",
    "    print(f'ΔA* = {delta:.3f}')\n",
    "    \n",
    "    if delta < 0.1:\n",
    "        print('\\n→ A* is CONSISTENT across models (Δ < 0.1)')\n",
    "    else:\n",
    "        print('\\n→ A* shows MODEL-SPECIFIC variation')\n",
    "\n",
    "print(f'\\nResults saved to: {SAVE_DIR_GPT}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final combined results\n",
    "summary = {\n",
    "    'experiment_date': EXPERIMENT_DATE,\n",
    "    'model': API_MODEL,\n",
    "    'n_problems': len(problems),\n",
    "    'direct_accuracy': gpt_direct_acc,\n",
    "    'accuracy_by_lambda': gpt_acc_by_lam,\n",
    "    'lambda_crit': gpt_lam_crit,\n",
    "    'a_crit': 1 - gpt_lam_crit if gpt_lam_crit else None,\n",
    "    'comparison_with_claude': {\n",
    "        'claude_lambda_crit': claude_lam_crit,\n",
    "        'claude_a_crit': 1 - claude_lam_crit,\n",
    "        'delta_lambda_crit': abs(gpt_lam_crit - claude_lam_crit) if gpt_lam_crit else None\n",
    "    }\n",
    "}\n",
    "\n",
    "save_json(summary, f'{SAVE_DIR_GPT}/results/summary_gpt.json')\n",
    "print('\\nExperiment complete!')"
   ]
  }
 ]
}
