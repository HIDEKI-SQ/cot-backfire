{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CoT Phase Transition Experiment v3 - Direct Condition (No CoT)\n",
    "\n",
    "**Version**: 3.0 (2024-12-24)\n",
    "\n",
    "**Purpose**: Baseline experiment without Chain-of-Thought reasoning.\n",
    "\n",
    "**Design**:\n",
    "- Same 200 problems as Experiment 1\n",
    "- NO CoT provided - model answers directly\n",
    "- Measures baseline accuracy without reasoning traces\n",
    "\n",
    "**Why This Matters**:\n",
    "- Establishes whether CoT actually helps (Clean vs Direct)\n",
    "- Shows at what corruption level CoT becomes worse than no CoT\n",
    "- Critical for the \"collapse\" narrative in the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Google Drive Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "EXPERIMENT_VERSION = 'v3'\n",
    "EXPERIMENT_DATE = datetime.now().strftime('%Y%m%d')\n",
    "\n",
    "SAVE_DIR = '/content/drive/MyDrive/CoT_Experiment'\n",
    "SAVE_DIR_DIRECT = f'{SAVE_DIR}/direct_experiment_{EXPERIMENT_VERSION}_{EXPERIMENT_DATE}'\n",
    "\n",
    "os.makedirs(SAVE_DIR_DIRECT, exist_ok=True)\n",
    "os.makedirs(f'{SAVE_DIR_DIRECT}/results', exist_ok=True)\n",
    "\n",
    "print(f'Direct experiment save directory: {SAVE_DIR_DIRECT}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets anthropic pandas tqdm -q\n",
    "print('Dependencies installed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "from typing import List, Dict, Optional, Any\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# =============================================================================\n",
    "# Configuration - MUST MATCH EXPERIMENT 1\n",
    "# =============================================================================\n",
    "GLOBAL_SEED = 20251224  # Same as Experiment 1\n",
    "N_PROBLEMS = 200        # Same as Experiment 1\n",
    "\n",
    "# API settings\n",
    "API_MAX_TOKENS_ANSWER = 256\n",
    "API_RATE_LIMIT_DELAY = 0.5\n",
    "\n",
    "print('='*60)\n",
    "print('DIRECT CONDITION EXPERIMENT')\n",
    "print('='*60)\n",
    "print(f'  GLOBAL_SEED: {GLOBAL_SEED}')\n",
    "print(f'  N_PROBLEMS: {N_PROBLEMS}')\n",
    "print(f'  Condition: Direct (No CoT)')\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GSM8KProblem:\n",
    "    index: int\n",
    "    question: str\n",
    "    answer_text: str\n",
    "    final_answer: int\n",
    "\n",
    "@dataclass\n",
    "class DirectResult:\n",
    "    problem_index: int\n",
    "    condition: str  # Always \"direct\"\n",
    "    model_answer: Optional[int]\n",
    "    correct_answer: int\n",
    "    is_correct: bool\n",
    "    raw_output: str\n",
    "    timestamp: str\n",
    "\n",
    "def extract_final_answer(answer_text: str) -> int:\n",
    "    match = re.search(r'####\\s*([\\d,]+)', answer_text)\n",
    "    if match:\n",
    "        return int(match.group(1).replace(',', ''))\n",
    "    raise ValueError(f'Could not extract final answer')\n",
    "\n",
    "def save_json(data: Any, filepath: str):\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    print(f'Saved: {filepath}')\n",
    "\n",
    "def load_json(filepath: str) -> Any:\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load GSM8K and Select Problems (Same as Experiment 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('gsm8k', 'main', split='test')\n",
    "print(f'GSM8K test set loaded: {len(dataset)} problems')\n",
    "\n",
    "def select_problems(dataset, n_problems: int, seed: int) -> List[int]:\n",
    "    rng = random.Random(seed)\n",
    "    indices = list(range(len(dataset)))\n",
    "    rng.shuffle(indices)\n",
    "    selected = sorted(indices[:n_problems])\n",
    "    return selected\n",
    "\n",
    "# IMPORTANT: Same seed as Experiment 1 → Same problems\n",
    "selected_indices = select_problems(dataset, N_PROBLEMS, GLOBAL_SEED)\n",
    "print(f'Selected {len(selected_indices)} problems (same as Experiment 1)')\n",
    "\n",
    "# Create problem objects\n",
    "problems = []\n",
    "for idx in selected_indices:\n",
    "    item = dataset[idx]\n",
    "    try:\n",
    "        final_ans = extract_final_answer(item['answer'])\n",
    "        prob = GSM8KProblem(\n",
    "            index=idx,\n",
    "            question=item['question'],\n",
    "            answer_text=item['answer'],\n",
    "            final_answer=final_ans\n",
    "        )\n",
    "        problems.append(prob)\n",
    "    except ValueError as e:\n",
    "        print(f'Warning: Could not extract answer for index {idx}')\n",
    "\n",
    "print(f'Prepared {len(problems)} problems')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. API Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "ANTHROPIC_API_KEY = getpass('Enter Anthropic API Key: ')\n",
    "print('API Key set.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "\n",
    "client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n",
    "\n",
    "def call_claude(system_prompt: str, user_prompt: str, max_tokens: int = 256, retries: int = 3) -> str:\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            message = client.messages.create(\n",
    "                model=\"claude-sonnet-4-20250514\",\n",
    "                max_tokens=max_tokens,\n",
    "                messages=[{\"role\": \"user\", \"content\": user_prompt}],\n",
    "                system=system_prompt,\n",
    "                temperature=0\n",
    "            )\n",
    "            time.sleep(API_RATE_LIMIT_DELAY)\n",
    "            return message.content[0].text\n",
    "        except Exception as e:\n",
    "            print(f'API error (attempt {attempt+1}): {e}')\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(1.0 * (attempt + 1))\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "# Test\n",
    "test_response = call_claude(\n",
    "    \"You output ONLY JSON.\",\n",
    "    'Respond with exactly: {\"test\": \"ok\"}',\n",
    "    max_tokens=50\n",
    ")\n",
    "print(f'API test: {test_response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Direct Condition Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt - Same JSON format as Experiment 1 for consistency\n",
    "DIRECT_SYSTEM_PROMPT = \"\"\"You are a calculator that outputs ONLY JSON.\n",
    "\n",
    "CRITICAL RULES:\n",
    "1. Your output MUST start with the character '{'\n",
    "2. Your output MUST be exactly: {\"final\": <number>}\n",
    "3. Replace <number> with an integer (the numerical answer)\n",
    "4. Do NOT write ANY explanation, reasoning, or text before or after the JSON\n",
    "5. Do NOT show your work or thinking\n",
    "6. ONLY output the JSON object, nothing else\n",
    "\n",
    "CORRECT OUTPUT EXAMPLE:\n",
    "{\"final\": 42}\n",
    "\"\"\"\n",
    "\n",
    "def create_direct_prompt(problem: GSM8KProblem) -> tuple:\n",
    "    \"\"\"Create prompt for Direct condition (no CoT)\"\"\"\n",
    "    user = f\"\"\"Problem: {problem.question}\n",
    "\n",
    "Solve this problem and give the final numerical answer.\n",
    "OUTPUT ONLY: {{\"final\": <number>}}\n",
    "START YOUR RESPONSE WITH '{{'\"\"\"\n",
    "    \n",
    "    return DIRECT_SYSTEM_PROMPT, user\n",
    "\n",
    "# Test\n",
    "test_sys, test_usr = create_direct_prompt(problems[0])\n",
    "print('--- Direct Prompt Example ---')\n",
    "print(test_usr[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Answer Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_model_answer(response: str) -> Optional[int]:\n",
    "    \"\"\"Extract numerical answer from model response\"\"\"\n",
    "    # Pattern 1: {\"final\": 123}\n",
    "    match = re.search(r'\\{\\s*\"final\"\\s*:\\s*(-?\\d+(?:\\.\\d+)?)\\s*\\}', response)\n",
    "    if match:\n",
    "        return int(round(float(match.group(1))))\n",
    "    \n",
    "    # Pattern 2: {'final': 123}\n",
    "    match = re.search(r\"\\{\\s*[\\\"']final[\\\"']\\s*:\\s*(-?\\d+(?:\\.\\d+)?)\\s*\\}\", response)\n",
    "    if match:\n",
    "        return int(round(float(match.group(1))))\n",
    "    \n",
    "    # Pattern 3: \"final\": 123\n",
    "    match = re.search(r'\"final\"\\s*:\\s*(-?\\d+(?:\\.\\d+)?)', response)\n",
    "    if match:\n",
    "        return int(round(float(match.group(1))))\n",
    "    \n",
    "    # Pattern 4: Last number in response\n",
    "    matches = re.findall(r'(?:^|\\s)(-?\\d+(?:\\.\\d+)?)(?:\\s|$|\\.|,)', response)\n",
    "    if matches:\n",
    "        return int(round(float(matches[-1])))\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Test\n",
    "test_cases = ['{\"final\": 300}', '{\"final\": 42.5}', 'The answer is 100.']\n",
    "for tc in test_cases:\n",
    "    print(f'  \"{tc}\" → {parse_model_answer(tc)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run Direct Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_direct_experiment(problem: GSM8KProblem) -> DirectResult:\n",
    "    \"\"\"Run single Direct condition experiment\"\"\"\n",
    "    sys_prompt, usr_prompt = create_direct_prompt(problem)\n",
    "    response = call_claude(sys_prompt, usr_prompt, max_tokens=API_MAX_TOKENS_ANSWER)\n",
    "    \n",
    "    model_answer = parse_model_answer(response)\n",
    "    is_correct = (model_answer == problem.final_answer) if model_answer is not None else False\n",
    "    \n",
    "    return DirectResult(\n",
    "        problem_index=problem.index,\n",
    "        condition=\"direct\",\n",
    "        model_answer=model_answer,\n",
    "        correct_answer=problem.final_answer,\n",
    "        is_correct=is_correct,\n",
    "        raw_output=response,\n",
    "        timestamp=datetime.now().isoformat()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pilot test (5 problems)\n",
    "print('='*60)\n",
    "print('PILOT: Testing with 5 problems')\n",
    "print('='*60)\n",
    "\n",
    "pilot_results = []\n",
    "for prob in tqdm(problems[:5], desc='Pilot'):\n",
    "    result = run_direct_experiment(prob)\n",
    "    pilot_results.append(result)\n",
    "    status = '✓' if result.is_correct else '✗'\n",
    "    print(f'  Problem {prob.index}: {status} (model={result.model_answer}, correct={result.correct_answer})')\n",
    "\n",
    "pilot_acc = sum(r.is_correct for r in pilot_results) / len(pilot_results)\n",
    "print(f'\\nPilot accuracy: {pilot_acc:.1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full experiment\n",
    "print('='*60)\n",
    "print('FULL DIRECT EXPERIMENT')\n",
    "print('='*60)\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for prob in tqdm(problems, desc='Direct condition'):\n",
    "    result = run_direct_experiment(prob)\n",
    "    all_results.append(result)\n",
    "\n",
    "# Save results\n",
    "save_json([asdict(r) for r in all_results], f'{SAVE_DIR_DIRECT}/results/direct_results_v3.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "df = pd.DataFrame([asdict(r) for r in all_results])\n",
    "\n",
    "# Calculate accuracy\n",
    "total = len(df)\n",
    "correct = df['is_correct'].sum()\n",
    "accuracy = correct / total\n",
    "\n",
    "print('='*60)\n",
    "print('DIRECT CONDITION RESULTS')\n",
    "print('='*60)\n",
    "print(f'Total problems: {total}')\n",
    "print(f'Correct: {correct}')\n",
    "print(f'Accuracy: {accuracy:.1%}')\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse failures analysis\n",
    "parse_failures = df[df['model_answer'].isna()]\n",
    "print(f'Parse failures: {len(parse_failures)} ({len(parse_failures)/total:.1%})')\n",
    "\n",
    "if len(parse_failures) > 0:\n",
    "    print('\\nSample parse failures:')\n",
    "    for _, row in parse_failures.head(3).iterrows():\n",
    "        print(f'  Problem {row[\"problem_index\"]}: {row[\"raw_output\"][:100]}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Comparison with Experiment 1 (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load Experiment 1 results for comparison\n",
    "exp1_dirs = [d for d in os.listdir(SAVE_DIR) if d.startswith('full_experiment_v3')]\n",
    "\n",
    "if exp1_dirs:\n",
    "    exp1_dir = f'{SAVE_DIR}/{sorted(exp1_dirs)[-1]}'\n",
    "    exp1_results_path = f'{exp1_dir}/results/results_full_v3.json'\n",
    "    \n",
    "    if os.path.exists(exp1_results_path):\n",
    "        exp1_data = load_json(exp1_results_path)\n",
    "        exp1_df = pd.DataFrame(exp1_data)\n",
    "        \n",
    "        print('='*60)\n",
    "        print('COMPARISON: Direct vs CoT Conditions')\n",
    "        print('='*60)\n",
    "        \n",
    "        # Direct accuracy\n",
    "        direct_acc = accuracy\n",
    "        print(f'Direct (No CoT):     {direct_acc:.1%}')\n",
    "        \n",
    "        # CoT accuracies by λ\n",
    "        if 'lam' in exp1_df.columns:\n",
    "            for lam in sorted(exp1_df['lam'].unique()):\n",
    "                lam_acc = exp1_df[exp1_df['lam'] == lam]['is_correct'].mean()\n",
    "                A = 1 - lam\n",
    "                comparison = '>' if lam_acc > direct_acc else ('<' if lam_acc < direct_acc else '=')\n",
    "                print(f'CoT (A={A:.1f}, λ={lam}): {lam_acc:.1%} {comparison} Direct')\n",
    "        \n",
    "        print('='*60)\n",
    "    else:\n",
    "        print('Experiment 1 results not found yet.')\n",
    "else:\n",
    "    print('Experiment 1 directory not found. Run comparison after Experiment 1 completes.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*60)\n",
    "print('DIRECT EXPERIMENT SUMMARY')\n",
    "print('='*60)\n",
    "print(f'Version: {EXPERIMENT_VERSION}')\n",
    "print(f'Date: {EXPERIMENT_DATE}')\n",
    "print(f'Problems: {len(all_results)}')\n",
    "print(f'\\nDirect Condition Accuracy: {accuracy:.1%}')\n",
    "print(f'\\nThis serves as the baseline for CoT effectiveness.')\n",
    "print(f'If CoT (A=1.0) > Direct: CoT helps')\n",
    "print(f'If CoT (A=x) < Direct: CoT at that corruption level hurts')\n",
    "print(f'\\nResults saved to: {SAVE_DIR_DIRECT}')\n",
    "print('='*60)"
   ]
  }
 ]
}
