{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E6: Conflicting Cue Experiment (FIXED VERSION)\n",
    "\n",
    "**Paper**: A2 (Cue-Dominant Extraction Explains Length Effects)\n",
    "\n",
    "**CRITICAL FIX**: Previous version had a bug where `Final = expr = value` format\n",
    "was only partially replaced, leaving residual expressions. This version replaces\n",
    "the ENTIRE final step content to ensure clean cue manipulation.\n",
    "\n",
    "**Purpose**: Directly test whether the model prioritizes the answer CUE over the reasoning CHAIN.\n",
    "\n",
    "**Conditions**:\n",
    "| Condition | Step 1-9 | Step 10 (Cue) | Prediction |\n",
    "|-----------|----------|---------------|------------|\n",
    "| **A: Wrong Cue Injection** | Clean (correct reasoning) | Wrong final answer | **WRONG** (if cue-dominant) |\n",
    "| **B: Correct Cue Only** | Corrupted (c=0.8) | Correct final answer | **CORRECT** (if cue-dominant) |\n",
    "| **C: Control** | Clean | Correct final answer | CORRECT |\n",
    "\n",
    "**Expected inference count**: 199 × 3 = 597\n",
    "\n",
    "**Date**: 2026-01-03\n",
    "**GLOBAL_SEED**: 20251224\n",
    "**E6_SEED**: 20260103\n",
    "**VERSION**: 2.0 (FIXED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Google Drive Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "EXPERIMENT_NAME = 'E6_conflicting_cue_v2'\n",
    "EXPERIMENT_DATE = datetime.now().strftime('%Y%m%d')\n",
    "\n",
    "BASE_DIR = '/content/drive/MyDrive/CoT_Experiment'\n",
    "V3_DATA_DIR = f'{BASE_DIR}/full_experiment_v3_20251224'\n",
    "\n",
    "SAVE_DIR = f'{BASE_DIR}/{EXPERIMENT_NAME}_{EXPERIMENT_DATE}'\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(f'{SAVE_DIR}/results', exist_ok=True)\n",
    "\n",
    "print(f'Experiment: {EXPERIMENT_NAME}')\n",
    "print(f'V3 data directory: {V3_DATA_DIR}')\n",
    "print(f'Save directory: {SAVE_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets anthropic matplotlib pandas tqdm scipy -q\n",
    "print('Dependencies installed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import random\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from typing import List, Dict, Tuple, Optional, Any\n",
    "from dataclasses import dataclass, asdict, field\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# =============================================================================\n",
    "# Global Configuration\n",
    "# =============================================================================\n",
    "GLOBAL_SEED = 20251224\n",
    "E6_SEED = 20260103\n",
    "\n",
    "# Corruption settings for Condition B\n",
    "CORRUPTION_RATE = 0.8  # c = 0.8 for corrupted reasoning\n",
    "CORRUPTION_RATIO = {'IRR': 1, 'LOC': 2, 'WRONG': 2}\n",
    "\n",
    "# API settings\n",
    "API_MAX_TOKENS_ANSWER = 256\n",
    "API_RETRY_DELAY = 1.0\n",
    "API_RATE_LIMIT_DELAY = 0.5\n",
    "\n",
    "print('='*70)\n",
    "print('E6: CONFLICTING CUE EXPERIMENT (FIXED VERSION 2.0)')\n",
    "print('='*70)\n",
    "print(f'  GLOBAL_SEED: {GLOBAL_SEED}')\n",
    "print(f'  E6_SEED: {E6_SEED}')\n",
    "print(f'  Corruption rate (Condition B): {CORRUPTION_RATE}')\n",
    "print('='*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GSM8KProblem:\n",
    "    index: int\n",
    "    question: str\n",
    "    answer_text: str\n",
    "    final_answer: int\n",
    "\n",
    "@dataclass\n",
    "class CleanTrace:\n",
    "    problem_index: int\n",
    "    I: int  # Number of steps (L)\n",
    "    steps: List[str]\n",
    "    full_text: str\n",
    "\n",
    "@dataclass\n",
    "class ManipulatedTrace:\n",
    "    \"\"\"Trace with manipulated cue and/or reasoning\"\"\"\n",
    "    problem_index: int\n",
    "    L: int\n",
    "    condition: str  # 'wrong_cue', 'correct_cue_only', 'control'\n",
    "    reasoning_status: str  # 'clean' or 'corrupted'\n",
    "    cue_status: str  # 'correct' or 'wrong'\n",
    "    cue_answer: int  # The answer shown in the final step\n",
    "    correct_answer: int  # The actual correct answer\n",
    "    steps: List[str]\n",
    "    full_text: str\n",
    "    seed: int\n",
    "\n",
    "@dataclass\n",
    "class ExperimentResult:\n",
    "    problem_index: int\n",
    "    condition: str\n",
    "    cue_answer: int\n",
    "    correct_answer: int\n",
    "    model_answer: Optional[int]\n",
    "    is_correct: bool\n",
    "    followed_cue: bool\n",
    "    raw_output: str\n",
    "    timestamp: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derive_seed(global_seed: int, problem_id: int, condition: str) -> int:\n",
    "    key = f\"{global_seed}|E6|{problem_id}|{condition}\"\n",
    "    h = hashlib.sha256(key.encode(\"utf-8\")).hexdigest()\n",
    "    return int(h[:8], 16)\n",
    "\n",
    "def save_json(data: Any, filepath: str):\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def load_json(filepath: str) -> Any:\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load problems\n",
    "problems_path = f'{V3_DATA_DIR}/problems_v3.json'\n",
    "problems_data = load_json(problems_path)\n",
    "problems = [GSM8KProblem(**p) for p in problems_data]\n",
    "prob_map = {p.index: p for p in problems}\n",
    "print(f'Loaded {len(problems)} problems')\n",
    "\n",
    "# Load clean traces (L=10)\n",
    "traces_path = f'{V3_DATA_DIR}/clean_traces/clean_traces_I10_v3.json'\n",
    "traces_data = load_json(traces_path)\n",
    "clean_traces = [CleanTrace(**t) for t in traces_data]\n",
    "trace_map = {t.problem_index: t for t in clean_traces}\n",
    "print(f'Loaded {len(clean_traces)} clean traces (L=10)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Corruption Logic (for Condition B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_corrupted_steps(L: int, c: float, seed: int) -> List[int]:\n",
    "    \"\"\"Select steps to corrupt (excluding the final step which keeps correct cue)\"\"\"\n",
    "    K = int(round(c * (L - 1)))  # Exclude last step\n",
    "    if K == 0:\n",
    "        return []\n",
    "    steps = list(range(1, L))  # Steps 1 to L-1\n",
    "    rng = random.Random(seed)\n",
    "    rng.shuffle(steps)\n",
    "    return sorted(steps[:K])\n",
    "\n",
    "def assign_corruption_types(corrupted_steps: List[int], seed: int) -> Dict[int, str]:\n",
    "    \"\"\"Assign corruption types with ratio IRR:LOC:WRONG = 1:2:2\"\"\"\n",
    "    K = len(corrupted_steps)\n",
    "    if K == 0:\n",
    "        return {}\n",
    "    n_irr = (K * 1) // 5\n",
    "    n_loc = (K * 2) // 5\n",
    "    n_wrong = K - n_irr - n_loc\n",
    "    if n_wrong == 0 and K > 0:\n",
    "        n_wrong = 1\n",
    "        if n_loc > 0:\n",
    "            n_loc -= 1\n",
    "        elif n_irr > 0:\n",
    "            n_irr -= 1\n",
    "    rng = random.Random(seed + 1)\n",
    "    perm = corrupted_steps[:]\n",
    "    rng.shuffle(perm)\n",
    "    type_map = {}\n",
    "    for s in perm[:n_irr]:\n",
    "        type_map[s] = \"IRR\"\n",
    "    for s in perm[n_irr:n_irr + n_loc]:\n",
    "        type_map[s] = \"LOC\"\n",
    "    for s in perm[n_irr + n_loc:]:\n",
    "        type_map[s] = \"WRONG\"\n",
    "    return type_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Corruption Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IRRELEVANT_TEMPLATES = [\n",
    "    \"Compute an auxiliary value: aux = {a} + {b} = {result}, but it will not be used later.\",\n",
    "    \"Compute a side quantity: aux = {a} * 2 = {result}, unrelated to the final result.\",\n",
    "    \"Note that we can also compute aux = {a} - {b} = {result}, though this is not needed.\",\n",
    "]\n",
    "\n",
    "WRONG_CONSTRAINT_TEMPLATES = [\n",
    "    \"Fix an intermediate condition: set {var} = {wrong_value} as a given constraint for the rest of the steps.\",\n",
    "    \"Assume the total is {var} = {wrong_value} and proceed using this fixed value.\",\n",
    "]\n",
    "\n",
    "def generate_irrelevant_step(step_num: int, seed: int) -> str:\n",
    "    rng = random.Random(seed)\n",
    "    a = rng.randint(2, 20)\n",
    "    b = rng.randint(2, 20)\n",
    "    template = rng.choice(IRRELEVANT_TEMPLATES)\n",
    "    if '+' in template:\n",
    "        result = a + b\n",
    "    elif '*' in template:\n",
    "        result = a * 2\n",
    "    else:\n",
    "        result = a - b\n",
    "    return template.format(a=a, b=b, result=result)\n",
    "\n",
    "def generate_local_error_step(original_step: str, seed: int) -> str:\n",
    "    rng = random.Random(seed)\n",
    "    numbers = re.findall(r'\\d+', original_step)\n",
    "    if not numbers:\n",
    "        return f\"Compute t = 10 * 3 = {rng.randint(28, 32)} (using the previous values).\"\n",
    "    original_result = int(numbers[-1])\n",
    "    offset = rng.choice([-3, -2, -1, 1, 2, 3])\n",
    "    wrong_result = max(0, original_result + offset)\n",
    "    modified = re.sub(r'= (\\d+)\\.$', f'= {wrong_result}.', original_step)\n",
    "    if modified == original_step:\n",
    "        modified = re.sub(r'(\\d+)\\.$', f'{wrong_result}.', original_step)\n",
    "    return modified\n",
    "\n",
    "def generate_wrong_constraint_step(step_num: int, seed: int) -> str:\n",
    "    rng = random.Random(seed)\n",
    "    var = rng.choice(['x', 'total', 'result', 'n'])\n",
    "    wrong_value = rng.randint(10, 100)\n",
    "    template = rng.choice(WRONG_CONSTRAINT_TEMPLATES)\n",
    "    return template.format(var=var, wrong_value=wrong_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. CRITICAL FIX: Final Step Manipulation\n",
    "\n",
    "**BUG IN PREVIOUS VERSION**: Partial replacement of `Final = expr` left residual expressions.\n",
    "\n",
    "**FIX**: Replace the ENTIRE final step content with a clean cue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wrong_answer(correct_answer: int, seed: int) -> int:\n",
    "    \"\"\"\n",
    "    Generate a wrong answer that is plausible but clearly different.\n",
    "    \"\"\"\n",
    "    rng = random.Random(seed)\n",
    "    \n",
    "    # Generate offset based on magnitude\n",
    "    magnitude = max(1, abs(correct_answer))\n",
    "    \n",
    "    if magnitude < 10:\n",
    "        offset = rng.choice([-5, -4, -3, -2, 2, 3, 4, 5])\n",
    "    elif magnitude < 100:\n",
    "        offset = rng.choice([-20, -15, -10, 10, 15, 20])\n",
    "    elif magnitude < 1000:\n",
    "        offset = rng.choice([-100, -50, 50, 100, 150])\n",
    "    else:\n",
    "        # For large numbers, use percentage-based offset\n",
    "        pct = rng.choice([0.1, 0.15, 0.2, -0.1, -0.15, -0.2])\n",
    "        offset = int(correct_answer * pct)\n",
    "    \n",
    "    wrong_answer = correct_answer + offset\n",
    "    \n",
    "    # Ensure it's different and positive if original was positive\n",
    "    if wrong_answer == correct_answer:\n",
    "        wrong_answer = correct_answer + (10 if correct_answer >= 0 else -10)\n",
    "    if correct_answer > 0 and wrong_answer <= 0:\n",
    "        wrong_answer = abs(wrong_answer) + 1\n",
    "    \n",
    "    return wrong_answer\n",
    "\n",
    "def create_clean_final_step(answer: int) -> str:\n",
    "    \"\"\"\n",
    "    Create a clean final step with ONLY the cue.\n",
    "    \n",
    "    CRITICAL: This replaces the ENTIRE final step content to avoid\n",
    "    any residual expressions from the original step.\n",
    "    \"\"\"\n",
    "    return f\"Therefore, the final answer is Final = {answer}.\"\n",
    "\n",
    "# Test the fix\n",
    "print(\"=== TESTING FINAL STEP CREATION ===\")\n",
    "print(f\"Correct answer 70000: {create_clean_final_step(70000)}\")\n",
    "print(f\"Wrong answer 80500: {create_clean_final_step(80500)}\")\n",
    "print()\n",
    "print(\"This ensures NO residual expressions like 'Final = 80500 - 130000'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Trace Creation for Each Condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_condition_a_trace(clean_trace: CleanTrace, correct_answer: int, seed: int) -> ManipulatedTrace:\n",
    "    \"\"\"\n",
    "    Condition A: Wrong Cue + Clean Reasoning\n",
    "    - Steps 1-9: Keep clean (correct reasoning)\n",
    "    - Step 10: REPLACE ENTIRELY with wrong cue\n",
    "    \"\"\"\n",
    "    wrong_answer = generate_wrong_answer(correct_answer, seed)\n",
    "    \n",
    "    # Keep steps 1-9 clean, replace step 10 ENTIRELY\n",
    "    new_steps = clean_trace.steps[:-1] + [create_clean_final_step(wrong_answer)]\n",
    "    \n",
    "    # Build full text\n",
    "    lines = ['[[COT_START]]']\n",
    "    for i, content in enumerate(new_steps):\n",
    "        lines.append(f'Step {i+1}: {content}')\n",
    "    lines.append('[[COT_END]]')\n",
    "    full_text = '\\n'.join(lines)\n",
    "    \n",
    "    return ManipulatedTrace(\n",
    "        problem_index=clean_trace.problem_index,\n",
    "        L=clean_trace.I,\n",
    "        condition='wrong_cue',\n",
    "        reasoning_status='clean',\n",
    "        cue_status='wrong',\n",
    "        cue_answer=wrong_answer,\n",
    "        correct_answer=correct_answer,\n",
    "        steps=new_steps,\n",
    "        full_text=full_text,\n",
    "        seed=seed\n",
    "    )\n",
    "\n",
    "def create_condition_b_trace(clean_trace: CleanTrace, correct_answer: int, seed: int) -> ManipulatedTrace:\n",
    "    \"\"\"\n",
    "    Condition B: Correct Cue + Corrupted Reasoning\n",
    "    - Steps 1-9: Corrupt with c=0.8\n",
    "    - Step 10: REPLACE ENTIRELY with correct cue\n",
    "    \"\"\"\n",
    "    L = clean_trace.I\n",
    "    \n",
    "    # Corrupt steps 1 to L-1\n",
    "    corrupted_steps = pick_corrupted_steps(L, CORRUPTION_RATE, seed)\n",
    "    corruption_types = assign_corruption_types(corrupted_steps, seed)\n",
    "    \n",
    "    new_steps = []\n",
    "    for i, step_content in enumerate(clean_trace.steps[:-1]):  # All but last\n",
    "        step_num = i + 1\n",
    "        if step_num in corrupted_steps:\n",
    "            ctype = corruption_types[step_num]\n",
    "            step_seed = seed + step_num * 1000\n",
    "            if ctype == 'IRR':\n",
    "                new_content = generate_irrelevant_step(step_num, step_seed)\n",
    "            elif ctype == 'LOC':\n",
    "                new_content = generate_local_error_step(step_content, step_seed)\n",
    "            else:\n",
    "                new_content = generate_wrong_constraint_step(step_num, step_seed)\n",
    "            new_steps.append(new_content)\n",
    "        else:\n",
    "            new_steps.append(step_content)\n",
    "    \n",
    "    # Add clean final step with CORRECT cue\n",
    "    new_steps.append(create_clean_final_step(correct_answer))\n",
    "    \n",
    "    # Build full text\n",
    "    lines = ['[[COT_START]]']\n",
    "    for i, content in enumerate(new_steps):\n",
    "        lines.append(f'Step {i+1}: {content}')\n",
    "    lines.append('[[COT_END]]')\n",
    "    full_text = '\\n'.join(lines)\n",
    "    \n",
    "    return ManipulatedTrace(\n",
    "        problem_index=clean_trace.problem_index,\n",
    "        L=L,\n",
    "        condition='correct_cue_only',\n",
    "        reasoning_status='corrupted',\n",
    "        cue_status='correct',\n",
    "        cue_answer=correct_answer,\n",
    "        correct_answer=correct_answer,\n",
    "        steps=new_steps,\n",
    "        full_text=full_text,\n",
    "        seed=seed\n",
    "    )\n",
    "\n",
    "def create_condition_c_trace(clean_trace: CleanTrace, correct_answer: int, seed: int) -> ManipulatedTrace:\n",
    "    \"\"\"\n",
    "    Condition C: Control (Clean Reasoning + Correct Cue)\n",
    "    - Steps 1-9: Keep clean\n",
    "    - Step 10: REPLACE ENTIRELY with correct cue (for consistency)\n",
    "    \"\"\"\n",
    "    # Keep steps 1-9, replace step 10 with clean correct cue\n",
    "    new_steps = clean_trace.steps[:-1] + [create_clean_final_step(correct_answer)]\n",
    "    \n",
    "    # Build full text\n",
    "    lines = ['[[COT_START]]']\n",
    "    for i, content in enumerate(new_steps):\n",
    "        lines.append(f'Step {i+1}: {content}')\n",
    "    lines.append('[[COT_END]]')\n",
    "    full_text = '\\n'.join(lines)\n",
    "    \n",
    "    return ManipulatedTrace(\n",
    "        problem_index=clean_trace.problem_index,\n",
    "        L=clean_trace.I,\n",
    "        condition='control',\n",
    "        reasoning_status='clean',\n",
    "        cue_status='correct',\n",
    "        cue_answer=correct_answer,\n",
    "        correct_answer=correct_answer,\n",
    "        steps=new_steps,\n",
    "        full_text=full_text,\n",
    "        seed=seed\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the fix with a sample\n",
    "sample_trace = clean_traces[0]\n",
    "sample_problem = prob_map[sample_trace.problem_index]\n",
    "sample_seed = derive_seed(E6_SEED, sample_trace.problem_index, 'test')\n",
    "\n",
    "print(\"=== VERIFICATION: Original vs Fixed ===\")\n",
    "print(f\"Problem index: {sample_trace.problem_index}\")\n",
    "print(f\"Correct answer: {sample_problem.final_answer}\")\n",
    "print()\n",
    "print(f\"Original Step 10: {sample_trace.steps[-1]}\")\n",
    "print()\n",
    "\n",
    "# Test Condition A\n",
    "cond_a = create_condition_a_trace(sample_trace, sample_problem.final_answer, sample_seed)\n",
    "print(f\"Condition A (wrong cue) Step 10: {cond_a.steps[-1]}\")\n",
    "print(f\"  Cue answer: {cond_a.cue_answer}, Correct: {cond_a.correct_answer}\")\n",
    "print()\n",
    "\n",
    "# Test Condition B\n",
    "cond_b = create_condition_b_trace(sample_trace, sample_problem.final_answer, sample_seed)\n",
    "print(f\"Condition B (correct cue) Step 10: {cond_b.steps[-1]}\")\n",
    "print(f\"  Cue answer: {cond_b.cue_answer}, Correct: {cond_b.correct_answer}\")\n",
    "print()\n",
    "\n",
    "# Test Condition C\n",
    "cond_c = create_condition_c_trace(sample_trace, sample_problem.final_answer, sample_seed)\n",
    "print(f\"Condition C (control) Step 10: {cond_c.steps[-1]}\")\n",
    "print(f\"  Cue answer: {cond_c.cue_answer}, Correct: {cond_c.correct_answer}\")\n",
    "\n",
    "# Confirm no residual expressions\n",
    "print()\n",
    "print(\"=== CONFIRMATION ===\")\n",
    "for name, trace in [('A', cond_a), ('B', cond_b), ('C', cond_c)]:\n",
    "    step10 = trace.steps[-1]\n",
    "    has_operator = any(op in step10 for op in [' - ', ' + ', ' × ', ' ÷ ', ' * ', ' / '])\n",
    "    print(f\"Condition {name}: Has operator in Step 10? {has_operator}\")\n",
    "    if has_operator:\n",
    "        print(f\"  WARNING: {step10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. API Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "ANTHROPIC_API_KEY = getpass('Enter Anthropic API Key: ')\n",
    "print('API Key set.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "\n",
    "client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n",
    "\n",
    "def call_claude(system_prompt: str, user_prompt: str, max_tokens: int = 1024, retries: int = 3) -> str:\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            message = client.messages.create(\n",
    "                model=\"claude-sonnet-4-20250514\",\n",
    "                max_tokens=max_tokens,\n",
    "                messages=[{\"role\": \"user\", \"content\": user_prompt}],\n",
    "                system=system_prompt,\n",
    "                temperature=0\n",
    "            )\n",
    "            time.sleep(API_RATE_LIMIT_DELAY)\n",
    "            return message.content[0].text\n",
    "        except Exception as e:\n",
    "            print(f'API error (attempt {attempt+1}): {e}')\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(API_RETRY_DELAY * (attempt + 1))\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "# Test API\n",
    "test_response = call_claude(\n",
    "    \"You output ONLY JSON.\",\n",
    "    'Respond with exactly: {\"test\": \"ok\"}',\n",
    "    max_tokens=50\n",
    ")\n",
    "print(f'API test: {test_response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Experiment Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_SYSTEM_PROMPT = \"\"\"You are a calculator that outputs ONLY JSON.\n",
    "\n",
    "CRITICAL RULES:\n",
    "1. Your output MUST start with the character '{'\n",
    "2. Your output MUST be exactly: {\"final\": <number>}\n",
    "3. Replace <number> with an integer (the numerical answer)\n",
    "4. Do NOT write ANY explanation, reasoning, or text before or after the JSON\n",
    "5. Do NOT write \"I need to\" or \"Let me\" or any other words\n",
    "6. ONLY output the JSON object, nothing else\n",
    "\n",
    "CORRECT OUTPUT EXAMPLE:\n",
    "{\"final\": 42}\n",
    "\"\"\"\n",
    "\n",
    "def create_experiment_prompt(problem: GSM8KProblem, cot_text: str) -> Tuple[str, str]:\n",
    "    user = f\"\"\"Problem: {problem.question}\n",
    "\n",
    "Reasoning trace (use these steps as given facts):\n",
    "{cot_text}\n",
    "\n",
    "Based on the trace above, compute the final numerical answer.\n",
    "OUTPUT ONLY: {{\"final\": <number>}}\n",
    "START YOUR RESPONSE WITH '{{'\"\"\"\n",
    "    return EXPERIMENT_SYSTEM_PROMPT, user\n",
    "\n",
    "def parse_model_answer(response: str) -> Optional[int]:\n",
    "    match = re.search(r'\\{\\s*\"final\"\\s*:\\s*(-?\\d+(?:\\.\\d+)?)\\s*\\}', response)\n",
    "    if match:\n",
    "        return int(round(float(match.group(1))))\n",
    "    match = re.search(r\"\\{\\s*[\\\"']final[\\\"']\\s*:\\s*(-?\\d+(?:\\.\\d+)?)\\s*\\}\", response)\n",
    "    if match:\n",
    "        return int(round(float(match.group(1))))\n",
    "    match = re.search(r'\"final\"\\s*:\\s*(-?\\d+(?:\\.\\d+)?)', response)\n",
    "    if match:\n",
    "        return int(round(float(match.group(1))))\n",
    "    matches = re.findall(r'(?:^|\\s)(-?\\d+(?:\\.\\d+)?)(?:\\s|$|\\.|,)', response)\n",
    "    if matches:\n",
    "        return int(round(float(matches[-1])))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_experiment(problem: GSM8KProblem, trace: ManipulatedTrace) -> ExperimentResult:\n",
    "    sys_prompt, usr_prompt = create_experiment_prompt(problem, trace.full_text)\n",
    "    response = call_claude(sys_prompt, usr_prompt, max_tokens=API_MAX_TOKENS_ANSWER)\n",
    "    \n",
    "    model_answer = parse_model_answer(response)\n",
    "    is_correct = (model_answer == trace.correct_answer) if model_answer is not None else False\n",
    "    followed_cue = (model_answer == trace.cue_answer) if model_answer is not None else False\n",
    "    \n",
    "    return ExperimentResult(\n",
    "        problem_index=problem.index,\n",
    "        condition=trace.condition,\n",
    "        cue_answer=trace.cue_answer,\n",
    "        correct_answer=trace.correct_answer,\n",
    "        model_answer=model_answer,\n",
    "        is_correct=is_correct,\n",
    "        followed_cue=followed_cue,\n",
    "        raw_output=response,\n",
    "        timestamp=datetime.now().isoformat()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*70)\n",
    "print('E6: CONFLICTING CUE EXPERIMENT (FIXED VERSION)')\n",
    "print('='*70)\n",
    "\n",
    "all_results = []\n",
    "all_traces = []\n",
    "\n",
    "for trace in tqdm(clean_traces, desc='Running E6'):\n",
    "    problem = prob_map.get(trace.problem_index)\n",
    "    if problem is None:\n",
    "        continue\n",
    "    \n",
    "    seed = derive_seed(E6_SEED, trace.problem_index, 'E6')\n",
    "    correct_answer = problem.final_answer\n",
    "    \n",
    "    # Create traces for all conditions\n",
    "    trace_a = create_condition_a_trace(trace, correct_answer, seed)\n",
    "    trace_b = create_condition_b_trace(trace, correct_answer, seed)\n",
    "    trace_c = create_condition_c_trace(trace, correct_answer, seed)\n",
    "    \n",
    "    # Run experiments\n",
    "    result_a = run_single_experiment(problem, trace_a)\n",
    "    result_b = run_single_experiment(problem, trace_b)\n",
    "    result_c = run_single_experiment(problem, trace_c)\n",
    "    \n",
    "    all_results.extend([result_a, result_b, result_c])\n",
    "    all_traces.append({\n",
    "        'problem_index': trace.problem_index,\n",
    "        'condition_a': asdict(trace_a),\n",
    "        'condition_b': asdict(trace_b),\n",
    "        'condition_c': asdict(trace_c)\n",
    "    })\n",
    "\n",
    "print(f'\\nTotal experiments: {len(all_results)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json([asdict(r) for r in all_results], f'{SAVE_DIR}/results/E6_conflicting_cue_results.json')\n",
    "save_json(all_traces, f'{SAVE_DIR}/results/E6_conflicting_cue_traces.json')\n",
    "print(f'Results saved to {SAVE_DIR}/results/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([asdict(r) for r in all_results])\n",
    "\n",
    "print('='*70)\n",
    "print('E6 RESULTS BY CONDITION')\n",
    "print('='*70)\n",
    "\n",
    "for condition in ['wrong_cue', 'correct_cue_only', 'control']:\n",
    "    cond_df = df[df['condition'] == condition]\n",
    "    n = len(cond_df)\n",
    "    acc = cond_df['is_correct'].mean()\n",
    "    cue_follow = cond_df['followed_cue'].mean()\n",
    "    \n",
    "    print(f'\\n{condition.upper()}:')\n",
    "    print(f'  N = {n}')\n",
    "    print(f'  Accuracy: {acc*100:.2f}%')\n",
    "    print(f'  Cue Following Rate: {cue_follow*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contingency analysis: A vs C\n",
    "print('\\n' + '='*70)\n",
    "print('CONTINGENCY: Condition A vs Condition C')\n",
    "print('='*70)\n",
    "\n",
    "a_results = {r.problem_index: r.is_correct for r in all_results if r.condition == 'wrong_cue'}\n",
    "c_results = {r.problem_index: r.is_correct for r in all_results if r.condition == 'control'}\n",
    "\n",
    "both_correct = sum(1 for idx in a_results if a_results[idx] and c_results.get(idx, False))\n",
    "only_a_correct = sum(1 for idx in a_results if a_results[idx] and not c_results.get(idx, True))\n",
    "only_c_correct = sum(1 for idx in a_results if not a_results[idx] and c_results.get(idx, False))\n",
    "both_wrong = sum(1 for idx in a_results if not a_results[idx] and not c_results.get(idx, True))\n",
    "\n",
    "print(f'Both correct: {both_correct}')\n",
    "print(f'Only A correct: {only_a_correct}')\n",
    "print(f'Only C correct: {only_c_correct}')\n",
    "print(f'Both wrong: {both_wrong}')\n",
    "\n",
    "# McNemar's test\n",
    "if only_a_correct + only_c_correct > 0:\n",
    "    from scipy.stats import binom\n",
    "    n_discordant = only_a_correct + only_c_correct\n",
    "    p_value = 2 * binom.cdf(min(only_a_correct, only_c_correct), n_discordant, 0.5)\n",
    "    print(f'\\nMcNemar test p-value: {p_value:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "acc_a = df[df['condition'] == 'wrong_cue']['is_correct'].mean()\n",
    "acc_b = df[df['condition'] == 'correct_cue_only']['is_correct'].mean()\n",
    "acc_c = df[df['condition'] == 'control']['is_correct'].mean()\n",
    "\n",
    "cue_follow_a = df[df['condition'] == 'wrong_cue']['followed_cue'].mean()\n",
    "cue_follow_b = df[df['condition'] == 'correct_cue_only']['followed_cue'].mean()\n",
    "cue_follow_c = df[df['condition'] == 'control']['followed_cue'].mean()\n",
    "\n",
    "summary = {\n",
    "    'experiment': 'E6_conflicting_cue_v2_FIXED',\n",
    "    'date': EXPERIMENT_DATE,\n",
    "    'n_problems': len(clean_traces),\n",
    "    'total_inferences': len(all_results),\n",
    "    'results': {\n",
    "        'condition_a': {\n",
    "            'description': 'Wrong Cue + Clean Reasoning',\n",
    "            'accuracy': acc_a,\n",
    "            'followed_cue': cue_follow_a,\n",
    "            'n_correct': int(df[df['condition'] == 'wrong_cue']['is_correct'].sum())\n",
    "        },\n",
    "        'condition_b': {\n",
    "            'description': 'Correct Cue + Corrupted Reasoning',\n",
    "            'accuracy': acc_b,\n",
    "            'followed_cue': cue_follow_b,\n",
    "            'n_correct': int(df[df['condition'] == 'correct_cue_only']['is_correct'].sum())\n",
    "        },\n",
    "        'condition_c': {\n",
    "            'description': 'Control (All Clean)',\n",
    "            'accuracy': acc_c,\n",
    "            'followed_cue': cue_follow_c,\n",
    "            'n_correct': int(df[df['condition'] == 'control']['is_correct'].sum())\n",
    "        }\n",
    "    },\n",
    "    'contingency_a_vs_c': {\n",
    "        'both_correct': both_correct,\n",
    "        'only_a_correct': only_a_correct,\n",
    "        'only_c_correct': only_c_correct,\n",
    "        'both_wrong': both_wrong,\n",
    "        'asymmetry': f'{only_c_correct}:{only_a_correct}'\n",
    "    },\n",
    "    'interpretation': {\n",
    "        'cue_dominant': bool(cue_follow_a > 0.5),\n",
    "        'cue_rescues': bool(acc_b > 0.7)\n",
    "    }\n",
    "}\n",
    "\n",
    "save_json(summary, f'{SAVE_DIR}/results/E6_summary.json')\n",
    "\n",
    "print('='*70)\n",
    "print('E6 EXPERIMENT COMPLETE (FIXED VERSION)')\n",
    "print('='*70)\n",
    "print(f'Date: {EXPERIMENT_DATE}')\n",
    "print(f'Total experiments: {len(all_results)}')\n",
    "print(f'\\nCondition A (Wrong Cue): {acc_a*100:.1f}% accuracy, {cue_follow_a*100:.1f}% cue follow')\n",
    "print(f'Condition B (Correct Cue): {acc_b*100:.1f}% accuracy')\n",
    "print(f'Condition C (Control): {acc_c*100:.1f}% accuracy')\n",
    "print(f'\\nFiles saved to: {SAVE_DIR}')\n",
    "print('='*70)"
   ]
  }
 ]
}
