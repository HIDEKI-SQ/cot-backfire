{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A3 Scaling Law Pilot Experiment: GPT-3.5-turbo\n",
    "\n",
    "**Purpose**: Test whether Î»* (Backfire boundary) correlates with model capability\n",
    "\n",
    "**Hypothesis**: If negative correlation exists (smarter = more susceptible),\n",
    "GPT-3.5 (lower capability) should have HIGHER Î»* than GPT-4o and Claude.\n",
    "\n",
    "**Current Data (2 points)**:\n",
    "| Model | Baseline | Î»* |\n",
    "|-------|----------|----|\n",
    "| GPT-4o | 56.28% | 0.865 |\n",
    "| Claude | 75.88% | 0.449 |\n",
    "\n",
    "**Expected Result**:\n",
    "- GPT-3.5 Baseline: ~30-50% (lower than both)\n",
    "- If negative correlation: GPT-3.5 Î»* > 0.865\n",
    "- If positive correlation: GPT-3.5 Î»* < 0.449\n",
    "\n",
    "**This is a PILOT (50 problems) to check feasibility before full A3 experiment.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Google Drive Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "EXPERIMENT_VERSION = 'a3_pilot_gpt35'\n",
    "EXPERIMENT_DATE = datetime.now().strftime('%Y%m%d')\n",
    "\n",
    "SAVE_DIR = '/content/drive/MyDrive/CoT_Experiment'\n",
    "SAVE_DIR_EXP = f'{SAVE_DIR}/a3_scaling_pilot_{EXPERIMENT_DATE}'\n",
    "\n",
    "os.makedirs(SAVE_DIR_EXP, exist_ok=True)\n",
    "os.makedirs(f'{SAVE_DIR_EXP}/results', exist_ok=True)\n",
    "os.makedirs(f'{SAVE_DIR_EXP}/checkpoints', exist_ok=True)\n",
    "\n",
    "print(f'A3 Pilot experiment save directory: {SAVE_DIR_EXP}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets openai pandas tqdm matplotlib scipy -q\n",
    "print('Dependencies installed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "**CRITICAL**: Use same GLOBAL_SEED and problem selection as original experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import hashlib\n",
    "from typing import List, Dict, Optional, Any\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# =============================================================================\n",
    "# Configuration - MUST MATCH ORIGINAL EXPERIMENTS\n",
    "# =============================================================================\n",
    "GLOBAL_SEED = 20251224  # Same as Claude/GPT-4o experiments\n",
    "\n",
    "# PILOT: Reduced problem set for quick feasibility check\n",
    "N_PROBLEMS_PILOT = 50  # Reduced from 200 for pilot\n",
    "\n",
    "# Experimental conditions\n",
    "I_FIXED = 10  # Same trace depth\n",
    "LAMBDA_VALUES = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]  # Same as original\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL: GPT-3.5-turbo (lower capability baseline)\n",
    "# =============================================================================\n",
    "API_MODEL = 'gpt-3.5-turbo'\n",
    "API_MAX_TOKENS = 256\n",
    "API_RATE_LIMIT_DELAY = 0.2  # GPT-3.5 is fast\n",
    "CHECKPOINT_EVERY = 25\n",
    "\n",
    "print('='*70)\n",
    "print('A3 SCALING LAW PILOT: GPT-3.5-turbo')\n",
    "print('='*70)\n",
    "print(f'  GLOBAL_SEED: {GLOBAL_SEED}')\n",
    "print(f'  N_PROBLEMS (pilot): {N_PROBLEMS_PILOT}')\n",
    "print(f'  I (fixed): {I_FIXED}')\n",
    "print(f'  Î» values: {LAMBDA_VALUES}')\n",
    "print(f'  Model: {API_MODEL}')\n",
    "print(f'  Total inferences: {N_PROBLEMS_PILOT * (len(LAMBDA_VALUES) + 1)}')\n",
    "print('='*70)\n",
    "print('\\nâš ï¸  This is a PILOT experiment to check A3 feasibility')\n",
    "print('    If results are promising, run full 200-problem experiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Structures & Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GSM8KProblem:\n",
    "    index: int\n",
    "    question: str\n",
    "    answer_text: str\n",
    "    final_answer: int\n",
    "\n",
    "@dataclass\n",
    "class ExperimentResult:\n",
    "    problem_index: int\n",
    "    condition: str\n",
    "    I: Optional[int]\n",
    "    lam: Optional[float]\n",
    "    A_target: Optional[float]\n",
    "    model_answer: Optional[int]\n",
    "    correct_answer: int\n",
    "    is_correct: bool\n",
    "    raw_output: str\n",
    "    timestamp: str\n",
    "    model: str\n",
    "\n",
    "def extract_final_answer(answer_text: str) -> int:\n",
    "    match = re.search(r'####\\s*([\\d,]+)', answer_text)\n",
    "    if match:\n",
    "        return int(match.group(1).replace(',', ''))\n",
    "    raise ValueError(f'Could not extract final answer')\n",
    "\n",
    "def save_json(data: Any, filepath: str):\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    print(f'Saved: {filepath}')\n",
    "\n",
    "def load_json(filepath: str) -> Any:\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def derive_seed(global_seed: int, problem_id: int, I: int, lam: float) -> int:\n",
    "    key = f\"{global_seed}|{problem_id}|I={I}|lam={lam}\"\n",
    "    h = hashlib.sha256(key.encode(\"utf-8\")).hexdigest()\n",
    "    return int(h[:8], 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load GSM8K and Select Problems\n",
    "\n",
    "**IMPORTANT**: Use same seed to select from the SAME 200 problems as original,\n",
    "then take first 50 for pilot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('gsm8k', 'main', split='test')\n",
    "print(f'GSM8K test set loaded: {len(dataset)} problems')\n",
    "\n",
    "def select_problems(dataset, n_problems: int, seed: int) -> List[int]:\n",
    "    rng = random.Random(seed)\n",
    "    indices = list(range(len(dataset)))\n",
    "    rng.shuffle(indices)\n",
    "    return sorted(indices[:n_problems])\n",
    "\n",
    "# Select same 200 as original, then take first N for pilot\n",
    "all_200_indices = select_problems(dataset, 200, GLOBAL_SEED)\n",
    "selected_indices = all_200_indices[:N_PROBLEMS_PILOT]\n",
    "\n",
    "print(f'Selected {len(selected_indices)} problems for pilot')\n",
    "print(f'(Subset of original 200 problems)')\n",
    "\n",
    "problems = []\n",
    "for idx in selected_indices:\n",
    "    item = dataset[idx]\n",
    "    try:\n",
    "        final_ans = extract_final_answer(item['answer'])\n",
    "        prob = GSM8KProblem(\n",
    "            index=idx,\n",
    "            question=item['question'],\n",
    "            answer_text=item['answer'],\n",
    "            final_answer=final_ans\n",
    "        )\n",
    "        problems.append(prob)\n",
    "    except ValueError as e:\n",
    "        print(f'Skipping problem {idx}: {e}')\n",
    "\n",
    "print(f'\\nLoaded {len(problems)} problems for experiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. OpenAI API Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "from openai import OpenAI\n",
    "\n",
    "# Get API key from Colab secrets\n",
    "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "def call_openai(prompt: str, max_tokens: int = API_MAX_TOKENS) -> str:\n",
    "    \"\"\"Call OpenAI API with retry logic\"\"\"\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=API_MODEL,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=0  # Deterministic\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f'API error (attempt {attempt+1}): {e}')\n",
    "            time.sleep(2 ** attempt)\n",
    "    return \"\"\n",
    "\n",
    "# Test API\n",
    "test_response = call_openai(\"What is 2+2? Reply with just the number.\")\n",
    "print(f'API test: {test_response}')\n",
    "print(f'Model: {API_MODEL}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. CoT Generation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_clean_cot(problem: GSM8KProblem, I: int) -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate I clean reasoning steps from GSM8K answer.\n",
    "    Uses the ground truth answer text, parsed into steps.\n",
    "    \"\"\"\n",
    "    # Parse answer text into steps\n",
    "    lines = problem.answer_text.split('\\n')\n",
    "    steps = [l.strip() for l in lines if l.strip() and not l.strip().startswith('####')]\n",
    "    \n",
    "    # Pad or truncate to exactly I steps\n",
    "    if len(steps) >= I:\n",
    "        return steps[:I]\n",
    "    else:\n",
    "        # Pad with generic steps\n",
    "        while len(steps) < I:\n",
    "            steps.append(f\"Step {len(steps)+1}: Continue calculation.\")\n",
    "        return steps\n",
    "\n",
    "def generate_corrupted_step(problem: GSM8KProblem, step_idx: int, rng: random.Random) -> str:\n",
    "    \"\"\"\n",
    "    Generate a corrupted reasoning step.\n",
    "    Introduces plausible but incorrect calculations.\n",
    "    \"\"\"\n",
    "    corruption_templates = [\n",
    "        f\"Step {step_idx+1}: Let's multiply the values: {rng.randint(10, 100)} Ã— {rng.randint(2, 10)} = {rng.randint(100, 1000)}.\",\n",
    "        f\"Step {step_idx+1}: Adding the totals: {rng.randint(50, 200)} + {rng.randint(50, 200)} = {rng.randint(100, 500)}.\",\n",
    "        f\"Step {step_idx+1}: The difference is: {rng.randint(100, 500)} - {rng.randint(20, 100)} = {rng.randint(50, 400)}.\",\n",
    "        f\"Step {step_idx+1}: Dividing gives us: {rng.randint(100, 1000)} Ã· {rng.randint(2, 10)} = {rng.randint(10, 200)}.\",\n",
    "        f\"Step {step_idx+1}: Converting: {rng.randint(1, 10)} Ã— {rng.randint(10, 100)} = {rng.randint(10, 1000)}.\"\n",
    "    ]\n",
    "    return rng.choice(corruption_templates)\n",
    "\n",
    "def generate_mixed_cot(problem: GSM8KProblem, I: int, lam: float, seed: int) -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate CoT with Î» proportion of corrupted steps.\n",
    "    Î»=0: all clean, Î»=1: all corrupted\n",
    "    \"\"\"\n",
    "    rng = random.Random(seed)\n",
    "    clean_steps = generate_clean_cot(problem, I)\n",
    "    \n",
    "    n_corrupt = int(round(I * lam))\n",
    "    corrupt_indices = set(rng.sample(range(I), n_corrupt)) if n_corrupt > 0 else set()\n",
    "    \n",
    "    mixed_steps = []\n",
    "    for i in range(I):\n",
    "        if i in corrupt_indices:\n",
    "            mixed_steps.append(generate_corrupted_step(problem, i, rng))\n",
    "        else:\n",
    "            mixed_steps.append(clean_steps[i] if i < len(clean_steps) else f\"Step {i+1}: Continue.\")\n",
    "    \n",
    "    return mixed_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Experiment Execution Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_direct_prompt(problem: GSM8KProblem) -> str:\n",
    "    return f\"\"\"Solve this math problem. Give ONLY the final numerical answer in JSON format.\n",
    "\n",
    "Problem: {problem.question}\n",
    "\n",
    "Reply with ONLY: {{\"final\": <number>}}\"\"\"\n",
    "\n",
    "def create_cot_prompt(problem: GSM8KProblem, cot_steps: List[str]) -> str:\n",
    "    steps_text = '\\n'.join(cot_steps)\n",
    "    return f\"\"\"Here is a math problem with a provided reasoning trace.\n",
    "Follow the reasoning and give the final numerical answer in JSON format.\n",
    "\n",
    "Problem: {problem.question}\n",
    "\n",
    "Reasoning trace:\n",
    "{steps_text}\n",
    "\n",
    "Based on this reasoning, what is the final answer?\n",
    "Reply with ONLY: {{\"final\": <number>}}\"\"\"\n",
    "\n",
    "def parse_answer(response: str) -> Optional[int]:\n",
    "    \"\"\"Extract numerical answer from model response\"\"\"\n",
    "    # Try JSON format\n",
    "    try:\n",
    "        match = re.search(r'\\{[^}]*\"final\"\\s*:\\s*(\\d+)[^}]*\\}', response)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Try plain number\n",
    "    try:\n",
    "        numbers = re.findall(r'\\b(\\d+)\\b', response)\n",
    "        if numbers:\n",
    "            return int(numbers[-1])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run Direct Condition (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*60)\n",
    "print('PHASE 1: Direct Condition (Baseline)')\n",
    "print('='*60)\n",
    "\n",
    "direct_results = []\n",
    "\n",
    "for i, problem in enumerate(tqdm(problems, desc='Direct')):\n",
    "    prompt = create_direct_prompt(problem)\n",
    "    response = call_openai(prompt)\n",
    "    \n",
    "    answer = parse_answer(response)\n",
    "    is_correct = (answer == problem.final_answer) if answer is not None else False\n",
    "    \n",
    "    result = {\n",
    "        'problem_index': problem.index,\n",
    "        'condition': 'direct',\n",
    "        'model': API_MODEL,\n",
    "        'model_answer': answer,\n",
    "        'correct_answer': problem.final_answer,\n",
    "        'is_correct': is_correct,\n",
    "        'raw_output': response,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    direct_results.append(result)\n",
    "    \n",
    "    time.sleep(API_RATE_LIMIT_DELAY)\n",
    "    \n",
    "    if (i + 1) % CHECKPOINT_EVERY == 0:\n",
    "        save_json(direct_results, f'{SAVE_DIR_EXP}/checkpoints/direct_checkpoint_{i+1}.json')\n",
    "\n",
    "# Save final direct results\n",
    "save_json(direct_results, f'{SAVE_DIR_EXP}/results/direct_results_gpt35.json')\n",
    "\n",
    "# Calculate baseline accuracy\n",
    "gpt35_direct_acc = sum(r['is_correct'] for r in direct_results) / len(direct_results)\n",
    "print(f'\\nâœ“ GPT-3.5 Direct (Baseline) Accuracy: {gpt35_direct_acc:.1%}')\n",
    "print(f'  (Compare: GPT-4o = 56.28%, Claude = 75.88%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Run CoT Conditions (Î» sweep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*60)\n",
    "print('PHASE 2: CoT Conditions (Î» sweep)')\n",
    "print('='*60)\n",
    "\n",
    "cot_results = []\n",
    "total_trials = len(problems) * len(LAMBDA_VALUES)\n",
    "\n",
    "with tqdm(total=total_trials, desc='CoT sweep') as pbar:\n",
    "    for lam in LAMBDA_VALUES:\n",
    "        print(f'\\n--- Î» = {lam} ---')\n",
    "        \n",
    "        for problem in problems:\n",
    "            # Generate mixed CoT\n",
    "            seed = derive_seed(GLOBAL_SEED, problem.index, I_FIXED, lam)\n",
    "            cot_steps = generate_mixed_cot(problem, I_FIXED, lam, seed)\n",
    "            \n",
    "            # Query model\n",
    "            prompt = create_cot_prompt(problem, cot_steps)\n",
    "            response = call_openai(prompt)\n",
    "            \n",
    "            answer = parse_answer(response)\n",
    "            is_correct = (answer == problem.final_answer) if answer is not None else False\n",
    "            \n",
    "            result = {\n",
    "                'problem_index': problem.index,\n",
    "                'condition': 'cot',\n",
    "                'model': API_MODEL,\n",
    "                'I': I_FIXED,\n",
    "                'lam': lam,\n",
    "                'A_target': 1 - lam,\n",
    "                'model_answer': answer,\n",
    "                'correct_answer': problem.final_answer,\n",
    "                'is_correct': is_correct,\n",
    "                'raw_output': response,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "            cot_results.append(result)\n",
    "            \n",
    "            time.sleep(API_RATE_LIMIT_DELAY)\n",
    "            pbar.update(1)\n",
    "        \n",
    "        # Checkpoint after each Î»\n",
    "        save_json(cot_results, f'{SAVE_DIR_EXP}/checkpoints/cot_checkpoint_lam{lam}.json')\n",
    "\n",
    "# Save final CoT results\n",
    "save_json(cot_results, f'{SAVE_DIR_EXP}/results/cot_results_gpt35.json')\n",
    "print('\\nâœ“ CoT experiment complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert to DataFrame\n",
    "cot_df = pd.DataFrame(cot_results)\n",
    "\n",
    "# Calculate accuracy by Î»\n",
    "gpt35_acc_by_lam = cot_df.groupby('lam')['is_correct'].mean().to_dict()\n",
    "\n",
    "print('='*60)\n",
    "print('GPT-3.5 RESULTS')\n",
    "print('='*60)\n",
    "print(f'\\nDirect (Baseline): {gpt35_direct_acc:.1%}')\n",
    "print('\\nCoT accuracy by Î»:')\n",
    "for lam, acc in sorted(gpt35_acc_by_lam.items()):\n",
    "    marker = 'â†' if acc < gpt35_direct_acc else ''\n",
    "    print(f'  Î»={lam:.1f}: {acc:.1%} {marker}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Estimate Î»* for GPT-3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "from scipy.optimize import brentq\n",
    "\n",
    "lam_points = np.array(sorted(gpt35_acc_by_lam.keys()))\n",
    "acc_points = np.array([gpt35_acc_by_lam[l] for l in lam_points])\n",
    "\n",
    "def estimate_lambda_crit(lam_arr, acc_arr, baseline):\n",
    "    \"\"\"Find Î» where CoT accuracy crosses baseline\"\"\"\n",
    "    f = interp1d(lam_arr, acc_arr - baseline, kind='linear', fill_value='extrapolate')\n",
    "    try:\n",
    "        for i in range(len(lam_arr) - 1):\n",
    "            if (acc_arr[i] - baseline) * (acc_arr[i+1] - baseline) < 0:\n",
    "                return brentq(f, lam_arr[i], lam_arr[i+1])\n",
    "    except:\n",
    "        pass\n",
    "    # If no crossing, estimate from trend\n",
    "    if acc_arr[-1] > baseline:\n",
    "        return 1.0  # Never crossed\n",
    "    return None\n",
    "\n",
    "gpt35_lam_crit = estimate_lambda_crit(lam_points, acc_points, gpt35_direct_acc)\n",
    "\n",
    "print('='*60)\n",
    "print('Î»* ESTIMATION FOR GPT-3.5')\n",
    "print('='*60)\n",
    "\n",
    "if gpt35_lam_crit is not None:\n",
    "    print(f'\\n  Î»* = {gpt35_lam_crit:.3f}')\n",
    "    print(f'  A* = {1 - gpt35_lam_crit:.3f}')\n",
    "else:\n",
    "    print('\\n  Could not estimate Î»* (no crossing detected)')\n",
    "    print('  This may indicate GPT-3.5 is VERY robust to corruption')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. A3 Scaling Law Visualization (3 Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previous results (hardcoded)\n",
    "models_data = {\n",
    "    'GPT-3.5': {'baseline': gpt35_direct_acc * 100, 'lambda_crit': gpt35_lam_crit},\n",
    "    'GPT-4o': {'baseline': 56.28, 'lambda_crit': 0.865},\n",
    "    'Claude': {'baseline': 75.88, 'lambda_crit': 0.449}\n",
    "}\n",
    "\n",
    "# Filter valid data\n",
    "valid_models = {k: v for k, v in models_data.items() if v['lambda_crit'] is not None}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Plot points\n",
    "colors = {'GPT-3.5': 'green', 'GPT-4o': 'blue', 'Claude': 'orange'}\n",
    "for model, data in valid_models.items():\n",
    "    ax.scatter(data['baseline'], data['lambda_crit'], \n",
    "               s=300, c=colors[model], label=model, zorder=5, edgecolors='black', linewidth=2)\n",
    "    ax.annotate(f\"{model}\\n(B={data['baseline']:.1f}%, Î»*={data['lambda_crit']:.3f})\",\n",
    "                (data['baseline'], data['lambda_crit']),\n",
    "                textcoords=\"offset points\", xytext=(15, 10),\n",
    "                fontsize=11, ha='left')\n",
    "\n",
    "# Fit line if 3+ points\n",
    "if len(valid_models) >= 2:\n",
    "    baselines = [v['baseline'] for v in valid_models.values()]\n",
    "    lambdas = [v['lambda_crit'] for v in valid_models.values()]\n",
    "    z = np.polyfit(baselines, lambdas, 1)\n",
    "    p = np.poly1d(z)\n",
    "    x_line = np.linspace(min(baselines) - 10, max(baselines) + 10, 100)\n",
    "    ax.plot(x_line, p(x_line), 'r--', linewidth=2, alpha=0.7,\n",
    "            label=f'Trend: Î»* = {z[0]:.4f}Ã—Baseline + {z[1]:.3f}')\n",
    "    \n",
    "    # Calculate correlation\n",
    "    if len(valid_models) >= 3:\n",
    "        corr = np.corrcoef(baselines, lambdas)[0, 1]\n",
    "        ax.text(0.05, 0.05, f'Correlation: r = {corr:.3f}', \n",
    "                transform=ax.transAxes, fontsize=12,\n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "# Interpretation box\n",
    "slope = z[0] if len(valid_models) >= 2 else 0\n",
    "if slope < 0:\n",
    "    interpretation = 'NEGATIVE CORRELATION\\n\\nHigher capability â†’ Lower Î»*\\nâ†’ \"Smarter = More Gullible\"\\n\\nâœ“ A3 (Nature) FEASIBLE'\n",
    "    box_color = 'lightgreen'\n",
    "elif slope > 0:\n",
    "    interpretation = 'POSITIVE CORRELATION\\n\\nHigher capability â†’ Higher Î»*\\nâ†’ \"Smarter = More Robust\"\\n\\nâœ— A3 requires different framing'\n",
    "    box_color = 'lightyellow'\n",
    "else:\n",
    "    interpretation = 'NO CLEAR CORRELATION\\n\\nÎ»* appears independent of capability\\n\\n? More data needed'\n",
    "    box_color = 'lightgray'\n",
    "\n",
    "props = dict(boxstyle='round', facecolor=box_color, alpha=0.9)\n",
    "ax.text(0.95, 0.95, interpretation, transform=ax.transAxes, fontsize=11,\n",
    "        verticalalignment='top', horizontalalignment='right', bbox=props)\n",
    "\n",
    "ax.set_xlabel('Baseline Accuracy (%)', fontsize=14)\n",
    "ax.set_ylabel('Î»* (Backfire Boundary)', fontsize=14)\n",
    "ax.set_title('A3 Scaling Law: Does Capability Predict Vulnerability?\\n(Pilot with 3 Models)', fontsize=14)\n",
    "ax.legend(loc='lower left', fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim(20, 90)\n",
    "ax.set_ylim(0.2, 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{SAVE_DIR_EXP}/a3_scaling_law_3models.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nSaved: {SAVE_DIR_EXP}/a3_scaling_law_3models.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*70)\n",
    "print('A3 SCALING LAW PILOT: FINAL SUMMARY')\n",
    "print('='*70)\n",
    "\n",
    "print('''\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    THREE-MODEL COMPARISON                           â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Model       â”‚ Baseline Accuracy â”‚ Î»* (Backfire)  â”‚ Interpretation   â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤''')\n",
    "\n",
    "for model, data in sorted(models_data.items(), key=lambda x: x[1]['baseline']):\n",
    "    lam_str = f\"{data['lambda_crit']:.3f}\" if data['lambda_crit'] else \"N/A\"\n",
    "    if data['baseline'] < 50:\n",
    "        interp = \"Low capability\"\n",
    "    elif data['baseline'] < 70:\n",
    "        interp = \"Medium capability\"\n",
    "    else:\n",
    "        interp = \"High capability\"\n",
    "    print(f\"â”‚ {model:<11} â”‚ {data['baseline']:>15.1f}% â”‚ {lam_str:>14} â”‚ {interp:<16} â”‚\")\n",
    "\n",
    "print('â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜')\n",
    "\n",
    "# Calculate slope\n",
    "if len(valid_models) >= 2:\n",
    "    baselines = [v['baseline'] for v in valid_models.values()]\n",
    "    lambdas = [v['lambda_crit'] for v in valid_models.values()]\n",
    "    slope = np.polyfit(baselines, lambdas, 1)[0]\n",
    "    \n",
    "    print(f'\\nTrend Analysis:')\n",
    "    print(f'  Slope: {slope:.4f}')\n",
    "    print(f'  For every 10% increase in baseline accuracy,')\n",
    "    print(f'  Î»* changes by {slope * 10:.3f}')\n",
    "    \n",
    "    if slope < -0.01:\n",
    "        print(f'\\nğŸ¯ CONCLUSION: NEGATIVE CORRELATION CONFIRMED')\n",
    "        print(f'   â†’ Smarter models are MORE susceptible to trace contamination')\n",
    "        print(f'   â†’ A3 \"Scaling Law of Vulnerability\" is FEASIBLE for Nature')\n",
    "    elif slope > 0.01:\n",
    "        print(f'\\nğŸ“Š CONCLUSION: POSITIVE CORRELATION')\n",
    "        print(f'   â†’ Smarter models are MORE robust')\n",
    "        print(f'   â†’ A3 needs reframing (not \"vulnerability\" story)')\n",
    "    else:\n",
    "        print(f'\\nâ“ CONCLUSION: WEAK/NO CORRELATION')\n",
    "        print(f'   â†’ More models needed to establish pattern')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Save Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save summary JSON\n",
    "summary = {\n",
    "    'experiment': 'A3_Scaling_Law_Pilot',\n",
    "    'date': EXPERIMENT_DATE,\n",
    "    'n_problems_pilot': N_PROBLEMS_PILOT,\n",
    "    'models': {\n",
    "        'gpt35': {\n",
    "            'model_name': API_MODEL,\n",
    "            'baseline_accuracy': gpt35_direct_acc,\n",
    "            'lambda_crit': gpt35_lam_crit,\n",
    "            'accuracy_by_lambda': {str(k): v for k, v in gpt35_acc_by_lam.items()}\n",
    "        },\n",
    "        'gpt4o': {\n",
    "            'model_name': 'gpt-4o',\n",
    "            'baseline_accuracy': 0.5628,\n",
    "            'lambda_crit': 0.865\n",
    "        },\n",
    "        'claude': {\n",
    "            'model_name': 'claude-3-sonnet',\n",
    "            'baseline_accuracy': 0.7588,\n",
    "            'lambda_crit': 0.449\n",
    "        }\n",
    "    },\n",
    "    'analysis': {\n",
    "        'slope': float(slope) if 'slope' in dir() else None,\n",
    "        'correlation_direction': 'negative' if slope < 0 else 'positive' if slope > 0 else 'none',\n",
    "        'a3_feasibility': 'high' if slope < -0.01 else 'low'\n",
    "    },\n",
    "    'next_steps': [\n",
    "        'If negative correlation confirmed: Proceed with full A3 experiment (200 problems, 5+ models)',\n",
    "        'Add more models: Llama-7B, Llama-70B, Mistral-7B, etc.',\n",
    "        'Consider different capability metrics (MMLU scores, parameter count)'\n",
    "    ]\n",
    "}\n",
    "\n",
    "save_json(summary, f'{SAVE_DIR_EXP}/results/a3_pilot_summary.json')\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('EXPERIMENT COMPLETE')\n",
    "print('='*70)\n",
    "print(f'\\nAll results saved to: {SAVE_DIR_EXP}')\n",
    "print('\\nNext Steps:')\n",
    "for i, step in enumerate(summary['next_steps'], 1):\n",
    "    print(f'  {i}. {step}')"
   ]
  }
 ]
}
