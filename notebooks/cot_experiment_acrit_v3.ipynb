{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CoT Phase Transition Experiment v3 - A_crit Detection (Fine-grained λ)\n",
    "\n",
    "**Version**: 3.0 (2024-12-24)\n",
    "\n",
    "**Purpose**: Precisely identify the critical alignment point (A_crit) where CoT collapse occurs.\n",
    "\n",
    "**Design**:\n",
    "- Same 200 problems as Experiment 1\n",
    "- I = 10 only (representative depth)\n",
    "- λ ∈ {0.1, 0.3, 0.5, 0.7, 0.9} (fills gaps in Experiment 1's grid)\n",
    "- Combined with Exp1 data: full 0.1-step resolution from 0.0 to 1.0\n",
    "\n",
    "**Why This Matters**:\n",
    "- Experiment 1 has λ ∈ {0.0, 0.2, 0.4, 0.6, 0.8, 1.0}\n",
    "- This adds λ ∈ {0.1, 0.3, 0.5, 0.7, 0.9}\n",
    "- Together: 11-point λ curve for precise A_crit estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Google Drive Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "EXPERIMENT_VERSION = 'v3'\n",
    "EXPERIMENT_DATE = datetime.now().strftime('%Y%m%d')\n",
    "\n",
    "SAVE_DIR = '/content/drive/MyDrive/CoT_Experiment'\n",
    "SAVE_DIR_ACRIT = f'{SAVE_DIR}/acrit_experiment_{EXPERIMENT_VERSION}_{EXPERIMENT_DATE}'\n",
    "\n",
    "os.makedirs(SAVE_DIR_ACRIT, exist_ok=True)\n",
    "os.makedirs(f'{SAVE_DIR_ACRIT}/results', exist_ok=True)\n",
    "os.makedirs(f'{SAVE_DIR_ACRIT}/checkpoints', exist_ok=True)\n",
    "os.makedirs(f'{SAVE_DIR_ACRIT}/figures', exist_ok=True)\n",
    "\n",
    "print(f'A_crit experiment save directory: {SAVE_DIR_ACRIT}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets anthropic pandas tqdm matplotlib -q\n",
    "print('Dependencies installed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import hashlib\n",
    "from typing import List, Dict, Optional, Any\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# =============================================================================\n",
    "# Configuration - MUST MATCH EXPERIMENT 1\n",
    "# =============================================================================\n",
    "GLOBAL_SEED = 20251224\n",
    "N_PROBLEMS = 200\n",
    "\n",
    "# Fine-grained λ values (fills gaps in Experiment 1)\n",
    "# Experiment 1: λ ∈ {0.0, 0.2, 0.4, 0.6, 0.8, 1.0}\n",
    "# This experiment: λ ∈ {0.1, 0.3, 0.5, 0.7, 0.9}\n",
    "LAMBDA_FINE = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "\n",
    "# Fixed I = 10 for this experiment\n",
    "I_FIXED = 10\n",
    "\n",
    "# API settings\n",
    "API_MAX_TOKENS_ANSWER = 256\n",
    "API_RATE_LIMIT_DELAY = 0.5\n",
    "CHECKPOINT_EVERY = 50\n",
    "\n",
    "print('='*60)\n",
    "print('A_CRIT DETECTION EXPERIMENT')\n",
    "print('='*60)\n",
    "print(f'  GLOBAL_SEED: {GLOBAL_SEED}')\n",
    "print(f'  N_PROBLEMS: {N_PROBLEMS}')\n",
    "print(f'  I (fixed): {I_FIXED}')\n",
    "print(f'  λ values: {LAMBDA_FINE}')\n",
    "print(f'  Total inferences: {N_PROBLEMS * len(LAMBDA_FINE)}')\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Structures & Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GSM8KProblem:\n",
    "    index: int\n",
    "    question: str\n",
    "    answer_text: str\n",
    "    final_answer: int\n",
    "\n",
    "@dataclass\n",
    "class CleanTrace:\n",
    "    problem_index: int\n",
    "    I: int\n",
    "    steps: List[str]\n",
    "    full_text: str\n",
    "\n",
    "@dataclass\n",
    "class ExperimentResult:\n",
    "    problem_index: int\n",
    "    I: int\n",
    "    lam: float\n",
    "    A_target: float\n",
    "    model_answer: Optional[int]\n",
    "    correct_answer: int\n",
    "    is_correct: bool\n",
    "    raw_output: str\n",
    "    timestamp: str\n",
    "\n",
    "def extract_final_answer(answer_text: str) -> int:\n",
    "    match = re.search(r'####\\s*([\\d,]+)', answer_text)\n",
    "    if match:\n",
    "        return int(match.group(1).replace(',', ''))\n",
    "    raise ValueError(f'Could not extract final answer')\n",
    "\n",
    "def save_json(data: Any, filepath: str):\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    print(f'Saved: {filepath}')\n",
    "\n",
    "def load_json(filepath: str) -> Any:\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def derive_seed(global_seed: int, problem_id: int, I: int, lam: float, replicate_id: int = 0) -> int:\n",
    "    key = f\"{global_seed}|{problem_id}|I={I}|lam={lam}|rep={replicate_id}\"\n",
    "    h = hashlib.sha256(key.encode(\"utf-8\")).hexdigest()\n",
    "    return int(h[:8], 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load GSM8K and Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('gsm8k', 'main', split='test')\n",
    "print(f'GSM8K test set loaded: {len(dataset)} problems')\n",
    "\n",
    "def select_problems(dataset, n_problems: int, seed: int) -> List[int]:\n",
    "    rng = random.Random(seed)\n",
    "    indices = list(range(len(dataset)))\n",
    "    rng.shuffle(indices)\n",
    "    return sorted(indices[:n_problems])\n",
    "\n",
    "# Same seed → Same problems as Experiment 1\n",
    "selected_indices = select_problems(dataset, N_PROBLEMS, GLOBAL_SEED)\n",
    "print(f'Selected {len(selected_indices)} problems (same as Experiment 1)')\n",
    "\n",
    "problems = []\n",
    "for idx in selected_indices:\n",
    "    item = dataset[idx]\n",
    "    try:\n",
    "        final_ans = extract_final_answer(item['answer'])\n",
    "        prob = GSM8KProblem(\n",
    "            index=idx,\n",
    "            question=item['question'],\n",
    "            answer_text=item['answer'],\n",
    "            final_answer=final_ans\n",
    "        )\n",
    "        problems.append(prob)\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "print(f'Prepared {len(problems)} problems')\n",
    "prob_map = {p.index: p for p in problems}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Clean Traces from Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Experiment 1 directory\n",
    "exp1_dirs = [d for d in os.listdir(SAVE_DIR) if d.startswith('full_experiment_v3')]\n",
    "if exp1_dirs:\n",
    "    EXP1_DIR = f'{SAVE_DIR}/{sorted(exp1_dirs)[-1]}'\n",
    "else:\n",
    "    EXP1_DIR = f'{SAVE_DIR}/pilot_v2'\n",
    "\n",
    "print(f'Experiment 1 directory: {EXP1_DIR}')\n",
    "\n",
    "# Load I=10 clean traces\n",
    "clean_traces_path = f'{EXP1_DIR}/clean_traces/clean_traces_I10_v3.json'\n",
    "if not os.path.exists(clean_traces_path):\n",
    "    clean_traces_path = f'{EXP1_DIR}/clean_traces_I10.json'\n",
    "if not os.path.exists(clean_traces_path):\n",
    "    for root, dirs, files in os.walk(SAVE_DIR):\n",
    "        for f in files:\n",
    "            if 'clean_traces_I10' in f and f.endswith('.json'):\n",
    "                clean_traces_path = os.path.join(root, f)\n",
    "                break\n",
    "\n",
    "print(f'Loading clean traces from: {clean_traces_path}')\n",
    "clean_traces_data = load_json(clean_traces_path)\n",
    "print(f'Loaded {len(clean_traces_data)} clean traces')\n",
    "\n",
    "traces_dict = {t['problem_index']: t for t in clean_traces_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Corruption Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_corrupted_steps(I: int, lam: float, seed: int) -> List[int]:\n",
    "    K = int(round(lam * I))\n",
    "    if K == 0:\n",
    "        return []\n",
    "    steps = list(range(1, I + 1))\n",
    "    rng = random.Random(seed)\n",
    "    rng.shuffle(steps)\n",
    "    return sorted(steps[:K])\n",
    "\n",
    "def assign_corruption_types(corrupted_steps: List[int], seed: int) -> Dict[int, str]:\n",
    "    K = len(corrupted_steps)\n",
    "    if K == 0:\n",
    "        return {}\n",
    "    n_irr = (K * 1) // 5\n",
    "    n_loc = (K * 2) // 5\n",
    "    n_wrong = K - n_irr - n_loc\n",
    "    if n_wrong == 0 and K > 0:\n",
    "        n_wrong = 1\n",
    "        if n_loc > 0:\n",
    "            n_loc -= 1\n",
    "        elif n_irr > 0:\n",
    "            n_irr -= 1\n",
    "    rng = random.Random(seed + 1)\n",
    "    perm = corrupted_steps[:]\n",
    "    rng.shuffle(perm)\n",
    "    type_map = {}\n",
    "    for s in perm[:n_irr]:\n",
    "        type_map[s] = \"IRR\"\n",
    "    for s in perm[n_irr:n_irr + n_loc]:\n",
    "        type_map[s] = \"LOC\"\n",
    "    for s in perm[n_irr + n_loc:]:\n",
    "        type_map[s] = \"WRONG\"\n",
    "    return type_map\n",
    "\n",
    "IRRELEVANT_TEMPLATES = [\n",
    "    \"Compute an auxiliary value: aux = {a} + {b} = {result}, but it will not be used later.\",\n",
    "]\n",
    "WRONG_CONSTRAINT_TEMPLATES = [\n",
    "    \"Fix an intermediate condition: set {var} = {wrong_value} as a given constraint for the rest of the steps.\",\n",
    "]\n",
    "\n",
    "def generate_irrelevant_step(step_num: int, seed: int) -> str:\n",
    "    rng = random.Random(seed)\n",
    "    a, b = rng.randint(2, 20), rng.randint(2, 20)\n",
    "    return IRRELEVANT_TEMPLATES[0].format(a=a, b=b, result=a+b)\n",
    "\n",
    "def generate_local_error_step(original_step: str, seed: int) -> str:\n",
    "    rng = random.Random(seed)\n",
    "    numbers = re.findall(r'\\d+', original_step)\n",
    "    if not numbers:\n",
    "        return f\"Compute t = 10 * 3 = {rng.randint(28, 32)} (using the previous values).\"\n",
    "    original_result = int(numbers[-1])\n",
    "    offset = rng.choice([-3, -2, -1, 1, 2, 3])\n",
    "    wrong_result = max(0, original_result + offset)\n",
    "    modified = re.sub(r'= (\\d+)\\.$', f'= {wrong_result}.', original_step)\n",
    "    if modified == original_step:\n",
    "        modified = re.sub(r'(\\d+)\\.$', f'{wrong_result}.', original_step)\n",
    "    return modified\n",
    "\n",
    "def generate_wrong_constraint_step(step_num: int, seed: int) -> str:\n",
    "    rng = random.Random(seed)\n",
    "    var = rng.choice(['x', 'total', 'result', 'n'])\n",
    "    wrong_value = rng.randint(10, 100)\n",
    "    return WRONG_CONSTRAINT_TEMPLATES[0].format(var=var, wrong_value=wrong_value)\n",
    "\n",
    "def apply_corruption(trace_data: dict, lam: float, seed: int) -> str:\n",
    "    steps = trace_data['steps'][:]\n",
    "    I = len(steps)\n",
    "    corrupted_steps = pick_corrupted_steps(I, lam, seed)\n",
    "    corruption_types = assign_corruption_types(corrupted_steps, seed)\n",
    "    new_steps = []\n",
    "    for i, step_content in enumerate(steps):\n",
    "        step_num = i + 1\n",
    "        if step_num in corruption_types:\n",
    "            ctype = corruption_types[step_num]\n",
    "            step_seed = seed + step_num * 1000\n",
    "            if ctype == 'IRR':\n",
    "                new_content = generate_irrelevant_step(step_num, step_seed)\n",
    "            elif ctype == 'LOC':\n",
    "                new_content = generate_local_error_step(step_content, step_seed)\n",
    "            else:\n",
    "                new_content = generate_wrong_constraint_step(step_num, step_seed)\n",
    "            new_steps.append(new_content)\n",
    "        else:\n",
    "            new_steps.append(step_content)\n",
    "    lines = ['[[COT_START]]']\n",
    "    for i, content in enumerate(new_steps):\n",
    "        lines.append(f'Step {i+1}: {content}')\n",
    "    lines.append('[[COT_END]]')\n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "print('Corruption logic defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. API Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "ANTHROPIC_API_KEY = getpass('Enter Anthropic API Key: ')\n",
    "print('API Key set.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "\n",
    "client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n",
    "\n",
    "def call_claude(system_prompt: str, user_prompt: str, max_tokens: int = 256, retries: int = 3) -> str:\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            message = client.messages.create(\n",
    "                model=\"claude-sonnet-4-20250514\",\n",
    "                max_tokens=max_tokens,\n",
    "                messages=[{\"role\": \"user\", \"content\": user_prompt}],\n",
    "                system=system_prompt,\n",
    "                temperature=0\n",
    "            )\n",
    "            time.sleep(API_RATE_LIMIT_DELAY)\n",
    "            return message.content[0].text\n",
    "        except Exception as e:\n",
    "            print(f'API error (attempt {attempt+1}): {e}')\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(1.0 * (attempt + 1))\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "# Test\n",
    "test_response = call_claude(\n",
    "    \"You output ONLY JSON.\",\n",
    "    'Respond with exactly: {\"test\": \"ok\"}',\n",
    "    max_tokens=50\n",
    ")\n",
    "print(f'API test: {test_response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Experiment Prompts & Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_SYSTEM_PROMPT = \"\"\"You are a calculator that outputs ONLY JSON.\n",
    "\n",
    "CRITICAL RULES:\n",
    "1. Your output MUST start with the character '{'\n",
    "2. Your output MUST be exactly: {\"final\": <number>}\n",
    "3. Replace <number> with an integer (the numerical answer)\n",
    "4. Do NOT write ANY explanation, reasoning, or text before or after the JSON\n",
    "5. ONLY output the JSON object, nothing else\n",
    "\n",
    "CORRECT OUTPUT EXAMPLE:\n",
    "{\"final\": 42}\n",
    "\"\"\"\n",
    "\n",
    "def create_experiment_prompt(problem: GSM8KProblem, cot_text: str) -> tuple:\n",
    "    user = f\"\"\"Problem: {problem.question}\n",
    "\n",
    "Reasoning trace (use these steps as given facts):\n",
    "{cot_text}\n",
    "\n",
    "Based on the trace above, compute the final numerical answer.\n",
    "OUTPUT ONLY: {{\"final\": <number>}}\n",
    "START YOUR RESPONSE WITH '{{'\"\"\"\n",
    "    return EXPERIMENT_SYSTEM_PROMPT, user\n",
    "\n",
    "def parse_model_answer(response: str) -> Optional[int]:\n",
    "    match = re.search(r'\\{\\s*\"final\"\\s*:\\s*(-?\\d+(?:\\.\\d+)?)\\s*\\}', response)\n",
    "    if match:\n",
    "        return int(round(float(match.group(1))))\n",
    "    match = re.search(r\"\\{\\s*[\\\"']final[\\\"']\\s*:\\s*(-?\\d+(?:\\.\\d+)?)\\s*\\}\", response)\n",
    "    if match:\n",
    "        return int(round(float(match.group(1))))\n",
    "    match = re.search(r'\"final\"\\s*:\\s*(-?\\d+(?:\\.\\d+)?)', response)\n",
    "    if match:\n",
    "        return int(round(float(match.group(1))))\n",
    "    matches = re.findall(r'(?:^|\\s)(-?\\d+(?:\\.\\d+)?)(?:\\s|$|\\.|,)', response)\n",
    "    if matches:\n",
    "        return int(round(float(matches[-1])))\n",
    "    return None\n",
    "\n",
    "print('Prompts and parsing defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Run A_crit Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(problem: GSM8KProblem, cot_text: str, I: int, lam: float) -> ExperimentResult:\n",
    "    sys_prompt, usr_prompt = create_experiment_prompt(problem, cot_text)\n",
    "    response = call_claude(sys_prompt, usr_prompt, max_tokens=API_MAX_TOKENS_ANSWER)\n",
    "    \n",
    "    model_answer = parse_model_answer(response)\n",
    "    is_correct = (model_answer == problem.final_answer) if model_answer is not None else False\n",
    "    \n",
    "    return ExperimentResult(\n",
    "        problem_index=problem.index,\n",
    "        I=I,\n",
    "        lam=lam,\n",
    "        A_target=1.0 - lam,\n",
    "        model_answer=model_answer,\n",
    "        correct_answer=problem.final_answer,\n",
    "        is_correct=is_correct,\n",
    "        raw_output=response,\n",
    "        timestamp=datetime.now().isoformat()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full experiment\n",
    "print('='*60)\n",
    "print('A_CRIT DETECTION EXPERIMENT')\n",
    "print('='*60)\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for lam in LAMBDA_FINE:\n",
    "    print(f'\\nλ = {lam} (A = {1-lam})')\n",
    "    \n",
    "    lam_results = []\n",
    "    for prob in tqdm(problems, desc=f'λ={lam}'):\n",
    "        if prob.index not in traces_dict:\n",
    "            continue\n",
    "        \n",
    "        trace_data = traces_dict[prob.index]\n",
    "        \n",
    "        # Apply corruption\n",
    "        seed = derive_seed(GLOBAL_SEED, prob.index, I=I_FIXED, lam=lam)\n",
    "        corrupted_cot = apply_corruption(trace_data, lam, seed)\n",
    "        \n",
    "        # Run experiment\n",
    "        result = run_experiment(prob, corrupted_cot, I=I_FIXED, lam=lam)\n",
    "        lam_results.append(result)\n",
    "        all_results.append(result)\n",
    "        \n",
    "        # Checkpoint\n",
    "        if len(all_results) % CHECKPOINT_EVERY == 0:\n",
    "            save_json([asdict(r) for r in all_results], \n",
    "                     f'{SAVE_DIR_ACRIT}/checkpoints/latest_acrit_v3.json')\n",
    "    \n",
    "    # Report accuracy for this λ\n",
    "    acc = sum(r.is_correct for r in lam_results) / len(lam_results) if lam_results else 0\n",
    "    print(f'  Accuracy: {acc:.1%} ({sum(r.is_correct for r in lam_results)}/{len(lam_results)})')\n",
    "\n",
    "# Save final results\n",
    "save_json([asdict(r) for r in all_results], f'{SAVE_DIR_ACRIT}/results/acrit_results_v3.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Analysis & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([asdict(r) for r in all_results])\n",
    "\n",
    "print('\\nA_crit Experiment Results (λ ∈ {0.1, 0.3, 0.5, 0.7, 0.9}):')\n",
    "print(df.groupby('lam')['is_correct'].agg(['mean', 'sum', 'count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load Experiment 1 results to combine\n",
    "exp1_results_path = f'{EXP1_DIR}/results/results_full_v3.json'\n",
    "\n",
    "if os.path.exists(exp1_results_path):\n",
    "    exp1_data = load_json(exp1_results_path)\n",
    "    exp1_df = pd.DataFrame(exp1_data)\n",
    "    \n",
    "    # Filter to I=10 only\n",
    "    exp1_I10 = exp1_df[exp1_df['I'] == I_FIXED]\n",
    "    \n",
    "    # Combine\n",
    "    combined_df = pd.concat([exp1_I10, df], ignore_index=True)\n",
    "    \n",
    "    print('\\n=== COMBINED RESULTS (Exp1 + Exp4, I=10) ===')\n",
    "    combined_summary = combined_df.groupby('lam')['is_correct'].agg(['mean', 'count'])\n",
    "    combined_summary.columns = ['Accuracy', 'N']\n",
    "    combined_summary['A'] = 1 - combined_summary.index\n",
    "    print(combined_summary.sort_index())\n",
    "else:\n",
    "    print('Experiment 1 results not found. Will combine after Exp1 completes.')\n",
    "    combined_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy curve\n",
    "acc_by_lam = combined_df.groupby('lam')['is_correct'].mean().sort_index()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.plot(acc_by_lam.index, acc_by_lam.values, 'o-', linewidth=2, markersize=8, color='#2E8B57')\n",
    "\n",
    "# Mark potential A_crit\n",
    "ax.axhline(y=0.7, color='red', linestyle='--', alpha=0.5, label='Potential A_crit threshold')\n",
    "\n",
    "ax.set_xlabel('λ (Corruption Rate)', fontsize=12)\n",
    "ax.set_ylabel('Accuracy', fontsize=12)\n",
    "ax.set_title(f'Accuracy vs Corruption Rate (I={I_FIXED}, Fine-grained)', fontsize=14)\n",
    "ax.set_xlim(-0.05, 1.05)\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "\n",
    "# Add A axis on top\n",
    "ax2 = ax.twiny()\n",
    "ax2.set_xlim(ax.get_xlim())\n",
    "ax2.set_xticks([0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "ax2.set_xticklabels(['1.0', '0.8', '0.6', '0.4', '0.2', '0.0'])\n",
    "ax2.set_xlabel('A (Alignment = 1 - λ)', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{SAVE_DIR_ACRIT}/figures/accuracy_curve_fine_v3.png', dpi=150)\n",
    "plt.show()\n",
    "print(f'Saved: {SAVE_DIR_ACRIT}/figures/accuracy_curve_fine_v3.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate A_crit (where accuracy drops below threshold)\n",
    "THRESHOLD = 0.7  # 70% accuracy threshold\n",
    "\n",
    "acc_sorted = acc_by_lam.sort_index()\n",
    "\n",
    "# Find λ where accuracy first drops below threshold\n",
    "lam_crit = None\n",
    "for lam, acc in acc_sorted.items():\n",
    "    if acc < THRESHOLD:\n",
    "        lam_crit = lam\n",
    "        break\n",
    "\n",
    "if lam_crit is not None:\n",
    "    A_crit = 1 - lam_crit\n",
    "    print(f'\\n=== A_CRIT ESTIMATION ===')\n",
    "    print(f'Threshold: {THRESHOLD:.0%}')\n",
    "    print(f'λ_crit ≈ {lam_crit}')\n",
    "    print(f'A_crit ≈ {A_crit}')\n",
    "    print(f'\\nInterpretation: CoT becomes unreliable when alignment drops below ~{A_crit:.1f}')\n",
    "else:\n",
    "    print('\\nA_crit not detected within the measured range.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*60)\n",
    "print('A_CRIT EXPERIMENT SUMMARY')\n",
    "print('='*60)\n",
    "print(f'Version: {EXPERIMENT_VERSION}')\n",
    "print(f'Date: {EXPERIMENT_DATE}')\n",
    "print(f'I (fixed): {I_FIXED}')\n",
    "print(f'λ values tested: {LAMBDA_FINE}')\n",
    "print(f'Total results: {len(all_results)}')\n",
    "print(f'\\nAccuracy by λ:')\n",
    "for lam in sorted(df['lam'].unique()):\n",
    "    acc = df[df['lam']==lam]['is_correct'].mean()\n",
    "    print(f'  λ={lam} (A={1-lam:.1f}): {acc:.1%}')\n",
    "print(f'\\nResults saved to: {SAVE_DIR_ACRIT}')\n",
    "print('='*60)"
   ]
  }
 ]
}
