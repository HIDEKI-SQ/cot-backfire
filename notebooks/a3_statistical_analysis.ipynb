{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A3 Statistical Analysis: Bootstrap CI & Supplementary Metrics\n",
    "\n",
    "**Purpose**: \n",
    "1. Calculate 95% confidence intervals for λ* using Bootstrap\n",
    "2. Compute supplementary metrics (Δ@λ=1, harm area)\n",
    "3. Perform McNemar test for significance\n",
    "4. Create publication-quality figures\n",
    "\n",
    "**Input**: cot_results_*.json and direct_results_*.json from all models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.optimize import brentq\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Base directory\n",
    "BASE_DIR = '/content/drive/MyDrive/CoT_Experiment'\n",
    "\n",
    "# Output directory for analysis\n",
    "ANALYSIS_DIR = f'{BASE_DIR}/a3_statistical_analysis'\n",
    "os.makedirs(ANALYSIS_DIR, exist_ok=True)\n",
    "os.makedirs(f'{ANALYSIS_DIR}/figures', exist_ok=True)\n",
    "\n",
    "print(f'Analysis output: {ANALYSIS_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load All Experimental Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def save_json(data, filepath):\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Model configurations - UPDATE PATHS AS NEEDED\n",
    "MODEL_CONFIGS = {\n",
    "    'Claude 3 Haiku': {\n",
    "        'dir': 'a3_claude_haiku3_20251228',\n",
    "        'short': 'haiku',\n",
    "        'color': '#1f77b4',\n",
    "        'marker': 'o'\n",
    "    },\n",
    "    'Claude 3.5 Haiku': {\n",
    "        'dir': 'a3_claude_haiku35_20251228',\n",
    "        'short': 'haiku35',\n",
    "        'color': '#2ca02c',\n",
    "        'marker': 's'\n",
    "    },\n",
    "    'Claude 4 Sonnet': {\n",
    "        'dir': 'a3_claude_sonnet4_20251228',\n",
    "        'short': 'sonnet4',\n",
    "        'color': '#9467bd',\n",
    "        'marker': '^'\n",
    "    },\n",
    "    'GPT-3.5 Turbo': {\n",
    "        'dir': 'a3_gpt_gpt35_20251228',\n",
    "        'short': 'gpt35',\n",
    "        'color': '#d62728',\n",
    "        'marker': 'v'\n",
    "    },\n",
    "    'GPT-4o-mini': {\n",
    "        'dir': 'a3_gpt_gpt4omini_20251228',\n",
    "        'short': 'gpt4omini',\n",
    "        'color': '#ff7f0e',\n",
    "        'marker': 'D'\n",
    "    },\n",
    "    'GPT-4o': {\n",
    "        'dir': 'chatgpt_experiment_20251225',  # Original experiment\n",
    "        'short': 'gpt4o',\n",
    "        'color': '#8c564b',\n",
    "        'marker': 'p',\n",
    "        'alt_dir': 'a3_gpt_gpt4o_20251228'  # Alternative if exists\n",
    "    }\n",
    "}\n",
    "\n",
    "# Load data for each model\n",
    "all_data = {}\n",
    "\n",
    "for model_name, config in MODEL_CONFIGS.items():\n",
    "    print(f'\\nLoading {model_name}...')\n",
    "    \n",
    "    # Try primary directory\n",
    "    model_dir = f\"{BASE_DIR}/{config['dir']}\"\n",
    "    \n",
    "    # Check alternative directory if primary doesn't exist\n",
    "    if not os.path.exists(model_dir) and 'alt_dir' in config:\n",
    "        model_dir = f\"{BASE_DIR}/{config['alt_dir']}\"\n",
    "    \n",
    "    if not os.path.exists(model_dir):\n",
    "        print(f'  ⚠️ Directory not found: {model_dir}')\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # Load direct results\n",
    "        direct_path = f\"{model_dir}/results/direct_results_{config['short']}.json\"\n",
    "        if not os.path.exists(direct_path):\n",
    "            # Try alternative naming\n",
    "            for f in os.listdir(f\"{model_dir}/results\"):\n",
    "                if f.startswith('direct_results'):\n",
    "                    direct_path = f\"{model_dir}/results/{f}\"\n",
    "                    break\n",
    "        \n",
    "        direct_results = load_json(direct_path)\n",
    "        \n",
    "        # Load CoT results\n",
    "        cot_path = f\"{model_dir}/results/cot_results_{config['short']}.json\"\n",
    "        if not os.path.exists(cot_path):\n",
    "            for f in os.listdir(f\"{model_dir}/results\"):\n",
    "                if f.startswith('cot_results'):\n",
    "                    cot_path = f\"{model_dir}/results/{f}\"\n",
    "                    break\n",
    "        \n",
    "        cot_results = load_json(cot_path)\n",
    "        \n",
    "        all_data[model_name] = {\n",
    "            'direct': direct_results,\n",
    "            'cot': cot_results,\n",
    "            'config': config\n",
    "        }\n",
    "        \n",
    "        print(f'  ✓ Loaded: {len(direct_results)} direct, {len(cot_results)} cot')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'  ❌ Error: {e}')\n",
    "\n",
    "print(f'\\n=== Loaded {len(all_data)} models ===')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compute Basic Metrics for Each Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_basic_metrics(direct_results, cot_results):\n",
    "    \"\"\"Compute baseline accuracy and accuracy by lambda\"\"\"\n",
    "    \n",
    "    # Baseline accuracy\n",
    "    baseline = sum(r['is_correct'] for r in direct_results) / len(direct_results)\n",
    "    \n",
    "    # Accuracy by lambda\n",
    "    cot_df = pd.DataFrame(cot_results)\n",
    "    acc_by_lam = cot_df.groupby('lam')['is_correct'].mean().to_dict()\n",
    "    \n",
    "    return baseline, acc_by_lam\n",
    "\n",
    "def estimate_lambda_crit(lam_arr, acc_arr, baseline):\n",
    "    \"\"\"Find λ where CoT accuracy crosses baseline\"\"\"\n",
    "    lam_arr = np.array(lam_arr)\n",
    "    acc_arr = np.array(acc_arr)\n",
    "    \n",
    "    try:\n",
    "        f = interp1d(lam_arr, acc_arr - baseline, kind='linear', fill_value='extrapolate')\n",
    "        for i in range(len(lam_arr) - 1):\n",
    "            if (acc_arr[i] - baseline) * (acc_arr[i+1] - baseline) < 0:\n",
    "                return brentq(f, lam_arr[i], lam_arr[i+1])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # If no crossing found\n",
    "    if acc_arr[-1] > baseline:\n",
    "        return 1.0  # Never crosses\n",
    "    return None\n",
    "\n",
    "# Compute metrics for all models\n",
    "metrics = {}\n",
    "\n",
    "for model_name, data in all_data.items():\n",
    "    baseline, acc_by_lam = compute_basic_metrics(data['direct'], data['cot'])\n",
    "    \n",
    "    lam_arr = sorted(acc_by_lam.keys())\n",
    "    acc_arr = [acc_by_lam[l] for l in lam_arr]\n",
    "    \n",
    "    lambda_crit = estimate_lambda_crit(lam_arr, acc_arr, baseline)\n",
    "    \n",
    "    metrics[model_name] = {\n",
    "        'baseline': baseline,\n",
    "        'acc_by_lam': acc_by_lam,\n",
    "        'lambda_crit': lambda_crit,\n",
    "        'delta_at_1': acc_by_lam.get(1.0, 0) - baseline,  # Δ@λ=1\n",
    "        'n_problems': len(data['direct'])\n",
    "    }\n",
    "\n",
    "# Display\n",
    "print('='*70)\n",
    "print('BASIC METRICS')\n",
    "print('='*70)\n",
    "print(f'{\"Model\":<20} {\"Baseline\":>10} {\"λ*\":>10} {\"Δ@λ=1\":>10} {\"N\":>6}')\n",
    "print('-'*70)\n",
    "for model, m in sorted(metrics.items(), key=lambda x: x[1]['baseline']):\n",
    "    lc = f\"{m['lambda_crit']:.3f}\" if m['lambda_crit'] else 'N/A'\n",
    "    print(f\"{model:<20} {m['baseline']:>10.1%} {lc:>10} {m['delta_at_1']:>+10.1%} {m['n_problems']:>6}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bootstrap Analysis for λ* Confidence Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_lambda_crit(direct_results, cot_results, n_bootstrap=1000, seed=42):\n",
    "    \"\"\"\n",
    "    Bootstrap estimation of λ* with 95% CI\n",
    "    \n",
    "    Returns: (point_estimate, ci_lower, ci_upper, bootstrap_samples)\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Get problem indices\n",
    "    direct_df = pd.DataFrame(direct_results)\n",
    "    cot_df = pd.DataFrame(cot_results)\n",
    "    \n",
    "    problem_ids = direct_df['problem_index'].unique()\n",
    "    n_problems = len(problem_ids)\n",
    "    \n",
    "    # Point estimate\n",
    "    baseline = direct_df['is_correct'].mean()\n",
    "    acc_by_lam = cot_df.groupby('lam')['is_correct'].mean()\n",
    "    lam_arr = np.array(acc_by_lam.index)\n",
    "    acc_arr = np.array(acc_by_lam.values)\n",
    "    point_estimate = estimate_lambda_crit(lam_arr, acc_arr, baseline)\n",
    "    \n",
    "    # Bootstrap\n",
    "    bootstrap_samples = []\n",
    "    \n",
    "    for b in range(n_bootstrap):\n",
    "        # Resample problem indices with replacement\n",
    "        boot_ids = np.random.choice(problem_ids, size=n_problems, replace=True)\n",
    "        \n",
    "        # Get bootstrap sample for direct\n",
    "        boot_direct = direct_df[direct_df['problem_index'].isin(boot_ids)]\n",
    "        boot_baseline = boot_direct['is_correct'].mean()\n",
    "        \n",
    "        # Get bootstrap sample for CoT\n",
    "        boot_cot = cot_df[cot_df['problem_index'].isin(boot_ids)]\n",
    "        boot_acc = boot_cot.groupby('lam')['is_correct'].mean()\n",
    "        \n",
    "        if len(boot_acc) == len(lam_arr):\n",
    "            boot_lc = estimate_lambda_crit(\n",
    "                np.array(boot_acc.index),\n",
    "                np.array(boot_acc.values),\n",
    "                boot_baseline\n",
    "            )\n",
    "            if boot_lc is not None:\n",
    "                bootstrap_samples.append(boot_lc)\n",
    "    \n",
    "    if len(bootstrap_samples) > 0:\n",
    "        ci_lower = np.percentile(bootstrap_samples, 2.5)\n",
    "        ci_upper = np.percentile(bootstrap_samples, 97.5)\n",
    "        boot_mean = np.mean(bootstrap_samples)\n",
    "        boot_std = np.std(bootstrap_samples)\n",
    "    else:\n",
    "        ci_lower, ci_upper, boot_mean, boot_std = None, None, None, None\n",
    "    \n",
    "    return {\n",
    "        'point_estimate': point_estimate,\n",
    "        'ci_lower': ci_lower,\n",
    "        'ci_upper': ci_upper,\n",
    "        'boot_mean': boot_mean,\n",
    "        'boot_std': boot_std,\n",
    "        'n_valid_samples': len(bootstrap_samples),\n",
    "        'samples': bootstrap_samples\n",
    "    }\n",
    "\n",
    "# Run bootstrap for all models\n",
    "print('Running Bootstrap analysis (this may take a few minutes)...')\n",
    "print('='*70)\n",
    "\n",
    "bootstrap_results = {}\n",
    "\n",
    "for model_name, data in tqdm(all_data.items(), desc='Bootstrap'):\n",
    "    result = bootstrap_lambda_crit(\n",
    "        data['direct'], \n",
    "        data['cot'], \n",
    "        n_bootstrap=1000\n",
    "    )\n",
    "    bootstrap_results[model_name] = result\n",
    "    \n",
    "    pe = result['point_estimate']\n",
    "    ci_l = result['ci_lower']\n",
    "    ci_u = result['ci_upper']\n",
    "    \n",
    "    if pe and ci_l and ci_u:\n",
    "        print(f\"{model_name}: λ* = {pe:.3f} [95% CI: {ci_l:.3f}, {ci_u:.3f}]\")\n",
    "    else:\n",
    "        print(f\"{model_name}: λ* = {pe} (CI could not be computed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. McNemar Test: Is Backfire Significant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcnemar_test(direct_results, cot_results, target_lambda):\n",
    "    \"\"\"\n",
    "    McNemar test comparing Direct vs CoT at specific λ\n",
    "    \n",
    "    Tests if the difference in accuracy is statistically significant\n",
    "    \"\"\"\n",
    "    direct_df = pd.DataFrame(direct_results)\n",
    "    cot_df = pd.DataFrame(cot_results)\n",
    "    \n",
    "    # Filter CoT for target lambda\n",
    "    cot_at_lam = cot_df[cot_df['lam'] == target_lambda]\n",
    "    \n",
    "    # Merge on problem_index\n",
    "    merged = pd.merge(\n",
    "        direct_df[['problem_index', 'is_correct']],\n",
    "        cot_at_lam[['problem_index', 'is_correct']],\n",
    "        on='problem_index',\n",
    "        suffixes=('_direct', '_cot')\n",
    "    )\n",
    "    \n",
    "    # Build contingency table\n",
    "    # a: both correct, b: direct correct/cot wrong\n",
    "    # c: direct wrong/cot correct, d: both wrong\n",
    "    a = ((merged['is_correct_direct'] == True) & (merged['is_correct_cot'] == True)).sum()\n",
    "    b = ((merged['is_correct_direct'] == True) & (merged['is_correct_cot'] == False)).sum()\n",
    "    c = ((merged['is_correct_direct'] == False) & (merged['is_correct_cot'] == True)).sum()\n",
    "    d = ((merged['is_correct_direct'] == False) & (merged['is_correct_cot'] == False)).sum()\n",
    "    \n",
    "    # McNemar statistic (with continuity correction)\n",
    "    if b + c > 0:\n",
    "        chi2 = (abs(b - c) - 1)**2 / (b + c)\n",
    "        p_value = 1 - stats.chi2.cdf(chi2, df=1)\n",
    "    else:\n",
    "        chi2, p_value = 0, 1.0\n",
    "    \n",
    "    return {\n",
    "        'lambda': target_lambda,\n",
    "        'contingency': [[a, b], [c, d]],\n",
    "        'direct_correct_cot_wrong': b,\n",
    "        'direct_wrong_cot_correct': c,\n",
    "        'chi2': chi2,\n",
    "        'p_value': p_value,\n",
    "        'significant': p_value < 0.05\n",
    "    }\n",
    "\n",
    "# Run McNemar test for key lambda values\n",
    "print('='*70)\n",
    "print('McNEMAR TEST: Is Backfire Statistically Significant?')\n",
    "print('='*70)\n",
    "\n",
    "mcnemar_results = {}\n",
    "\n",
    "for model_name, data in all_data.items():\n",
    "    print(f'\\n{model_name}:')\n",
    "    model_results = {}\n",
    "    \n",
    "    for lam in [0.4, 0.6, 0.8, 1.0]:\n",
    "        result = mcnemar_test(data['direct'], data['cot'], lam)\n",
    "        model_results[lam] = result\n",
    "        \n",
    "        sig = '***' if result['p_value'] < 0.001 else '**' if result['p_value'] < 0.01 else '*' if result['p_value'] < 0.05 else ''\n",
    "        direction = 'CoT better' if result['direct_wrong_cot_correct'] > result['direct_correct_cot_wrong'] else 'Direct better'\n",
    "        \n",
    "        print(f\"  λ={lam}: p={result['p_value']:.4f} {sig} ({direction})\")\n",
    "    \n",
    "    mcnemar_results[model_name] = model_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compute Supplementary Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_harm_area(acc_by_lam, baseline):\n",
    "    \"\"\"\n",
    "    Compute the \"harm area\" - integral of (baseline - acc) where acc < baseline\n",
    "    Uses trapezoidal integration\n",
    "    \"\"\"\n",
    "    lams = sorted(acc_by_lam.keys())\n",
    "    harm = 0\n",
    "    \n",
    "    for i in range(len(lams) - 1):\n",
    "        l1, l2 = lams[i], lams[i+1]\n",
    "        a1, a2 = acc_by_lam[l1], acc_by_lam[l2]\n",
    "        \n",
    "        # Compute harm (negative benefit) for this segment\n",
    "        h1 = max(0, baseline - a1)\n",
    "        h2 = max(0, baseline - a2)\n",
    "        \n",
    "        # Trapezoidal area\n",
    "        harm += (h1 + h2) * (l2 - l1) / 2\n",
    "    \n",
    "    return harm\n",
    "\n",
    "def compute_benefit_area(acc_by_lam, baseline):\n",
    "    \"\"\"\n",
    "    Compute the \"benefit area\" - integral of (acc - baseline) where acc > baseline\n",
    "    \"\"\"\n",
    "    lams = sorted(acc_by_lam.keys())\n",
    "    benefit = 0\n",
    "    \n",
    "    for i in range(len(lams) - 1):\n",
    "        l1, l2 = lams[i], lams[i+1]\n",
    "        a1, a2 = acc_by_lam[l1], acc_by_lam[l2]\n",
    "        \n",
    "        b1 = max(0, a1 - baseline)\n",
    "        b2 = max(0, a2 - baseline)\n",
    "        \n",
    "        benefit += (b1 + b2) * (l2 - l1) / 2\n",
    "    \n",
    "    return benefit\n",
    "\n",
    "# Compute all supplementary metrics\n",
    "print('='*70)\n",
    "print('SUPPLEMENTARY METRICS')\n",
    "print('='*70)\n",
    "\n",
    "supplementary = {}\n",
    "\n",
    "for model_name, m in metrics.items():\n",
    "    harm = compute_harm_area(m['acc_by_lam'], m['baseline'])\n",
    "    benefit = compute_benefit_area(m['acc_by_lam'], m['baseline'])\n",
    "    \n",
    "    supplementary[model_name] = {\n",
    "        'lambda_crit': m['lambda_crit'],\n",
    "        'delta_at_1': m['delta_at_1'],\n",
    "        'harm_area': harm,\n",
    "        'benefit_area': benefit,\n",
    "        'net_area': benefit - harm,\n",
    "        'harm_ratio': harm / (harm + benefit) if (harm + benefit) > 0 else 0\n",
    "    }\n",
    "\n",
    "print(f'\\n{\"Model\":<20} {\"λ*\":>8} {\"Δ@λ=1\":>10} {\"Harm\":>8} {\"Benefit\":>8} {\"Net\":>8}')\n",
    "print('-'*70)\n",
    "for model, s in sorted(supplementary.items(), key=lambda x: metrics[x[0]]['baseline']):\n",
    "    lc = f\"{s['lambda_crit']:.3f}\" if s['lambda_crit'] else 'N/A'\n",
    "    print(f\"{model:<20} {lc:>8} {s['delta_at_1']:>+10.3f} {s['harm_area']:>8.3f} {s['benefit_area']:>8.3f} {s['net_area']:>+8.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Publication-Quality Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 1: Baseline vs λ* (Main Result)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "for model_name, data in all_data.items():\n",
    "    m = metrics[model_name]\n",
    "    b = bootstrap_results[model_name]\n",
    "    config = data['config']\n",
    "    \n",
    "    if m['lambda_crit'] is not None:\n",
    "        # Plot point\n",
    "        ax.scatter(\n",
    "            m['baseline'] * 100, \n",
    "            m['lambda_crit'],\n",
    "            s=200, \n",
    "            c=config['color'],\n",
    "            marker=config['marker'],\n",
    "            label=model_name,\n",
    "            zorder=5,\n",
    "            edgecolors='black',\n",
    "            linewidth=1.5\n",
    "        )\n",
    "        \n",
    "        # Plot CI as error bar\n",
    "        if b['ci_lower'] and b['ci_upper']:\n",
    "            ax.errorbar(\n",
    "                m['baseline'] * 100,\n",
    "                m['lambda_crit'],\n",
    "                yerr=[[m['lambda_crit'] - b['ci_lower']], [b['ci_upper'] - m['lambda_crit']]],\n",
    "                fmt='none',\n",
    "                c=config['color'],\n",
    "                capsize=5,\n",
    "                capthick=2,\n",
    "                elinewidth=2,\n",
    "                zorder=4\n",
    "            )\n",
    "\n",
    "# Fit trend line (excluding λ*=1.0 models)\n",
    "valid_points = [(metrics[m]['baseline'], metrics[m]['lambda_crit']) \n",
    "                for m in metrics if metrics[m]['lambda_crit'] and metrics[m]['lambda_crit'] < 1.0]\n",
    "if len(valid_points) >= 2:\n",
    "    baselines = [p[0] * 100 for p in valid_points]\n",
    "    lambdas = [p[1] for p in valid_points]\n",
    "    z = np.polyfit(baselines, lambdas, 1)\n",
    "    p = np.poly1d(z)\n",
    "    x_line = np.linspace(min(baselines) - 5, max(baselines) + 5, 100)\n",
    "    ax.plot(x_line, p(x_line), 'k--', linewidth=2, alpha=0.5, label=f'Trend (slope={z[0]:.4f})')\n",
    "\n",
    "ax.set_xlabel('Baseline Accuracy (%)', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('λ* (Backfire Boundary)', fontsize=14, fontweight='bold')\n",
    "ax.set_title('Scaling Law of Vulnerability:\\nHigher Capability → Lower λ* → More Susceptible', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='upper right', fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim(25, 100)\n",
    "ax.set_ylim(0.3, 1.05)\n",
    "\n",
    "# Add annotation for key finding\n",
    "ax.annotate(\n",
    "    'Claude 4 Sonnet:\\nHighest capability (92.5%)\\nLowest λ* (≈0.4)',\n",
    "    xy=(92.5, 0.4), xytext=(70, 0.5),\n",
    "    fontsize=10,\n",
    "    arrowprops=dict(arrowstyle='->', color='purple', lw=2),\n",
    "    bbox=dict(boxstyle='round', facecolor='lavender', alpha=0.8)\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{ANALYSIS_DIR}/figures/fig1_baseline_vs_lambda_crit.png', dpi=300, bbox_inches='tight')\n",
    "plt.savefig(f'{ANALYSIS_DIR}/figures/fig1_baseline_vs_lambda_crit.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: fig1_baseline_vs_lambda_crit.png/pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 2: All Collapse Curves\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "for model_name, data in all_data.items():\n",
    "    m = metrics[model_name]\n",
    "    config = data['config']\n",
    "    \n",
    "    lams = sorted(m['acc_by_lam'].keys())\n",
    "    accs = [m['acc_by_lam'][l] * 100 for l in lams]\n",
    "    \n",
    "    ax.plot(\n",
    "        lams, accs,\n",
    "        marker=config['marker'],\n",
    "        color=config['color'],\n",
    "        linewidth=2.5,\n",
    "        markersize=10,\n",
    "        label=f\"{model_name} (B={m['baseline']*100:.1f}%)\"\n",
    "    )\n",
    "    \n",
    "    # Add baseline as horizontal dashed line\n",
    "    ax.axhline(y=m['baseline']*100, color=config['color'], linestyle=':', alpha=0.5)\n",
    "\n",
    "ax.set_xlabel('Corruption Rate (λ)', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=14, fontweight='bold')\n",
    "ax.set_title('CoT Collapse Curves Across Models', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='lower left', fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim(-0.05, 1.05)\n",
    "ax.set_ylim(0, 105)\n",
    "\n",
    "# Add secondary x-axis for Alignment\n",
    "ax2 = ax.twiny()\n",
    "ax2.set_xlim(ax.get_xlim())\n",
    "ax2.set_xticks([0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "ax2.set_xticklabels(['1.0', '0.8', '0.6', '0.4', '0.2', '0.0'])\n",
    "ax2.set_xlabel('Alignment (A = 1 - λ)', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{ANALYSIS_DIR}/figures/fig2_all_collapse_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.savefig(f'{ANALYSIS_DIR}/figures/fig2_all_collapse_curves.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: fig2_all_collapse_curves.png/pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 3: Bootstrap Distribution of λ* for Claude 4 Sonnet\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (model_name, b) in enumerate(bootstrap_results.items()):\n",
    "    if idx >= 6:\n",
    "        break\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    \n",
    "    if b['samples']:\n",
    "        ax.hist(b['samples'], bins=30, color=all_data[model_name]['config']['color'], \n",
    "                alpha=0.7, edgecolor='black')\n",
    "        \n",
    "        # Add vertical lines for CI\n",
    "        if b['ci_lower'] and b['ci_upper']:\n",
    "            ax.axvline(b['point_estimate'], color='red', linewidth=2, label=f\"λ*={b['point_estimate']:.3f}\")\n",
    "            ax.axvline(b['ci_lower'], color='red', linestyle='--', linewidth=1.5)\n",
    "            ax.axvline(b['ci_upper'], color='red', linestyle='--', linewidth=1.5)\n",
    "            ax.axvspan(b['ci_lower'], b['ci_upper'], alpha=0.2, color='red', label='95% CI')\n",
    "    \n",
    "    ax.set_title(f\"{model_name}\\nλ*={b['point_estimate']:.3f} [{b['ci_lower']:.3f}, {b['ci_upper']:.3f}]\" \n",
    "                 if b['ci_lower'] else model_name, fontsize=11)\n",
    "    ax.set_xlabel('λ*')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "plt.suptitle('Bootstrap Distributions of λ* (n=1000)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{ANALYSIS_DIR}/figures/fig3_bootstrap_distributions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: fig3_bootstrap_distributions.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary Table for Publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary table\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "for model_name in sorted(metrics.keys(), key=lambda x: metrics[x]['baseline']):\n",
    "    m = metrics[model_name]\n",
    "    b = bootstrap_results[model_name]\n",
    "    s = supplementary[model_name]\n",
    "    \n",
    "    row = {\n",
    "        'Model': model_name,\n",
    "        'N': m['n_problems'],\n",
    "        'Baseline': f\"{m['baseline']*100:.1f}%\",\n",
    "        'λ*': f\"{m['lambda_crit']:.3f}\" if m['lambda_crit'] else 'N/A',\n",
    "        '95% CI': f\"[{b['ci_lower']:.3f}, {b['ci_upper']:.3f}]\" if b['ci_lower'] else 'N/A',\n",
    "        'Δ@λ=1': f\"{s['delta_at_1']*100:+.1f}%\",\n",
    "        'Harm Area': f\"{s['harm_area']:.3f}\",\n",
    "        'Benefit Area': f\"{s['benefit_area']:.3f}\"\n",
    "    }\n",
    "    summary_data.append(row)\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "print('='*100)\n",
    "print('TABLE 1: Summary of A3 Scaling Law Experiment Results')\n",
    "print('='*100)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Save as CSV\n",
    "summary_df.to_csv(f'{ANALYSIS_DIR}/table1_summary.csv', index=False)\n",
    "print(f\"\\nSaved: table1_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all results into a single JSON\n",
    "\n",
    "final_results = {\n",
    "    'experiment': 'A3_Scaling_Law_Statistical_Analysis',\n",
    "    'date': pd.Timestamp.now().strftime('%Y-%m-%d'),\n",
    "    'n_models': len(metrics),\n",
    "    'models': {}\n",
    "}\n",
    "\n",
    "for model_name in metrics.keys():\n",
    "    m = metrics[model_name]\n",
    "    b = bootstrap_results[model_name]\n",
    "    s = supplementary[model_name]\n",
    "    \n",
    "    final_results['models'][model_name] = {\n",
    "        'basic_metrics': {\n",
    "            'n_problems': m['n_problems'],\n",
    "            'baseline_accuracy': m['baseline'],\n",
    "            'accuracy_by_lambda': {str(k): v for k, v in m['acc_by_lam'].items()},\n",
    "            'lambda_crit_point': m['lambda_crit']\n",
    "        },\n",
    "        'bootstrap': {\n",
    "            'lambda_crit_mean': b['boot_mean'],\n",
    "            'lambda_crit_std': b['boot_std'],\n",
    "            'ci_lower': b['ci_lower'],\n",
    "            'ci_upper': b['ci_upper'],\n",
    "            'n_valid_samples': b['n_valid_samples']\n",
    "        },\n",
    "        'supplementary': s\n",
    "    }\n",
    "\n",
    "save_json(final_results, f'{ANALYSIS_DIR}/a3_complete_analysis.json')\n",
    "print(f\"Saved: a3_complete_analysis.json\")\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('ANALYSIS COMPLETE')\n",
    "print('='*70)\n",
    "print(f'\\nAll outputs saved to: {ANALYSIS_DIR}')\n",
    "print('\\nFiles generated:')\n",
    "print('  - figures/fig1_baseline_vs_lambda_crit.png/pdf')\n",
    "print('  - figures/fig2_all_collapse_curves.png/pdf')\n",
    "print('  - figures/fig3_bootstrap_distributions.png')\n",
    "print('  - table1_summary.csv')\n",
    "print('  - a3_complete_analysis.json')"
   ]
  }
 ]
}
