{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E8': GPT-4o Cross-Model Validation (FIXED VERSION)\n",
    "\n",
    "**Paper**: A2 (Cue-Dominant Extraction Explains Length Effects)\n",
    "\n",
    "**Purpose**: Cross-model validation of E8 findings using GPT-4o.\n",
    "\n",
    "**FIXES**: Uses complete final step replacement (same as E8 v2).\n",
    "\n",
    "**Cost-optimized design**:\n",
    "- L ∈ {10, 20} only (2 lengths vs 4)\n",
    "- c = 0.8 only (single corruption level)\n",
    "- cue ∈ {present, absent}\n",
    "\n",
    "**Expected inferences**: 188 × 2 × 2 = ~752\n",
    "\n",
    "**Date**: 2026-01-03\n",
    "**VERSION**: 2.0 (FIXED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Google Drive Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "EXPERIMENT_NAME = 'E8prime_GPT4o_v2'\n",
    "EXPERIMENT_DATE = datetime.now().strftime('%Y%m%d')\n",
    "\n",
    "BASE_DIR = '/content/drive/MyDrive/CoT_Experiment'\n",
    "V3_DATA_DIR = f'{BASE_DIR}/full_experiment_v3_20251224'\n",
    "TRACES_DIR = f'{V3_DATA_DIR}/clean_traces'\n",
    "\n",
    "SAVE_DIR = f'{BASE_DIR}/{EXPERIMENT_NAME}_{EXPERIMENT_DATE}'\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(f'{SAVE_DIR}/results', exist_ok=True)\n",
    "\n",
    "print(f'Experiment: {EXPERIMENT_NAME}')\n",
    "print(f'Save directory: {SAVE_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets openai matplotlib pandas tqdm scipy -q\n",
    "print('Dependencies installed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import random\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from typing import List, Dict, Tuple, Optional, Any, Set\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# =============================================================================\n",
    "# Configuration - Cost-optimized for GPT-4o\n",
    "# =============================================================================\n",
    "GLOBAL_SEED = 20251224\n",
    "E8_SEED = 20260103\n",
    "\n",
    "LENGTHS = [10, 20]  # Reduced from [5, 10, 15, 20]\n",
    "CORRUPTION_LEVEL = 0.8  # Single level\n",
    "CUE_CONDITIONS = ['present', 'absent']\n",
    "\n",
    "API_MAX_TOKENS_ANSWER = 256\n",
    "API_RETRY_DELAY = 1.0\n",
    "API_RATE_LIMIT_DELAY = 0.5\n",
    "\n",
    "print('='*70)\n",
    "print('E8\\': GPT-4o CROSS-MODEL VALIDATION (FIXED)')\n",
    "print('='*70)\n",
    "print(f'  Lengths: {LENGTHS}')\n",
    "print(f'  Corruption: {CORRUPTION_LEVEL}')\n",
    "print(f'  Cue: {CUE_CONDITIONS}')\n",
    "print('='*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GSM8KProblem:\n",
    "    index: int\n",
    "    question: str\n",
    "    answer_text: str\n",
    "    final_answer: int\n",
    "\n",
    "@dataclass\n",
    "class CleanTrace:\n",
    "    problem_index: int\n",
    "    I: int\n",
    "    steps: List[str]\n",
    "    full_text: str\n",
    "\n",
    "@dataclass\n",
    "class CorruptedTrace:\n",
    "    problem_index: int\n",
    "    L: int\n",
    "    c: float\n",
    "    cue_condition: str\n",
    "    K_corrupt: int\n",
    "    corrupted_steps: List[int]\n",
    "    corruption_types: Dict[int, str]\n",
    "    steps: List[str]\n",
    "    full_text: str\n",
    "    seed: int\n",
    "\n",
    "@dataclass\n",
    "class ExperimentResult:\n",
    "    problem_index: int\n",
    "    model: str\n",
    "    L: int\n",
    "    c: float\n",
    "    cue_condition: str\n",
    "    K_corrupt: int\n",
    "    K_clean: int\n",
    "    model_answer: Optional[int]\n",
    "    correct_answer: int\n",
    "    is_correct: bool\n",
    "    raw_output: str\n",
    "    timestamp: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derive_seed(global_seed: int, problem_id: int, L: int, c: float, cue: str) -> int:\n",
    "    key = f\"{global_seed}|E8|{problem_id}|L={L}|c={c}|cue={cue}\"\n",
    "    h = hashlib.sha256(key.encode(\"utf-8\")).hexdigest()\n",
    "    return int(h[:8], 16)\n",
    "\n",
    "def save_json(data: Any, filepath: str):\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def load_json(filepath: str) -> Any:\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load problems\n",
    "problems_path = f'{V3_DATA_DIR}/problems_v3.json'\n",
    "problems_data = load_json(problems_path)\n",
    "problems = [GSM8KProblem(**p) for p in problems_data]\n",
    "prob_map = {p.index: p for p in problems}\n",
    "print(f'Loaded {len(problems)} problems')\n",
    "\n",
    "# Load traces for L=10 and L=20\n",
    "trace_maps = {}\n",
    "for L in LENGTHS:\n",
    "    trace_path = f'{TRACES_DIR}/clean_traces_I{L}_v3.json'\n",
    "    traces_data = load_json(trace_path)\n",
    "    traces = [CleanTrace(**t) for t in traces_data]\n",
    "    trace_maps[L] = {t.problem_index: t for t in traces}\n",
    "    print(f'Loaded {len(traces)} traces for L={L}')\n",
    "\n",
    "# Find common problems (need traces for ALL lengths including L=5,10,15,20 for consistency)\n",
    "# Load L=5 and L=15 just to check common indices\n",
    "all_lengths = [5, 10, 15, 20]\n",
    "all_trace_maps = {}\n",
    "for L in all_lengths:\n",
    "    trace_path = f'{TRACES_DIR}/clean_traces_I{L}_v3.json'\n",
    "    if os.path.exists(trace_path):\n",
    "        traces_data = load_json(trace_path)\n",
    "        traces = [CleanTrace(**t) for t in traces_data]\n",
    "        all_trace_maps[L] = {t.problem_index: t for t in traces}\n",
    "\n",
    "common_indices = set.intersection(*[set(tm.keys()) for tm in all_trace_maps.values()])\n",
    "print(f'\\nCommon problems across L={all_lengths}: {len(common_indices)}')\n",
    "\n",
    "experiment_problems = [p for p in problems if p.index in common_indices]\n",
    "print(f'Using {len(experiment_problems)} problems')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Corruption Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_corrupted_steps(L: int, c: float, seed: int) -> List[int]:\n",
    "    K = int(round(c * (L - 1)))\n",
    "    if K == 0:\n",
    "        return []\n",
    "    steps = list(range(1, L))\n",
    "    rng = random.Random(seed)\n",
    "    rng.shuffle(steps)\n",
    "    return sorted(steps[:K])\n",
    "\n",
    "def assign_corruption_types(corrupted_steps: List[int], seed: int) -> Dict[int, str]:\n",
    "    K = len(corrupted_steps)\n",
    "    if K == 0:\n",
    "        return {}\n",
    "    n_irr = (K * 1) // 5\n",
    "    n_loc = (K * 2) // 5\n",
    "    n_wrong = K - n_irr - n_loc\n",
    "    if n_wrong == 0 and K > 0:\n",
    "        n_wrong = 1\n",
    "        if n_loc > 0:\n",
    "            n_loc -= 1\n",
    "        elif n_irr > 0:\n",
    "            n_irr -= 1\n",
    "    rng = random.Random(seed + 1)\n",
    "    perm = corrupted_steps[:]\n",
    "    rng.shuffle(perm)\n",
    "    type_map = {}\n",
    "    for s in perm[:n_irr]:\n",
    "        type_map[s] = \"IRR\"\n",
    "    for s in perm[n_irr:n_irr + n_loc]:\n",
    "        type_map[s] = \"LOC\"\n",
    "    for s in perm[n_irr + n_loc:]:\n",
    "        type_map[s] = \"WRONG\"\n",
    "    return type_map\n",
    "\n",
    "IRRELEVANT_TEMPLATES = [\n",
    "    \"Compute an auxiliary value: aux = {a} + {b} = {result}, but it will not be used later.\",\n",
    "    \"Compute a side quantity: aux = {a} * 2 = {result}, unrelated to the final result.\",\n",
    "]\n",
    "\n",
    "WRONG_CONSTRAINT_TEMPLATES = [\n",
    "    \"Fix an intermediate condition: set {var} = {wrong_value} as a given constraint.\",\n",
    "    \"Assume the total is {var} = {wrong_value} and proceed using this fixed value.\",\n",
    "]\n",
    "\n",
    "def generate_irrelevant_step(step_num: int, seed: int) -> str:\n",
    "    rng = random.Random(seed)\n",
    "    a, b = rng.randint(2, 20), rng.randint(2, 20)\n",
    "    template = rng.choice(IRRELEVANT_TEMPLATES)\n",
    "    result = a + b if '+' in template else a * 2\n",
    "    return template.format(a=a, b=b, result=result)\n",
    "\n",
    "def generate_local_error_step(original_step: str, seed: int) -> str:\n",
    "    rng = random.Random(seed)\n",
    "    numbers = re.findall(r'\\d+', original_step)\n",
    "    if not numbers:\n",
    "        return f\"Compute t = 10 * 3 = {rng.randint(28, 32)}.\"\n",
    "    original_result = int(numbers[-1])\n",
    "    offset = rng.choice([-3, -2, -1, 1, 2, 3])\n",
    "    wrong_result = max(0, original_result + offset)\n",
    "    modified = re.sub(r'= (\\d+)\\.$', f'= {wrong_result}.', original_step)\n",
    "    if modified == original_step:\n",
    "        modified = re.sub(r'(\\d+)\\.$', f'{wrong_result}.', original_step)\n",
    "    return modified\n",
    "\n",
    "def generate_wrong_constraint_step(step_num: int, seed: int) -> str:\n",
    "    rng = random.Random(seed)\n",
    "    var = rng.choice(['x', 'total', 'result', 'n'])\n",
    "    wrong_value = rng.randint(10, 100)\n",
    "    template = rng.choice(WRONG_CONSTRAINT_TEMPLATES)\n",
    "    return template.format(var=var, wrong_value=wrong_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. FIXED: Final Step Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_final_step_with_cue(answer: int) -> str:\n",
    "    \"\"\"FIXED: Complete replacement with clean cue\"\"\"\n",
    "    return f\"Therefore, the final answer is Final = {answer}.\"\n",
    "\n",
    "def create_final_step_without_cue() -> str:\n",
    "    \"\"\"FIXED: Complete replacement without cue\"\"\"\n",
    "    return \"The reasoning steps above lead to the solution. The calculation is now complete.\"\n",
    "\n",
    "print(\"FIXED final step templates:\")\n",
    "print(f\"  With cue: {create_final_step_with_cue(70000)}\")\n",
    "print(f\"  Without cue: {create_final_step_without_cue()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Trace Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corrupted_trace(\n",
    "    clean_trace: CleanTrace,\n",
    "    correct_answer: int,\n",
    "    c: float,\n",
    "    cue_condition: str,\n",
    "    seed: int\n",
    ") -> CorruptedTrace:\n",
    "    L = clean_trace.I\n",
    "    corrupted_steps = pick_corrupted_steps(L, c, seed)\n",
    "    corruption_types = assign_corruption_types(corrupted_steps, seed)\n",
    "    K_corrupt = len(corrupted_steps)\n",
    "    \n",
    "    new_steps = []\n",
    "    for i, step_content in enumerate(clean_trace.steps[:-1]):\n",
    "        step_num = i + 1\n",
    "        if step_num in corrupted_steps:\n",
    "            ctype = corruption_types[step_num]\n",
    "            step_seed = seed + step_num * 1000\n",
    "            if ctype == 'IRR':\n",
    "                new_content = generate_irrelevant_step(step_num, step_seed)\n",
    "            elif ctype == 'LOC':\n",
    "                new_content = generate_local_error_step(step_content, step_seed)\n",
    "            else:\n",
    "                new_content = generate_wrong_constraint_step(step_num, step_seed)\n",
    "            new_steps.append(new_content)\n",
    "        else:\n",
    "            new_steps.append(step_content)\n",
    "    \n",
    "    # FIXED: Complete final step replacement\n",
    "    if cue_condition == 'present':\n",
    "        final_step = create_final_step_with_cue(correct_answer)\n",
    "    else:\n",
    "        final_step = create_final_step_without_cue()\n",
    "    new_steps.append(final_step)\n",
    "    \n",
    "    lines = ['[[COT_START]]']\n",
    "    for i, content in enumerate(new_steps):\n",
    "        lines.append(f'Step {i+1}: {content}')\n",
    "    lines.append('[[COT_END]]')\n",
    "    full_text = '\\n'.join(lines)\n",
    "    \n",
    "    return CorruptedTrace(\n",
    "        problem_index=clean_trace.problem_index,\n",
    "        L=L, c=c, cue_condition=cue_condition,\n",
    "        K_corrupt=K_corrupt, corrupted_steps=corrupted_steps,\n",
    "        corruption_types=corruption_types, steps=new_steps,\n",
    "        full_text=full_text, seed=seed\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. API Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "OPENAI_API_KEY = getpass('Enter OpenAI API Key: ')\n",
    "print('API Key set.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "MODEL = 'gpt-4o'\n",
    "\n",
    "def call_gpt4o(system_prompt: str, user_prompt: str, max_tokens: int = 1024, retries: int = 3) -> str:\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=MODEL, max_tokens=max_tokens,\n",
    "                messages=[{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": user_prompt}],\n",
    "                temperature=0\n",
    "            )\n",
    "            time.sleep(API_RATE_LIMIT_DELAY)\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f'API error (attempt {attempt+1}): {e}')\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(API_RETRY_DELAY * (attempt + 1))\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "test_response = call_gpt4o(\"You output ONLY JSON.\", 'Respond: {\"test\": \"ok\"}', max_tokens=50)\n",
    "print(f'API test: {test_response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a calculator that outputs ONLY JSON.\n",
    "Output exactly: {\"final\": <number>}\n",
    "Do NOT write any explanation.\n",
    "\"\"\"\n",
    "\n",
    "def create_prompt(problem: GSM8KProblem, trace_text: str) -> str:\n",
    "    return f\"\"\"Problem: {problem.question}\n",
    "\n",
    "Reasoning trace:\n",
    "{trace_text}\n",
    "\n",
    "OUTPUT ONLY: {{\"final\": <number>}}\"\"\"\n",
    "\n",
    "def parse_model_answer(response: str) -> Optional[int]:\n",
    "    match = re.search(r'\\{\\s*\"final\"\\s*:\\s*(-?\\d+(?:\\.\\d+)?)\\s*\\}', response)\n",
    "    if match:\n",
    "        return int(round(float(match.group(1))))\n",
    "    match = re.search(r'\"final\"\\s*:\\s*(-?\\d+(?:\\.\\d+)?)', response)\n",
    "    if match:\n",
    "        return int(round(float(match.group(1))))\n",
    "    matches = re.findall(r'(-?\\d+)', response)\n",
    "    if matches:\n",
    "        return int(matches[-1])\n",
    "    return None\n",
    "\n",
    "def run_single_experiment(problem: GSM8KProblem, trace: CorruptedTrace) -> ExperimentResult:\n",
    "    user_prompt = create_prompt(problem, trace.full_text)\n",
    "    response = call_gpt4o(SYSTEM_PROMPT, user_prompt, max_tokens=API_MAX_TOKENS_ANSWER)\n",
    "    model_answer = parse_model_answer(response)\n",
    "    is_correct = (model_answer == problem.final_answer) if model_answer is not None else False\n",
    "    \n",
    "    return ExperimentResult(\n",
    "        problem_index=problem.index, model=MODEL, L=trace.L, c=trace.c,\n",
    "        cue_condition=trace.cue_condition, K_corrupt=trace.K_corrupt,\n",
    "        K_clean=trace.L - trace.K_corrupt, model_answer=model_answer,\n",
    "        correct_answer=problem.final_answer, is_correct=is_correct,\n",
    "        raw_output=response, timestamp=datetime.now().isoformat()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*70)\n",
    "print('E8\\': GPT-4o CROSS-MODEL VALIDATION (FIXED)')\n",
    "print('='*70)\n",
    "\n",
    "total_conditions = len(LENGTHS) * len(CUE_CONDITIONS)\n",
    "total_inferences = len(experiment_problems) * total_conditions\n",
    "print(f'Problems: {len(experiment_problems)}')\n",
    "print(f'Conditions: {total_conditions}')\n",
    "print(f'Total inferences: {total_inferences}')\n",
    "print('='*70)\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for L in LENGTHS:\n",
    "    for cue in CUE_CONDITIONS:\n",
    "        condition_name = f'L={L}_c={CORRUPTION_LEVEL}_cue={cue}'\n",
    "        print(f'\\nRunning: {condition_name}')\n",
    "        \n",
    "        condition_results = []\n",
    "        \n",
    "        for prob in tqdm(experiment_problems, desc=condition_name):\n",
    "            clean_trace = trace_maps[L].get(prob.index)\n",
    "            if clean_trace is None:\n",
    "                continue\n",
    "            \n",
    "            seed = derive_seed(E8_SEED, prob.index, L, CORRUPTION_LEVEL, cue)\n",
    "            corrupted_trace = create_corrupted_trace(clean_trace, prob.final_answer, CORRUPTION_LEVEL, cue, seed)\n",
    "            result = run_single_experiment(prob, corrupted_trace)\n",
    "            \n",
    "            condition_results.append(result)\n",
    "            all_results.append(result)\n",
    "        \n",
    "        acc = sum(r.is_correct for r in condition_results) / len(condition_results) if condition_results else 0\n",
    "        print(f'  → Accuracy: {acc*100:.1f}%')\n",
    "\n",
    "print(f'\\n\\nTotal experiments: {len(all_results)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json([asdict(r) for r in all_results], f'{SAVE_DIR}/results/E8prime_GPT4o_results.json')\n",
    "print(f'Results saved')\n",
    "\n",
    "df = pd.DataFrame([asdict(r) for r in all_results])\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('E8\\' GPT-4o RESULTS')\n",
    "print('='*70)\n",
    "\n",
    "for L in LENGTHS:\n",
    "    for cue in CUE_CONDITIONS:\n",
    "        mask = (df['L'] == L) & (df['cue_condition'] == cue)\n",
    "        acc = df[mask]['is_correct'].mean()\n",
    "        print(f'L={L}, cue={cue}: {acc*100:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L effect analysis\n",
    "print('\\n' + '='*70)\n",
    "print('L EFFECT ANALYSIS')\n",
    "print('='*70)\n",
    "\n",
    "for cue in CUE_CONDITIONS:\n",
    "    acc_10 = df[(df['L'] == 10) & (df['cue_condition'] == cue)]['is_correct'].mean()\n",
    "    acc_20 = df[(df['L'] == 20) & (df['cue_condition'] == cue)]['is_correct'].mean()\n",
    "    diff = acc_20 - acc_10\n",
    "    \n",
    "    print(f'\\nCue {cue}:')\n",
    "    print(f'  L=10: {acc_10*100:.1f}%')\n",
    "    print(f'  L=20: {acc_20*100:.1f}%')\n",
    "    print(f'  L effect: {diff*100:+.1f}pp')\n",
    "    \n",
    "    if cue == 'present' and abs(diff) < 0.1:\n",
    "        print(f'  ✓ L-insensitive (cue dominates)')\n",
    "    elif cue == 'absent' and diff > 0.05:\n",
    "        print(f'  ✓ L-sensitive (redundancy matters)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "summary = {\n",
    "    'experiment': 'E8prime_GPT4o_v2_FIXED',\n",
    "    'model': MODEL,\n",
    "    'date': EXPERIMENT_DATE,\n",
    "    'n_problems': len(experiment_problems),\n",
    "    'total_inferences': len(all_results),\n",
    "    'conditions': {\n",
    "        'lengths': LENGTHS,\n",
    "        'corruption_level': CORRUPTION_LEVEL,\n",
    "        'cue_conditions': CUE_CONDITIONS\n",
    "    },\n",
    "    'results': {},\n",
    "    'l_effects': {}\n",
    "}\n",
    "\n",
    "for L in LENGTHS:\n",
    "    for cue in CUE_CONDITIONS:\n",
    "        mask = (df['L'] == L) & (df['cue_condition'] == cue)\n",
    "        key = f'L{L}_cue{cue}'\n",
    "        summary['results'][key] = {\n",
    "            'accuracy': float(df[mask]['is_correct'].mean()),\n",
    "            'n_correct': int(df[mask]['is_correct'].sum()),\n",
    "            'n_total': int(mask.sum())\n",
    "        }\n",
    "\n",
    "for cue in CUE_CONDITIONS:\n",
    "    acc_10 = summary['results'][f'L10_cue{cue}']['accuracy']\n",
    "    acc_20 = summary['results'][f'L20_cue{cue}']['accuracy']\n",
    "    summary['l_effects'][f'cue{cue}'] = acc_20 - acc_10\n",
    "\n",
    "save_json(summary, f'{SAVE_DIR}/results/E8prime_GPT4o_summary.json')\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('E8\\' COMPLETE')\n",
    "print('='*70)\n",
    "print(f'Total experiments: {len(all_results)}')\n",
    "print(f'\\nL Effects (L=20 - L=10):')\n",
    "for cue, effect in summary['l_effects'].items():\n",
    "    print(f'  {cue}: {effect*100:+.1f}pp')\n",
    "print(f'\\nFiles saved to: {SAVE_DIR}')"
   ]
  }
 ]
}
