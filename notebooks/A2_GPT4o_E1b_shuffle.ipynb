{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2-GPT4o: E1b Shuffle Experiment at c=0.4\n",
    "\n",
    "**Paper**: A2 (Cue-Dominant Extraction)\n",
    "\n",
    "**Purpose**: Replicate E1b (shuffle experiment at c=0.4) using GPT-4o to test model generality.\n",
    "\n",
    "**Design**:\n",
    "- Model: GPT-4o\n",
    "- c = 0.4 (same as Claude E1b)\n",
    "- L = 10\n",
    "- Conditions: Original vs Shuffled\n",
    "\n",
    "**Claude E1b results (for comparison)**:\n",
    "- Original (c=0.4): 83.9%\n",
    "- Shuffled (c=0.4): 91.5% (+7.5 pp, P=0.001)\n",
    "\n",
    "**Expected inference count**: 398 (199 × 2 conditions)\n",
    "\n",
    "**Date**: 2026-01-03\n",
    "**GLOBAL_SEED**: 20251224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Google Drive Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "EXPERIMENT_NAME = 'A2_GPT4o_E1b_shuffle'\n",
    "EXPERIMENT_DATE = datetime.now().strftime('%Y%m%d')\n",
    "\n",
    "BASE_DIR = '/content/drive/MyDrive/CoT_Experiment'\n",
    "V3_DATA_DIR = f'{BASE_DIR}/full_experiment_v3_20251224'\n",
    "\n",
    "SAVE_DIR = f'{BASE_DIR}/{EXPERIMENT_NAME}_{EXPERIMENT_DATE}'\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(f'{SAVE_DIR}/results', exist_ok=True)\n",
    "\n",
    "print(f'Experiment: {EXPERIMENT_NAME}')\n",
    "print(f'V3 data directory: {V3_DATA_DIR}')\n",
    "print(f'Save directory: {SAVE_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets openai matplotlib pandas tqdm scipy -q\n",
    "print('Dependencies installed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import random\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from typing import List, Dict, Tuple, Optional, Any\n",
    "from dataclasses import dataclass, asdict, field\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# =============================================================================\n",
    "# Global Configuration\n",
    "# =============================================================================\n",
    "GLOBAL_SEED = 20251224\n",
    "SHUFFLE_SEED = 20250102\n",
    "\n",
    "# Experiment parameters\n",
    "L = 10\n",
    "C_TARGET = 0.4\n",
    "\n",
    "# Corruption type ratio\n",
    "CORRUPTION_RATIO = {'IRR': 1, 'LOC': 2, 'WRONG': 2}\n",
    "\n",
    "# API settings\n",
    "API_MAX_TOKENS_ANSWER = 256\n",
    "API_RETRY_DELAY = 1.0\n",
    "API_RATE_LIMIT_DELAY = 0.5\n",
    "\n",
    "print('='*70)\n",
    "print('A2-GPT4o: E1b SHUFFLE EXPERIMENT AT c=0.4')\n",
    "print('='*70)\n",
    "print(f'  Model: GPT-4o')\n",
    "print(f'  GLOBAL_SEED: {GLOBAL_SEED}')\n",
    "print(f'  SHUFFLE_SEED: {SHUFFLE_SEED}')\n",
    "print(f'  L (trace length): {L}')\n",
    "print(f'  c (corruption fraction): {C_TARGET}')\n",
    "print(f'  K_corrupt = round(c * L) = {round(C_TARGET * L)}')\n",
    "print(f'  K_clean = L - K_corrupt = {L - round(C_TARGET * L)}')\n",
    "print('='*70)\n",
    "print('\\nClaude E1b results (for comparison):')\n",
    "print('  Original (c=0.4): 83.9%')\n",
    "print('  Shuffled (c=0.4): 91.5%')\n",
    "print('  Shuffle effect: +7.5 pp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GSM8KProblem:\n",
    "    index: int\n",
    "    question: str\n",
    "    answer_text: str\n",
    "    final_answer: int\n",
    "\n",
    "@dataclass\n",
    "class CleanTrace:\n",
    "    problem_index: int\n",
    "    I: int\n",
    "    steps: List[str]\n",
    "    full_text: str\n",
    "\n",
    "@dataclass\n",
    "class CorruptedTrace:\n",
    "    problem_index: int\n",
    "    L: int\n",
    "    c: float\n",
    "    original_order: List[int]\n",
    "    shuffled_order: List[int]\n",
    "    corrupted_steps: List[int]\n",
    "    corruption_types: Dict[int, str]\n",
    "    steps: List[str]\n",
    "    full_text: str\n",
    "    is_shuffled: bool\n",
    "    seed: int\n",
    "\n",
    "@dataclass\n",
    "class ExperimentResult:\n",
    "    problem_index: int\n",
    "    condition: str\n",
    "    model: str\n",
    "    L: int\n",
    "    c: float\n",
    "    K_clean: int\n",
    "    model_answer: Optional[int]\n",
    "    correct_answer: int\n",
    "    is_correct: bool\n",
    "    raw_output: str\n",
    "    timestamp: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derive_seed(global_seed: int, problem_id: int, I: int, lam: float, replicate_id: int = 0) -> int:\n",
    "    key = f\"{global_seed}|{problem_id}|I={I}|lam={lam}|rep={replicate_id}\"\n",
    "    h = hashlib.sha256(key.encode(\"utf-8\")).hexdigest()\n",
    "    return int(h[:8], 16)\n",
    "\n",
    "def save_json(data: Any, filepath: str):\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def load_json(filepath: str) -> Any:\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Existing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problems_path = f'{V3_DATA_DIR}/problems_v3.json'\n",
    "problems_data = load_json(problems_path)\n",
    "problems = [GSM8KProblem(**p) for p in problems_data]\n",
    "print(f'Loaded {len(problems)} problems')\n",
    "\n",
    "traces_path = f'{V3_DATA_DIR}/clean_traces/clean_traces_I10_v3.json'\n",
    "traces_data = load_json(traces_path)\n",
    "clean_traces = [CleanTrace(**t) for t in traces_data]\n",
    "trace_map = {t.problem_index: t for t in clean_traces}\n",
    "print(f'Loaded {len(clean_traces)} clean traces (L=10)')\n",
    "\n",
    "prob_map = {p.index: p for p in problems}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Corruption Logic (Same as E1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_corrupted_steps(L: int, c: float, seed: int) -> List[int]:\n",
    "    K = int(round(c * L))\n",
    "    if K == 0:\n",
    "        return []\n",
    "    steps = list(range(1, L + 1))\n",
    "    rng = random.Random(seed)\n",
    "    rng.shuffle(steps)\n",
    "    return sorted(steps[:K])\n",
    "\n",
    "def assign_corruption_types(corrupted_steps: List[int], seed: int) -> Dict[int, str]:\n",
    "    K = len(corrupted_steps)\n",
    "    if K == 0:\n",
    "        return {}\n",
    "    \n",
    "    n_irr = (K * 1) // 5\n",
    "    n_loc = (K * 2) // 5\n",
    "    n_wrong = K - n_irr - n_loc\n",
    "    \n",
    "    if n_wrong == 0 and K > 0:\n",
    "        n_wrong = 1\n",
    "        if n_loc > 0:\n",
    "            n_loc -= 1\n",
    "        elif n_irr > 0:\n",
    "            n_irr -= 1\n",
    "    \n",
    "    rng = random.Random(seed + 1)\n",
    "    perm = corrupted_steps[:]\n",
    "    rng.shuffle(perm)\n",
    "    \n",
    "    type_map = {}\n",
    "    for s in perm[:n_irr]:\n",
    "        type_map[s] = \"IRR\"\n",
    "    for s in perm[n_irr:n_irr + n_loc]:\n",
    "        type_map[s] = \"LOC\"\n",
    "    for s in perm[n_irr + n_loc:]:\n",
    "        type_map[s] = \"WRONG\"\n",
    "    \n",
    "    return type_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Corruption Templates (Same as E1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IRRELEVANT_TEMPLATES = [\n",
    "    \"Compute an auxiliary value: aux = {a} + {b} = {result}, but it will not be used later.\",\n",
    "    \"Compute a side quantity: aux = {a} * 2 = {result}, unrelated to the final result.\",\n",
    "    \"Note that we can also compute aux = {a} - {b} = {result}, though this is not needed.\",\n",
    "]\n",
    "\n",
    "WRONG_CONSTRAINT_TEMPLATES = [\n",
    "    \"Fix an intermediate condition: set {var} = {wrong_value} as a given constraint for the rest of the steps.\",\n",
    "    \"Assume the total is {var} = {wrong_value} and proceed using this fixed value.\",\n",
    "]\n",
    "\n",
    "def generate_irrelevant_step(step_num: int, seed: int) -> str:\n",
    "    rng = random.Random(seed)\n",
    "    a = rng.randint(2, 20)\n",
    "    b = rng.randint(2, 20)\n",
    "    template = rng.choice(IRRELEVANT_TEMPLATES)\n",
    "    if '+' in template:\n",
    "        result = a + b\n",
    "    elif '*' in template:\n",
    "        result = a * 2\n",
    "    else:\n",
    "        result = a - b\n",
    "    return template.format(a=a, b=b, result=result)\n",
    "\n",
    "def generate_local_error_step(original_step: str, seed: int) -> str:\n",
    "    rng = random.Random(seed)\n",
    "    numbers = re.findall(r'\\d+', original_step)\n",
    "    if not numbers:\n",
    "        return f\"Compute t = 10 * 3 = {rng.randint(28, 32)} (using the previous values).\"\n",
    "    original_result = int(numbers[-1])\n",
    "    offset = rng.choice([-3, -2, -1, 1, 2, 3])\n",
    "    wrong_result = max(0, original_result + offset)\n",
    "    modified = re.sub(r'= (\\d+)\\.$', f'= {wrong_result}.', original_step)\n",
    "    if modified == original_step:\n",
    "        modified = re.sub(r'(\\d+)\\.$', f'{wrong_result}.', original_step)\n",
    "    return modified\n",
    "\n",
    "def generate_wrong_constraint_step(step_num: int, seed: int) -> str:\n",
    "    rng = random.Random(seed)\n",
    "    var = rng.choice(['x', 'total', 'result', 'n'])\n",
    "    wrong_value = rng.randint(10, 100)\n",
    "    template = rng.choice(WRONG_CONSTRAINT_TEMPLATES)\n",
    "    return template.format(var=var, wrong_value=wrong_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Trace Generation (Original & Shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corrupted_trace(\n",
    "    clean_trace: CleanTrace,\n",
    "    c: float,\n",
    "    corruption_seed: int,\n",
    "    shuffle: bool = False,\n",
    "    shuffle_seed: int = None\n",
    ") -> CorruptedTrace:\n",
    "    \"\"\"Create corrupted trace with optional shuffling.\"\"\"\n",
    "    L = clean_trace.I\n",
    "    \n",
    "    # Step 1: Determine which steps to corrupt\n",
    "    corrupted_steps = pick_corrupted_steps(L, c, corruption_seed)\n",
    "    corruption_types = assign_corruption_types(corrupted_steps, corruption_seed)\n",
    "    \n",
    "    # Step 2: Apply corruption to get step contents\n",
    "    step_contents = []\n",
    "    for i, step_content in enumerate(clean_trace.steps):\n",
    "        step_num = i + 1\n",
    "        if step_num in corruption_types:\n",
    "            ctype = corruption_types[step_num]\n",
    "            step_seed = corruption_seed + step_num * 1000\n",
    "            if ctype == 'IRR':\n",
    "                new_content = generate_irrelevant_step(step_num, step_seed)\n",
    "            elif ctype == 'LOC':\n",
    "                new_content = generate_local_error_step(step_content, step_seed)\n",
    "            else:\n",
    "                new_content = generate_wrong_constraint_step(step_num, step_seed)\n",
    "            step_contents.append(new_content)\n",
    "        else:\n",
    "            step_contents.append(step_content)\n",
    "    \n",
    "    # Step 3: Shuffle if requested\n",
    "    original_order = list(range(1, L + 1))\n",
    "    if shuffle and shuffle_seed is not None:\n",
    "        shuffled_indices = list(range(L))\n",
    "        rng = random.Random(shuffle_seed)\n",
    "        rng.shuffle(shuffled_indices)\n",
    "        shuffled_order = [original_order[i] for i in shuffled_indices]\n",
    "        final_contents = [step_contents[i] for i in shuffled_indices]\n",
    "    else:\n",
    "        shuffled_order = original_order[:]\n",
    "        final_contents = step_contents\n",
    "    \n",
    "    # Step 4: Build full text\n",
    "    lines = ['[[COT_START]]']\n",
    "    for i, content in enumerate(final_contents):\n",
    "        lines.append(f'Step {i+1}: {content}')\n",
    "    lines.append('[[COT_END]]')\n",
    "    full_text = '\\n'.join(lines)\n",
    "    \n",
    "    return CorruptedTrace(\n",
    "        problem_index=clean_trace.problem_index,\n",
    "        L=L,\n",
    "        c=c,\n",
    "        original_order=original_order,\n",
    "        shuffled_order=shuffled_order,\n",
    "        corrupted_steps=corrupted_steps,\n",
    "        corruption_types=corruption_types,\n",
    "        steps=final_contents,\n",
    "        full_text=full_text,\n",
    "        is_shuffled=shuffle,\n",
    "        seed=corruption_seed\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test trace generation\n",
    "test_trace = clean_traces[0]\n",
    "test_corruption_seed = derive_seed(GLOBAL_SEED, test_trace.problem_index, L, C_TARGET)\n",
    "test_shuffle_seed = derive_seed(SHUFFLE_SEED, test_trace.problem_index, L, C_TARGET)\n",
    "\n",
    "original = create_corrupted_trace(test_trace, C_TARGET, test_corruption_seed, shuffle=False)\n",
    "shuffled = create_corrupted_trace(test_trace, C_TARGET, test_corruption_seed, shuffle=True, shuffle_seed=test_shuffle_seed)\n",
    "\n",
    "print('Test Trace Generation (c=0.4):')\n",
    "print(f'  Corrupted steps: {original.corrupted_steps}')\n",
    "print(f'  Corruption types: {original.corruption_types}')\n",
    "print(f'  K_corrupt: {len(original.corrupted_steps)}')\n",
    "print(f'  K_clean: {L - len(original.corrupted_steps)}')\n",
    "print(f'\\n  Original order: {original.shuffled_order}')\n",
    "print(f'  Shuffled order: {shuffled.shuffled_order}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. API Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "OPENAI_API_KEY = getpass('Enter OpenAI API Key: ')\n",
    "print('API Key set.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "MODEL = 'gpt-4o'\n",
    "\n",
    "def call_gpt4o(system_prompt: str, user_prompt: str, max_tokens: int = 1024, retries: int = 3) -> str:\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=MODEL,\n",
    "                max_tokens=max_tokens,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt}\n",
    "                ],\n",
    "                temperature=0\n",
    "            )\n",
    "            time.sleep(API_RATE_LIMIT_DELAY)\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f'API error (attempt {attempt+1}): {e}')\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(API_RETRY_DELAY * (attempt + 1))\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "test_response = call_gpt4o(\n",
    "    \"You output ONLY JSON.\",\n",
    "    'Respond with exactly: {\"test\": \"ok\"}',\n",
    "    max_tokens=50\n",
    ")\n",
    "print(f'API test: {test_response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Experiment Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_SYSTEM_PROMPT = \"\"\"You are a calculator that outputs ONLY JSON.\n",
    "\n",
    "CRITICAL RULES:\n",
    "1. Your output MUST start with the character '{'\n",
    "2. Your output MUST be exactly: {\"final\": <number>}\n",
    "3. Replace <number> with an integer (the numerical answer)\n",
    "4. Do NOT write ANY explanation, reasoning, or text before or after the JSON\n",
    "5. Do NOT write \"I need to\" or \"Let me\" or any other words\n",
    "6. ONLY output the JSON object, nothing else\n",
    "\n",
    "CORRECT OUTPUT EXAMPLE:\n",
    "{\"final\": 42}\n",
    "\"\"\"\n",
    "\n",
    "def create_experiment_prompt(problem: GSM8KProblem, cot_text: str) -> Tuple[str, str]:\n",
    "    user = f\"\"\"Problem: {problem.question}\n",
    "\n",
    "Reasoning trace (use these steps as given facts):\n",
    "{cot_text}\n",
    "\n",
    "Based on the trace above, compute the final numerical answer.\n",
    "OUTPUT ONLY: {{\"final\": <number>}}\n",
    "START YOUR RESPONSE WITH '{{'\"\"\"\n",
    "    return EXPERIMENT_SYSTEM_PROMPT, user\n",
    "\n",
    "def parse_model_answer(response: str) -> Optional[int]:\n",
    "    match = re.search(r'\\{\\s*\"final\"\\s*:\\s*(-?\\d+(?:\\.\\d+)?)\\s*\\}', response)\n",
    "    if match:\n",
    "        return int(round(float(match.group(1))))\n",
    "    match = re.search(r\"\\{\\s*[\\\"']final[\\\"']\\s*:\\s*(-?\\d+(?:\\.\\d+)?)\\s*\\}\", response)\n",
    "    if match:\n",
    "        return int(round(float(match.group(1))))\n",
    "    match = re.search(r'\"final\"\\s*:\\s*(-?\\d+(?:\\.\\d+)?)', response)\n",
    "    if match:\n",
    "        return int(round(float(match.group(1))))\n",
    "    matches = re.findall(r'(?:^|\\s)(-?\\d+(?:\\.\\d+)?)(?:\\s|$|\\.|,)', response)\n",
    "    if matches:\n",
    "        return int(round(float(matches[-1])))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(\n",
    "    problem: GSM8KProblem,\n",
    "    cot_text: str,\n",
    "    condition: str,\n",
    "    L: int,\n",
    "    c: float\n",
    ") -> ExperimentResult:\n",
    "    sys_prompt, usr_prompt = create_experiment_prompt(problem, cot_text)\n",
    "    response = call_gpt4o(sys_prompt, usr_prompt, max_tokens=API_MAX_TOKENS_ANSWER)\n",
    "    \n",
    "    model_answer = parse_model_answer(response)\n",
    "    is_correct = (model_answer == problem.final_answer) if model_answer is not None else False\n",
    "    \n",
    "    K_clean = L - int(round(c * L))\n",
    "    \n",
    "    return ExperimentResult(\n",
    "        problem_index=problem.index,\n",
    "        condition=condition,\n",
    "        model=MODEL,\n",
    "        L=L,\n",
    "        c=c,\n",
    "        K_clean=K_clean,\n",
    "        model_answer=model_answer,\n",
    "        correct_answer=problem.final_answer,\n",
    "        is_correct=is_correct,\n",
    "        raw_output=response,\n",
    "        timestamp=datetime.now().isoformat()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*70)\n",
    "print('A2-GPT4o: E1b SHUFFLE EXPERIMENT (c=0.4)')\n",
    "print('='*70)\n",
    "print(f'Model: {MODEL}')\n",
    "print(f'Conditions: Original, Shuffled')\n",
    "print(f'Expected inferences: {len(problems) * 2}')\n",
    "print('='*70)\n",
    "\n",
    "results_original = []\n",
    "results_shuffled = []\n",
    "traces_log = []\n",
    "\n",
    "for prob in tqdm(problems, desc='E1b GPT-4o (Original + Shuffled)'):\n",
    "    if prob.index not in trace_map:\n",
    "        continue\n",
    "    \n",
    "    clean_trace = trace_map[prob.index]\n",
    "    \n",
    "    # Generate seeds\n",
    "    corruption_seed = derive_seed(GLOBAL_SEED, prob.index, L, C_TARGET)\n",
    "    shuffle_seed = derive_seed(SHUFFLE_SEED, prob.index, L, C_TARGET)\n",
    "    \n",
    "    # Condition 1: Original order\n",
    "    original_trace = create_corrupted_trace(clean_trace, C_TARGET, corruption_seed, shuffle=False)\n",
    "    result_original = run_experiment(prob, original_trace.full_text, 'original_c04', L, C_TARGET)\n",
    "    results_original.append(result_original)\n",
    "    \n",
    "    # Condition 2: Shuffled order\n",
    "    shuffled_trace = create_corrupted_trace(clean_trace, C_TARGET, corruption_seed, shuffle=True, shuffle_seed=shuffle_seed)\n",
    "    result_shuffled = run_experiment(prob, shuffled_trace.full_text, 'shuffled_c04', L, C_TARGET)\n",
    "    results_shuffled.append(result_shuffled)\n",
    "    \n",
    "    # Log traces\n",
    "    traces_log.append({\n",
    "        'problem_index': prob.index,\n",
    "        'original': asdict(original_trace),\n",
    "        'shuffled': asdict(shuffled_trace)\n",
    "    })\n",
    "\n",
    "print(f'\\nCompleted: {len(results_original) + len(results_shuffled)} experiments')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = results_original + results_shuffled\n",
    "save_json([asdict(r) for r in all_results], f'{SAVE_DIR}/results/A2_GPT4o_E1b_results.json')\n",
    "print(f'Results saved: {SAVE_DIR}/results/A2_GPT4o_E1b_results.json')\n",
    "\n",
    "save_json(traces_log, f'{SAVE_DIR}/results/A2_GPT4o_E1b_traces.json')\n",
    "print(f'Traces saved: {SAVE_DIR}/results/A2_GPT4o_E1b_traces.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = pd.DataFrame([asdict(r) for r in results_original])\n",
    "df_shuffled = pd.DataFrame([asdict(r) for r in results_shuffled])\n",
    "\n",
    "original_acc = df_original['is_correct'].mean()\n",
    "shuffled_acc = df_shuffled['is_correct'].mean()\n",
    "\n",
    "print('='*70)\n",
    "print('A2-GPT4o E1b RESULTS (c=0.4)')\n",
    "print('='*70)\n",
    "print(f'Original (c=0.4, L=10):  {original_acc:.1%} ({df_original[\"is_correct\"].sum()}/{len(df_original)})')\n",
    "print(f'Shuffled (c=0.4, L=10):  {shuffled_acc:.1%} ({df_shuffled[\"is_correct\"].sum()}/{len(df_shuffled)})')\n",
    "print(f'Shuffle effect:          {(shuffled_acc - original_acc)*100:+.1f} pp')\n",
    "print('='*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# McNemar's test\n",
    "N = len(results_original)\n",
    "both_correct = sum(1 for i in range(N) if results_original[i].is_correct and results_shuffled[i].is_correct)\n",
    "orig_only = sum(1 for i in range(N) if results_original[i].is_correct and not results_shuffled[i].is_correct)\n",
    "shuf_only = sum(1 for i in range(N) if not results_original[i].is_correct and results_shuffled[i].is_correct)\n",
    "both_wrong = sum(1 for i in range(N) if not results_original[i].is_correct and not results_shuffled[i].is_correct)\n",
    "\n",
    "print('\\nContingency Table:')\n",
    "print('                    Shuffled')\n",
    "print('                 Correct  Wrong')\n",
    "print(f'Original Correct   {both_correct:3d}     {orig_only:3d}')\n",
    "print(f'         Wrong     {shuf_only:3d}     {both_wrong:3d}')\n",
    "\n",
    "if orig_only + shuf_only > 0:\n",
    "    chi2 = (abs(orig_only - shuf_only) - 1)**2 / (orig_only + shuf_only)\n",
    "    p_value = 1 - stats.chi2.cdf(chi2, 1)\n",
    "    print(f'\\nMcNemar χ² = {chi2:.2f}, P = {p_value:.4f}')\n",
    "else:\n",
    "    chi2 = None\n",
    "    p_value = None\n",
    "    print('\\nNo discordant pairs for McNemar test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison with Claude\n",
    "claude_original = 83.9\n",
    "claude_shuffled = 91.5\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('COMPARISON: Claude vs GPT-4o (E1b, c=0.4)')\n",
    "print('='*70)\n",
    "print(f'{\"Condition\":<20} {\"Claude\":>12} {\"GPT-4o\":>12}')\n",
    "print('-'*50)\n",
    "print(f'{\"Original\":<20} {claude_original:>11.1f}% {original_acc*100:>11.1f}%')\n",
    "print(f'{\"Shuffled\":<20} {claude_shuffled:>11.1f}% {shuffled_acc*100:>11.1f}%')\n",
    "print(f'{\"Shuffle effect\":<20} {claude_shuffled-claude_original:>+11.1f}pp {(shuffled_acc-original_acc)*100:>+11.1f}pp')\n",
    "print('='*70)\n",
    "\n",
    "print('\\nINTERPRETATION:')\n",
    "if shuffled_acc >= original_acc:\n",
    "    print('✓ GPT-4o: Shuffling does NOT reduce accuracy')\n",
    "    print('✓ Order invariance confirmed for GPT-4o')\n",
    "    print('→ Cue-dominant extraction generalizes across models')\n",
    "else:\n",
    "    print('△ GPT-4o: Shuffling reduces accuracy')\n",
    "    print('△ Model-specific processing strategies may exist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {\n",
    "    'experiment': 'A2_GPT4o_E1b_shuffle',\n",
    "    'model': MODEL,\n",
    "    'date': EXPERIMENT_DATE,\n",
    "    'n_problems': N,\n",
    "    'c': C_TARGET,\n",
    "    'L': L,\n",
    "    'results': {\n",
    "        'original_accuracy': original_acc,\n",
    "        'shuffled_accuracy': shuffled_acc,\n",
    "        'shuffle_effect_pp': (shuffled_acc - original_acc) * 100\n",
    "    },\n",
    "    'mcnemar': {\n",
    "        'chi2': chi2,\n",
    "        'p_value': p_value\n",
    "    },\n",
    "    'contingency': {\n",
    "        'both_correct': both_correct,\n",
    "        'original_only': orig_only,\n",
    "        'shuffled_only': shuf_only,\n",
    "        'both_wrong': both_wrong\n",
    "    },\n",
    "    'comparison_claude': {\n",
    "        'original': claude_original,\n",
    "        'shuffled': claude_shuffled,\n",
    "        'effect': claude_shuffled - claude_original\n",
    "    }\n",
    "}\n",
    "\n",
    "save_json(summary, f'{SAVE_DIR}/results/A2_GPT4o_E1b_summary.json')\n",
    "\n",
    "print('='*70)\n",
    "print('A2-GPT4o E1b EXPERIMENT COMPLETE')\n",
    "print('='*70)\n",
    "print(f'Date: {EXPERIMENT_DATE}')\n",
    "print(f'Model: {MODEL}')\n",
    "print(f'Total experiments: {len(all_results)}')\n",
    "print(f'\\nResults:')\n",
    "print(f'  Original: {original_acc:.1%}')\n",
    "print(f'  Shuffled: {shuffled_acc:.1%}')\n",
    "print(f'  Effect:   {(shuffled_acc-original_acc)*100:+.1f} pp')\n",
    "print(f'\\nFiles saved to: {SAVE_DIR}')\n",
    "print('='*70)"
   ]
  }
 ]
}
