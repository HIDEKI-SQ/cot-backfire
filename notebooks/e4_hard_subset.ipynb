{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E4: Hard Subset Analysis\n",
    "\n",
    "**Purpose**: Avoid ceiling effect in Claude 4 Sonnet (92.5% baseline)\n",
    "\n",
    "**Approach**:\n",
    "1. **GSM8K-Hard-Consensus**: Problems where ≥4 models got WRONG in direct\n",
    "2. Test on this harder subset where baseline is lower\n",
    "3. Check if backfire becomes clearer\n",
    "\n",
    "**Hypothesis**:\n",
    "- On harder problems, Claude 4 Sonnet's λ* should be more clearly defined\n",
    "- The backfire effect should be more pronounced\n",
    "\n",
    "**λ conditions**: 0.0, 0.4, 0.8, 1.0 (4 points for efficiency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "EXPERIMENT_DATE = datetime.now().strftime('%Y%m%d')\n",
    "SAVE_DIR = '/content/drive/MyDrive/CoT_Experiment'\n",
    "SAVE_DIR_EXP = f'{SAVE_DIR}/e4_hard_subset_{EXPERIMENT_DATE}'\n",
    "os.makedirs(SAVE_DIR_EXP, exist_ok=True)\n",
    "os.makedirs(f'{SAVE_DIR_EXP}/results', exist_ok=True)\n",
    "\n",
    "print(f'Save directory: {SAVE_DIR_EXP}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets openai anthropic pandas tqdm matplotlib scipy -q\n",
    "print('Dependencies installed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Previous Direct Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import hashlib\n",
    "from typing import List, Dict, Optional, Any\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_json(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def save_json(data, filepath):\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    print(f'Saved: {filepath}')\n",
    "\n",
    "# Load direct results from all models\n",
    "# UPDATE THESE PATHS based on your folder structure\n",
    "DIRECT_RESULTS_PATHS = {\n",
    "    'Claude 3 Haiku': f'{SAVE_DIR}/a3_claude_haiku_20251228/results/direct_results_haiku.json',\n",
    "    'Claude 3.5 Haiku': f'{SAVE_DIR}/a3_claude_haiku35_20251228/results/direct_results_haiku35.json',\n",
    "    'Claude 4 Sonnet': f'{SAVE_DIR}/a3_claude_sonnet4_20251228/results/direct_results_sonnet4.json',\n",
    "    'GPT-3.5 Turbo': f'{SAVE_DIR}/a3_gpt_gpt35_20251228/results/direct_results_gpt35.json',\n",
    "    'GPT-4o-mini': f'{SAVE_DIR}/a3_gpt_gpt4omini_20251228/results/direct_results_gpt4omini.json',\n",
    "    'GPT-4o': f'{SAVE_DIR}/chatgpt_experiment_20251224/results/direct_results_gpt4o.json',\n",
    "}\n",
    "\n",
    "all_direct = {}\n",
    "for model, path in DIRECT_RESULTS_PATHS.items():\n",
    "    try:\n",
    "        data = load_json(path)\n",
    "        all_direct[model] = {r['problem_index']: r['is_correct'] for r in data}\n",
    "        print(f'✓ {model}: {len(data)} problems, {sum(r[\"is_correct\"] for r in data)/len(data):.1%} accuracy')\n",
    "    except Exception as e:\n",
    "        print(f'✗ {model}: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Identify Hard Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all problem indices\n",
    "all_problem_ids = set()\n",
    "for model, results in all_direct.items():\n",
    "    all_problem_ids.update(results.keys())\n",
    "\n",
    "# Count how many models got each problem WRONG\n",
    "problem_difficulty = {}\n",
    "for pid in all_problem_ids:\n",
    "    n_wrong = sum(1 for model in all_direct if pid in all_direct[model] and not all_direct[model][pid])\n",
    "    n_models = sum(1 for model in all_direct if pid in all_direct[model])\n",
    "    problem_difficulty[pid] = {'n_wrong': n_wrong, 'n_models': n_models}\n",
    "\n",
    "# Create hard subset: problems where >= 4 models got WRONG\n",
    "HARD_THRESHOLD = 4  # At least 4 models got it wrong\n",
    "\n",
    "hard_problems = [pid for pid, info in problem_difficulty.items() \n",
    "                 if info['n_wrong'] >= HARD_THRESHOLD]\n",
    "\n",
    "print(f'\\nTotal problems: {len(all_problem_ids)}')\n",
    "print(f'Hard problems (≥{HARD_THRESHOLD} models wrong): {len(hard_problems)}')\n",
    "\n",
    "# Distribution\n",
    "print('\\nDifficulty distribution:')\n",
    "for n in range(7):\n",
    "    count = sum(1 for pid, info in problem_difficulty.items() if info['n_wrong'] == n)\n",
    "    print(f'  {n} models wrong: {count} problems')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Claude 4 Sonnet's performance on hard subset\n",
    "if 'Claude 4 Sonnet' in all_direct:\n",
    "    sonnet4_hard = [all_direct['Claude 4 Sonnet'].get(pid, False) for pid in hard_problems]\n",
    "    sonnet4_hard_acc = sum(sonnet4_hard) / len(sonnet4_hard) if sonnet4_hard else 0\n",
    "    print(f'\\nClaude 4 Sonnet on hard subset:')\n",
    "    print(f'  Full dataset baseline: 92.5%')\n",
    "    print(f'  Hard subset baseline: {sonnet4_hard_acc:.1%}')\n",
    "    print(f'  → Baseline dropped by {92.5 - sonnet4_hard_acc*100:.1f}pp')\n",
    "\n",
    "# Save hard problem list\n",
    "save_json({\n",
    "    'threshold': HARD_THRESHOLD,\n",
    "    'n_problems': len(hard_problems),\n",
    "    'problem_indices': hard_problems\n",
    "}, f'{SAVE_DIR_EXP}/hard_problem_list.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "GLOBAL_SEED = 20251224\n",
    "I_FIXED = 10\n",
    "LAMBDA_VALUES = [0.0, 0.4, 0.8, 1.0]  # 4 points for efficiency\n",
    "\n",
    "# Models to test on hard subset\n",
    "MODELS = {\n",
    "    'Claude 4 Sonnet': {\n",
    "        'provider': 'anthropic',\n",
    "        'api_name': 'claude-sonnet-4-20250514',\n",
    "        'short': 'sonnet4'\n",
    "    },\n",
    "    'GPT-4o': {\n",
    "        'provider': 'openai',\n",
    "        'api_name': 'gpt-4o',\n",
    "        'short': 'gpt4o'\n",
    "    },\n",
    "    'Claude 3.5 Haiku': {\n",
    "        'provider': 'anthropic',\n",
    "        'api_name': 'claude-3-5-haiku-latest',\n",
    "        'short': 'haiku35'\n",
    "    }\n",
    "}\n",
    "\n",
    "print('='*60)\n",
    "print('E4: HARD SUBSET EXPERIMENT')\n",
    "print('='*60)\n",
    "print(f'Hard problems: {len(hard_problems)}')\n",
    "print(f'λ values: {LAMBDA_VALUES}')\n",
    "print(f'Models: {list(MODELS.keys())}')\n",
    "print(f'Total inferences per model: {len(hard_problems) * (len(LAMBDA_VALUES) + 1)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. API Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "\n",
    "print(\"OpenAI APIキーを入力してください：\")\n",
    "OPENAI_API_KEY = getpass.getpass(\"OpenAI API Key: \")\n",
    "\n",
    "print(\"\\nAnthropic APIキーを入力してください：\")\n",
    "ANTHROPIC_API_KEY = getpass.getpass(\"Anthropic API Key: \")\n",
    "\n",
    "openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "anthropic_client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n",
    "\n",
    "def call_api(prompt: str, model_config: dict, max_tokens: int = 256) -> str:\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            if model_config['provider'] == 'openai':\n",
    "                response = openai_client.chat.completions.create(\n",
    "                    model=model_config['api_name'],\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    max_tokens=max_tokens,\n",
    "                    temperature=0\n",
    "                )\n",
    "                return response.choices[0].message.content\n",
    "            else:\n",
    "                response = anthropic_client.messages.create(\n",
    "                    model=model_config['api_name'],\n",
    "                    max_tokens=max_tokens,\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "                )\n",
    "                return response.content[0].text\n",
    "        except Exception as e:\n",
    "            print(f'API error: {e}')\n",
    "            time.sleep(2 ** attempt)\n",
    "    return \"\"\n",
    "\n",
    "print('\\nTesting APIs...')\n",
    "for name, config in MODELS.items():\n",
    "    resp = call_api(\"2+2=?\", config)\n",
    "    print(f'{name}: OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load GSM8K Hard Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GSM8KProblem:\n",
    "    index: int\n",
    "    question: str\n",
    "    answer_text: str\n",
    "    final_answer: int\n",
    "\n",
    "def extract_final_answer(answer_text: str) -> int:\n",
    "    match = re.search(r'####\\s*([\\d,]+)', answer_text)\n",
    "    if match:\n",
    "        return int(match.group(1).replace(',', ''))\n",
    "    raise ValueError('Could not extract')\n",
    "\n",
    "dataset = load_dataset('gsm8k', 'main', split='test')\n",
    "\n",
    "# Load only hard problems\n",
    "problems = []\n",
    "for idx in hard_problems:\n",
    "    item = dataset[idx]\n",
    "    try:\n",
    "        final_ans = extract_final_answer(item['answer'])\n",
    "        problems.append(GSM8KProblem(\n",
    "            index=idx,\n",
    "            question=item['question'],\n",
    "            answer_text=item['answer'],\n",
    "            final_answer=final_ans\n",
    "        ))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(f'Loaded {len(problems)} hard problems')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. CoT Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_clean_cot(problem: GSM8KProblem, I: int) -> List[str]:\n",
    "    lines = problem.answer_text.split('\\n')\n",
    "    steps = [l.strip() for l in lines if l.strip() and not l.strip().startswith('####')]\n",
    "    while len(steps) < I:\n",
    "        steps.append(f\"Step {len(steps)+1}: Continue.\")\n",
    "    return steps[:I]\n",
    "\n",
    "def generate_corrupted_step(problem, step_idx, rng):\n",
    "    templates = [\n",
    "        f\"Step {step_idx+1}: Calculate {rng.randint(10,100)} × {rng.randint(2,10)} = {rng.randint(100,1000)}.\",\n",
    "        f\"Step {step_idx+1}: Add {rng.randint(50,200)} + {rng.randint(50,200)} = {rng.randint(100,500)}.\",\n",
    "    ]\n",
    "    return rng.choice(templates)\n",
    "\n",
    "def generate_mixed_cot(problem, I, lam, seed):\n",
    "    rng = random.Random(seed)\n",
    "    clean_steps = generate_clean_cot(problem, I)\n",
    "    n_corrupt = int(round(I * lam))\n",
    "    corrupt_indices = set(rng.sample(range(I), n_corrupt)) if n_corrupt > 0 else set()\n",
    "    \n",
    "    return [generate_corrupted_step(problem, i, rng) if i in corrupt_indices \n",
    "            else clean_steps[i] for i in range(I)]\n",
    "\n",
    "def derive_seed(global_seed, problem_id, lam):\n",
    "    key = f\"{global_seed}|{problem_id}|lam={lam}\"\n",
    "    return int(hashlib.sha256(key.encode()).hexdigest()[:8], 16)\n",
    "\n",
    "def create_direct_prompt(problem):\n",
    "    return f\"\"\"Solve this math problem. Give ONLY the final numerical answer in JSON format.\n",
    "\n",
    "Problem: {problem.question}\n",
    "\n",
    "Reply with ONLY: {{\"final\": <number>}}\"\"\"\n",
    "\n",
    "def create_cot_prompt(problem, cot_steps):\n",
    "    steps_text = '\\n'.join(cot_steps)\n",
    "    return f\"\"\"Here is a math problem with a provided reasoning trace.\n",
    "Follow the reasoning and give the final numerical answer in JSON format.\n",
    "\n",
    "Problem: {problem.question}\n",
    "\n",
    "Reasoning trace:\n",
    "{steps_text}\n",
    "\n",
    "Based on this reasoning, what is the final answer?\n",
    "Reply with ONLY: {{\"final\": <number>}}\"\"\"\n",
    "\n",
    "def parse_answer(response):\n",
    "    try:\n",
    "        match = re.search(r'\\{[^}]*\"final\"\\s*:\\s*(\\d+)[^}]*\\}', response)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        numbers = re.findall(r'\\b(\\d+)\\b', response)\n",
    "        if numbers:\n",
    "            return int(numbers[-1])\n",
    "    except:\n",
    "        pass\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select model\n",
    "#@title Select Model { run: \"auto\" }\n",
    "MODEL_CHOICE = \"Claude 4 Sonnet\" #@param [\"Claude 4 Sonnet\", \"GPT-4o\", \"Claude 3.5 Haiku\"]\n",
    "\n",
    "model_config = MODELS[MODEL_CHOICE]\n",
    "print(f'Running E4 for: {MODEL_CHOICE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*60)\n",
    "print(f'E4: HARD SUBSET EXPERIMENT - {MODEL_CHOICE}')\n",
    "print('='*60)\n",
    "\n",
    "all_results = []\n",
    "\n",
    "# Direct condition\n",
    "print('\\n--- DIRECT (Baseline on Hard) ---')\n",
    "for problem in tqdm(problems, desc='Direct'):\n",
    "    prompt = create_direct_prompt(problem)\n",
    "    response = call_api(prompt, model_config)\n",
    "    answer = parse_answer(response)\n",
    "    is_correct = (answer == problem.final_answer) if answer else False\n",
    "    \n",
    "    all_results.append({\n",
    "        'problem_index': problem.index,\n",
    "        'model': MODEL_CHOICE,\n",
    "        'condition': 'direct',\n",
    "        'lam': None,\n",
    "        'model_answer': answer,\n",
    "        'correct_answer': problem.final_answer,\n",
    "        'is_correct': is_correct,\n",
    "        'subset': 'hard'\n",
    "    })\n",
    "    time.sleep(0.3)\n",
    "\n",
    "# CoT conditions\n",
    "for lam in LAMBDA_VALUES:\n",
    "    print(f'\\n--- λ={lam} ---')\n",
    "    \n",
    "    for problem in tqdm(problems, desc=f'λ={lam}'):\n",
    "        seed = derive_seed(GLOBAL_SEED, problem.index, lam)\n",
    "        cot_steps = generate_mixed_cot(problem, I_FIXED, lam, seed)\n",
    "        prompt = create_cot_prompt(problem, cot_steps)\n",
    "        response = call_api(prompt, model_config)\n",
    "        \n",
    "        answer = parse_answer(response)\n",
    "        is_correct = (answer == problem.final_answer) if answer else False\n",
    "        \n",
    "        all_results.append({\n",
    "            'problem_index': problem.index,\n",
    "            'model': MODEL_CHOICE,\n",
    "            'condition': 'cot',\n",
    "            'lam': lam,\n",
    "            'model_answer': answer,\n",
    "            'correct_answer': problem.final_answer,\n",
    "            'is_correct': is_correct,\n",
    "            'subset': 'hard'\n",
    "        })\n",
    "        time.sleep(0.5)\n",
    "\n",
    "save_json(all_results, f\"{SAVE_DIR_EXP}/results/e4_results_{model_config['short']}.json\")\n",
    "print('\\n✓ E4 experiment complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.optimize import brentq\n",
    "\n",
    "df = pd.DataFrame(all_results)\n",
    "\n",
    "# Baseline on hard subset\n",
    "baseline_hard = df[df['condition'] == 'direct']['is_correct'].mean()\n",
    "\n",
    "# Accuracy by lambda\n",
    "cot_df = df[df['condition'] == 'cot']\n",
    "acc_by_lam = cot_df.groupby('lam')['is_correct'].mean().to_dict()\n",
    "\n",
    "print('='*60)\n",
    "print(f'E4 RESULTS: {MODEL_CHOICE} on Hard Subset')\n",
    "print('='*60)\n",
    "print(f'\\nBaseline (Hard): {baseline_hard:.1%}')\n",
    "print(f'Baseline (Full): 92.5%')  # From main experiment\n",
    "print(f'Drop: {92.5 - baseline_hard*100:.1f}pp')\n",
    "print('\\nAccuracy by λ:')\n",
    "for lam, acc in sorted(acc_by_lam.items()):\n",
    "    marker = '← BACKFIRE' if acc < baseline_hard else ''\n",
    "    print(f'  λ={lam}: {acc:.1%} {marker}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate λ* on hard subset\n",
    "def estimate_lambda_crit(lam_arr, acc_arr, baseline):\n",
    "    lam_arr = np.array(lam_arr)\n",
    "    acc_arr = np.array(acc_arr)\n",
    "    try:\n",
    "        f = interp1d(lam_arr, acc_arr - baseline, kind='linear', fill_value='extrapolate')\n",
    "        for i in range(len(lam_arr) - 1):\n",
    "            if (acc_arr[i] - baseline) * (acc_arr[i+1] - baseline) < 0:\n",
    "                return brentq(f, lam_arr[i], lam_arr[i+1])\n",
    "    except:\n",
    "        pass\n",
    "    return 1.0 if acc_arr[-1] > baseline else None\n",
    "\n",
    "lam_arr = sorted(acc_by_lam.keys())\n",
    "acc_arr = [acc_by_lam[l] for l in lam_arr]\n",
    "lambda_crit_hard = estimate_lambda_crit(lam_arr, acc_arr, baseline_hard)\n",
    "\n",
    "print(f'\\nλ* (Hard subset): {lambda_crit_hard:.3f}' if lambda_crit_hard else 'λ* not found')\n",
    "print(f'λ* (Full dataset): ~0.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Compare full vs hard\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Hard subset\n",
    "lams = sorted(acc_by_lam.keys())\n",
    "accs = [acc_by_lam[l] * 100 for l in lams]\n",
    "ax.plot(lams, accs, 'o-', color='#9467bd', linewidth=2.5, markersize=10, \n",
    "        label=f'{MODEL_CHOICE} (Hard subset)')\n",
    "ax.axhline(y=baseline_hard * 100, color='#9467bd', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Reference: Full dataset (hardcoded from main experiment)\n",
    "if MODEL_CHOICE == 'Claude 4 Sonnet':\n",
    "    full_acc = {0.0: 98.0, 0.2: 97.0, 0.4: 92.5, 0.6: 90.5, 0.8: 91.5, 1.0: 88.4}\n",
    "    full_baseline = 92.5\n",
    "    ax.plot(list(full_acc.keys()), list(full_acc.values()), 's--', color='gray', \n",
    "            linewidth=1.5, markersize=8, alpha=0.5, label='Full dataset (reference)')\n",
    "    ax.axhline(y=full_baseline, color='gray', linestyle=':', alpha=0.5)\n",
    "\n",
    "ax.set_xlabel('Corruption Rate (λ)', fontsize=13)\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=13)\n",
    "ax.set_title(f'E4: Hard Subset Analysis - {MODEL_CHOICE}\\n(n={len(problems)} hard problems)', fontsize=14)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim(-0.05, 1.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{SAVE_DIR_EXP}/e4_hard_subset_{model_config['short']}.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "summary = {\n",
    "    'experiment': 'E4_Hard_Subset',\n",
    "    'model': MODEL_CHOICE,\n",
    "    'date': EXPERIMENT_DATE,\n",
    "    'n_hard_problems': len(problems),\n",
    "    'hard_threshold': HARD_THRESHOLD,\n",
    "    'baseline_hard': baseline_hard,\n",
    "    'baseline_full': 0.925,  # Reference\n",
    "    'accuracy_by_lambda': acc_by_lam,\n",
    "    'lambda_crit_hard': lambda_crit_hard,\n",
    "    'lambda_crit_full': 0.4,  # Reference\n",
    "}\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('INTERPRETATION')\n",
    "print('='*60)\n",
    "\n",
    "if lambda_crit_hard and lambda_crit_hard < 0.8:\n",
    "    print(f'\\n→ λ* on hard subset ({lambda_crit_hard:.3f}) is similar to full dataset (~0.4)')\n",
    "    print('→ Backfire effect is consistent, not due to ceiling effect')\n",
    "elif lambda_crit_hard and lambda_crit_hard >= 0.8:\n",
    "    print(f'\\n→ λ* on hard subset ({lambda_crit_hard:.3f}) is HIGHER than full dataset')\n",
    "    print('→ Ceiling effect may have contributed to early backfire on full dataset')\n",
    "else:\n",
    "    print('\\n→ No clear backfire on hard subset')\n",
    "\n",
    "save_json(summary, f\"{SAVE_DIR_EXP}/results/e4_summary_{model_config['short']}.json\")"
   ]
  }
 ]
}
