{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CoT Backfire Analysis\n",
    "\n",
    "This notebook reproduces the main analyses from:\n",
    "\n",
    "> HIDEKI. \"When Reasoning Traces Backfire: Identifying the Backfire Boundary of Provided Chain-of-Thought Reasoning.\" (2025)\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configuration\n",
    "GLOBAL_SEED = 20251224\n",
    "np.random.seed(GLOBAL_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Claude results\n",
    "with open('data/claude/results_full_v3.json', 'r') as f:\n",
    "    claude_full = json.load(f)\n",
    "\n",
    "with open('data/claude/direct_results_v3.json', 'r') as f:\n",
    "    claude_direct = json.load(f)\n",
    "\n",
    "# Load GPT-4o results\n",
    "with open('data/gpt4o/cot_results_gpt.json', 'r') as f:\n",
    "    gpt_cot = json.load(f)\n",
    "\n",
    "with open('data/gpt4o/direct_results_gpt.json', 'r') as f:\n",
    "    gpt_direct = json.load(f)\n",
    "\n",
    "# Convert to DataFrames\n",
    "claude_df = pd.DataFrame(claude_full)\n",
    "claude_direct_df = pd.DataFrame(claude_direct)\n",
    "gpt_df = pd.DataFrame(gpt_cot)\n",
    "gpt_direct_df = pd.DataFrame(gpt_direct)\n",
    "\n",
    "print(f'Claude CoT: {len(claude_df)} records')\n",
    "print(f'Claude Direct: {len(claude_direct_df)} records')\n",
    "print(f'GPT-4o CoT: {len(gpt_df)} records')\n",
    "print(f'GPT-4o Direct: {len(gpt_direct_df)} records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Analysis: Accuracy by Corruption Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude accuracy by lambda\n",
    "claude_acc = claude_df.groupby('lam')['is_correct'].mean()\n",
    "claude_direct_acc = claude_direct_df['is_correct'].mean()\n",
    "\n",
    "print('Claude Sonnet Results:')\n",
    "print(f'Direct (no CoT): {claude_direct_acc:.1%}')\n",
    "print('\\nCoT by λ:')\n",
    "for lam, acc in claude_acc.items():\n",
    "    print(f'  λ={lam}: {acc:.1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4o accuracy by lambda\n",
    "gpt_acc = gpt_df.groupby('lam')['is_correct'].mean()\n",
    "gpt_direct_acc = gpt_direct_df['is_correct'].mean()\n",
    "\n",
    "print('GPT-4o Results:')\n",
    "print(f'Direct (no CoT): {gpt_direct_acc:.1%}')\n",
    "print('\\nCoT by λ:')\n",
    "for lam, acc in gpt_acc.items():\n",
    "    print(f'  λ={lam}: {acc:.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backfire Boundary Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "from scipy.optimize import brentq\n",
    "\n",
    "def estimate_lambda_crit(lam_arr, acc_arr, baseline):\n",
    "    \"\"\"Estimate λ* where accuracy crosses baseline.\"\"\"\n",
    "    f = interp1d(lam_arr, acc_arr - baseline, kind='linear', fill_value='extrapolate')\n",
    "    try:\n",
    "        for i in range(len(lam_arr) - 1):\n",
    "            if (acc_arr[i] - baseline) * (acc_arr[i+1] - baseline) < 0:\n",
    "                return brentq(f, lam_arr[i], lam_arr[i+1])\n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "# Claude boundary\n",
    "claude_lams = np.array(list(claude_acc.index))\n",
    "claude_accs = np.array(list(claude_acc.values))\n",
    "claude_lambda_crit = estimate_lambda_crit(claude_lams, claude_accs, claude_direct_acc)\n",
    "\n",
    "# GPT-4o boundary\n",
    "gpt_lams = np.array(list(gpt_acc.index))\n",
    "gpt_accs = np.array(list(gpt_acc.values))\n",
    "gpt_lambda_crit = estimate_lambda_crit(gpt_lams, gpt_accs, gpt_direct_acc)\n",
    "\n",
    "print('Backfire Boundary Estimates:')\n",
    "print(f'  Claude: λ* = {claude_lambda_crit:.3f}, A* = {1-claude_lambda_crit:.3f}')\n",
    "print(f'  GPT-4o: λ* = {gpt_lambda_crit:.3f}, A* = {1-gpt_lambda_crit:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compliance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compliance_analysis(cot_df, direct_df, model_name):\n",
    "    \"\"\"Analyze compliance-induced failures.\"\"\"\n",
    "    direct_correct = set(direct_df[direct_df['is_correct']]['problem_index'])\n",
    "    \n",
    "    # Clean vs Corrupted (λ=0.8)\n",
    "    clean = cot_df[cot_df['lam'] == 0.0].set_index('problem_index')['is_correct']\n",
    "    corrupted = cot_df[cot_df['lam'] == 0.8].set_index('problem_index')['is_correct']\n",
    "    \n",
    "    common = clean.index.intersection(corrupted.index)\n",
    "    \n",
    "    clean_to_wrong = ((clean.loc[common] == True) & (corrupted.loc[common] == False)).sum()\n",
    "    wrong_to_clean = ((clean.loc[common] == False) & (corrupted.loc[common] == True)).sum()\n",
    "    \n",
    "    # McNemar's test\n",
    "    n = clean_to_wrong + wrong_to_clean\n",
    "    mcnemar_stat = (abs(clean_to_wrong - wrong_to_clean) - 1)**2 / n if n > 0 else 0\n",
    "    p_value = stats.chi2.sf(mcnemar_stat, 1)\n",
    "    \n",
    "    flip_rate = clean_to_wrong / len(common)\n",
    "    \n",
    "    print(f'{model_name}:')\n",
    "    print(f'  Flip rate (Clean→Wrong): {flip_rate:.1%}')\n",
    "    print(f'  McNemar χ² = {mcnemar_stat:.1f}, p = {p_value:.2e}')\n",
    "    print(f'  Net loss: {clean_to_wrong - wrong_to_clean}')\n",
    "    \n",
    "compliance_analysis(claude_df, claude_direct_df, 'Claude Sonnet')\n",
    "print()\n",
    "compliance_analysis(gpt_df, gpt_direct_df, 'GPT-4o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Claude\n",
    "ax.plot(claude_lams, claude_accs * 100, 'o-', color='#2166ac', \n",
    "        linewidth=2.5, markersize=10, label='Claude Sonnet')\n",
    "ax.axhline(y=claude_direct_acc * 100, color='#2166ac', \n",
    "           linestyle='--', linewidth=1.5, alpha=0.7)\n",
    "\n",
    "# GPT-4o\n",
    "ax.plot(gpt_lams, gpt_accs * 100, 's-', color='#d62728',\n",
    "        linewidth=2.5, markersize=10, label='GPT-4o')\n",
    "ax.axhline(y=gpt_direct_acc * 100, color='#d62728',\n",
    "           linestyle='--', linewidth=1.5, alpha=0.7)\n",
    "\n",
    "# Boundaries\n",
    "if claude_lambda_crit:\n",
    "    ax.axvline(x=claude_lambda_crit, color='#2166ac', linestyle=':', alpha=0.7)\n",
    "if gpt_lambda_crit:\n",
    "    ax.axvline(x=gpt_lambda_crit, color='#d62728', linestyle=':', alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Corruption Rate (λ)', fontsize=13)\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=13)\n",
    "ax.set_title('Backfire Boundary: Claude vs GPT-4o', fontsize=14)\n",
    "ax.legend(loc='lower left')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim(-0.05, 1.05)\n",
    "ax.set_ylim(25, 105)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pd.DataFrame({\n",
    "    'Metric': ['Direct Accuracy', 'Clean CoT (λ=0)', 'λ*', 'A*'],\n",
    "    'Claude Sonnet': [\n",
    "        f'{claude_direct_acc:.1%}',\n",
    "        f'{claude_accs[0]:.1%}',\n",
    "        f'{claude_lambda_crit:.3f}',\n",
    "        f'{1-claude_lambda_crit:.3f}'\n",
    "    ],\n",
    "    'GPT-4o': [\n",
    "        f'{gpt_direct_acc:.1%}',\n",
    "        f'{gpt_accs[0]:.1%}',\n",
    "        f'{gpt_lambda_crit:.3f}',\n",
    "        f'{1-gpt_lambda_crit:.3f}'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(summary.to_string(index=False))"
   ]
  }
 ]
}
