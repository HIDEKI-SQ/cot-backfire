{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2: Direct Baseline Experiment (Claude + GPT-4o)\n",
    "\n",
    "**Paper**: A2 (Cue-Dominant Extraction)\n",
    "\n",
    "**Purpose**: Establish direct baseline accuracy (no trace provided) for both models on the A2 problem set.\n",
    "\n",
    "**Design**:\n",
    "- Models: Claude Sonnet 4, GPT-4o\n",
    "- N = 199 problems\n",
    "- Condition: Direct (no reasoning trace provided)\n",
    "\n",
    "**Rationale**:\n",
    "- Establish baseline capability for each model\n",
    "- Enable comparison: Direct vs Trace-provided conditions\n",
    "- Answer: \"Is the model extracting from trace or solving independently?\"\n",
    "\n",
    "**Expected inference count**: 398 (199 × 2 models)\n",
    "\n",
    "**Date**: 2026-01-03\n",
    "**GLOBAL_SEED**: 20251224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Google Drive Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "EXPERIMENT_NAME = 'A2_Direct_baseline'\n",
    "EXPERIMENT_DATE = datetime.now().strftime('%Y%m%d')\n",
    "\n",
    "BASE_DIR = '/content/drive/MyDrive/CoT_Experiment'\n",
    "V3_DATA_DIR = f'{BASE_DIR}/full_experiment_v3_20251224'\n",
    "\n",
    "SAVE_DIR = f'{BASE_DIR}/{EXPERIMENT_NAME}_{EXPERIMENT_DATE}'\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(f'{SAVE_DIR}/results', exist_ok=True)\n",
    "\n",
    "print(f'Experiment: {EXPERIMENT_NAME}')\n",
    "print(f'V3 data directory: {V3_DATA_DIR}')\n",
    "print(f'Save directory: {SAVE_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets openai anthropic matplotlib pandas tqdm scipy -q\n",
    "print('Dependencies installed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import random\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from typing import List, Dict, Tuple, Optional, Any\n",
    "from dataclasses import dataclass, asdict, field\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# =============================================================================\n",
    "# Global Configuration\n",
    "# =============================================================================\n",
    "GLOBAL_SEED = 20251224\n",
    "\n",
    "# API settings\n",
    "API_MAX_TOKENS_ANSWER = 1024  # More tokens for direct solving\n",
    "API_RETRY_DELAY = 1.0\n",
    "API_RATE_LIMIT_DELAY = 0.5\n",
    "\n",
    "# Model identifiers\n",
    "CLAUDE_MODEL = 'claude-sonnet-4-20250514'\n",
    "GPT_MODEL = 'gpt-4o'\n",
    "\n",
    "print('='*70)\n",
    "print('A2: DIRECT BASELINE EXPERIMENT')\n",
    "print('='*70)\n",
    "print(f'  Models: {CLAUDE_MODEL}, {GPT_MODEL}')\n",
    "print(f'  GLOBAL_SEED: {GLOBAL_SEED}')\n",
    "print(f'  Condition: Direct (no trace provided)')\n",
    "print('='*70)\n",
    "print('\\nPurpose:')\n",
    "print('  Establish each model\\'s baseline capability on GSM8K')\n",
    "print('  for comparison with trace-provided conditions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GSM8KProblem:\n",
    "    index: int\n",
    "    question: str\n",
    "    answer_text: str\n",
    "    final_answer: int\n",
    "\n",
    "@dataclass\n",
    "class DirectResult:\n",
    "    problem_index: int\n",
    "    model: str\n",
    "    model_answer: Optional[int]\n",
    "    correct_answer: int\n",
    "    is_correct: bool\n",
    "    raw_output: str\n",
    "    timestamp: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(data: Any, filepath: str):\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def load_json(filepath: str) -> Any:\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problems_path = f'{V3_DATA_DIR}/problems_v3.json'\n",
    "problems_data = load_json(problems_path)\n",
    "problems = [GSM8KProblem(**p) for p in problems_data]\n",
    "print(f'Loaded {len(problems)} problems')\n",
    "\n",
    "prob_map = {p.index: p for p in problems}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. API Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "print('Enter API Keys for both models:')\n",
    "ANTHROPIC_API_KEY = getpass('Enter Anthropic API Key: ')\n",
    "OPENAI_API_KEY = getpass('Enter OpenAI API Key: ')\n",
    "print('API Keys set.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "from openai import OpenAI\n",
    "\n",
    "claude_client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n",
    "gpt_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "print(f'Claude client initialized: {CLAUDE_MODEL}')\n",
    "print(f'GPT client initialized: {GPT_MODEL}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude(system_prompt: str, user_prompt: str, max_tokens: int = 1024, retries: int = 3) -> str:\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            message = claude_client.messages.create(\n",
    "                model=CLAUDE_MODEL,\n",
    "                max_tokens=max_tokens,\n",
    "                messages=[{\"role\": \"user\", \"content\": user_prompt}],\n",
    "                system=system_prompt,\n",
    "                temperature=0\n",
    "            )\n",
    "            time.sleep(API_RATE_LIMIT_DELAY)\n",
    "            return message.content[0].text\n",
    "        except Exception as e:\n",
    "            print(f'Claude API error (attempt {attempt+1}): {e}')\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(API_RETRY_DELAY * (attempt + 1))\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "def call_gpt4o(system_prompt: str, user_prompt: str, max_tokens: int = 1024, retries: int = 3) -> str:\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = gpt_client.chat.completions.create(\n",
    "                model=GPT_MODEL,\n",
    "                max_tokens=max_tokens,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt}\n",
    "                ],\n",
    "                temperature=0\n",
    "            )\n",
    "            time.sleep(API_RATE_LIMIT_DELAY)\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f'GPT API error (attempt {attempt+1}): {e}')\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(API_RETRY_DELAY * (attempt + 1))\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "# Test both APIs\n",
    "print('Testing APIs...')\n",
    "test_claude = call_claude(\"You output ONLY JSON.\", 'Respond with exactly: {\"test\": \"ok\"}', max_tokens=50)\n",
    "print(f'Claude test: {test_claude}')\n",
    "\n",
    "test_gpt = call_gpt4o(\"You output ONLY JSON.\", 'Respond with exactly: {\"test\": \"ok\"}', max_tokens=50)\n",
    "print(f'GPT-4o test: {test_gpt}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Direct Solving Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECT_SYSTEM_PROMPT = \"\"\"You are a math problem solver. Solve the problem step by step, then provide the final answer.\n",
    "\n",
    "After your reasoning, you MUST output the final answer in this exact format:\n",
    "{\"final\": <number>}\n",
    "\n",
    "Replace <number> with the integer answer.\n",
    "\"\"\"\n",
    "\n",
    "def create_direct_prompt(problem: GSM8KProblem) -> Tuple[str, str]:\n",
    "    user = f\"\"\"Solve this math problem. Show your work step by step.\n",
    "\n",
    "Problem: {problem.question}\n",
    "\n",
    "After solving, output the final answer as: {{\"final\": <number>}}\"\"\"\n",
    "    return DIRECT_SYSTEM_PROMPT, user\n",
    "\n",
    "def parse_model_answer(response: str) -> Optional[int]:\n",
    "    if response is None:\n",
    "        return None\n",
    "    # Try JSON format\n",
    "    match = re.search(r'\\{\\s*\"final\"\\s*:\\s*(-?\\d+(?:\\.\\d+)?)\\s*\\}', response)\n",
    "    if match:\n",
    "        return int(round(float(match.group(1))))\n",
    "    match = re.search(r\"\\{\\s*[\\\"']final[\\\"']\\s*:\\s*(-?\\d+(?:\\.\\d+)?)\\s*\\}\", response)\n",
    "    if match:\n",
    "        return int(round(float(match.group(1))))\n",
    "    match = re.search(r'\"final\"\\s*:\\s*(-?\\d+(?:\\.\\d+)?)', response)\n",
    "    if match:\n",
    "        return int(round(float(match.group(1))))\n",
    "    # Try #### format (GSM8K style)\n",
    "    match = re.search(r'####\\s*(-?\\d+)', response)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    # Last number in response\n",
    "    matches = re.findall(r'(?:^|\\s)(-?\\d+(?:\\.\\d+)?)(?:\\s|$|\\.|,)', response)\n",
    "    if matches:\n",
    "        return int(round(float(matches[-1])))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_direct_experiment(problem: GSM8KProblem, model: str) -> DirectResult:\n",
    "    sys_prompt, usr_prompt = create_direct_prompt(problem)\n",
    "    \n",
    "    if model == CLAUDE_MODEL:\n",
    "        response = call_claude(sys_prompt, usr_prompt, max_tokens=API_MAX_TOKENS_ANSWER)\n",
    "    else:\n",
    "        response = call_gpt4o(sys_prompt, usr_prompt, max_tokens=API_MAX_TOKENS_ANSWER)\n",
    "    \n",
    "    model_answer = parse_model_answer(response)\n",
    "    is_correct = (model_answer == problem.final_answer) if model_answer is not None else False\n",
    "    \n",
    "    return DirectResult(\n",
    "        problem_index=problem.index,\n",
    "        model=model,\n",
    "        model_answer=model_answer,\n",
    "        correct_answer=problem.final_answer,\n",
    "        is_correct=is_correct,\n",
    "        raw_output=response,\n",
    "        timestamp=datetime.now().isoformat()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*70)\n",
    "print('A2: DIRECT BASELINE EXPERIMENT')\n",
    "print('='*70)\n",
    "print(f'Models: Claude Sonnet 4, GPT-4o')\n",
    "print(f'Condition: Direct (no trace)')\n",
    "print(f'Expected inferences: {len(problems) * 2}')\n",
    "print('='*70)\n",
    "\n",
    "results_claude = []\n",
    "results_gpt = []\n",
    "\n",
    "for prob in tqdm(problems, desc='Direct Baseline (Claude + GPT-4o)'):\n",
    "    # Claude\n",
    "    result_claude = run_direct_experiment(prob, CLAUDE_MODEL)\n",
    "    results_claude.append(result_claude)\n",
    "    \n",
    "    # GPT-4o\n",
    "    result_gpt = run_direct_experiment(prob, GPT_MODEL)\n",
    "    results_gpt.append(result_gpt)\n",
    "\n",
    "print(f'\\nCompleted: {len(results_claude) + len(results_gpt)} experiments')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json([asdict(r) for r in results_claude], f'{SAVE_DIR}/results/A2_Direct_claude_results.json')\n",
    "print(f'Claude results saved: {SAVE_DIR}/results/A2_Direct_claude_results.json')\n",
    "\n",
    "save_json([asdict(r) for r in results_gpt], f'{SAVE_DIR}/results/A2_Direct_gpt4o_results.json')\n",
    "print(f'GPT-4o results saved: {SAVE_DIR}/results/A2_Direct_gpt4o_results.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_claude = pd.DataFrame([asdict(r) for r in results_claude])\n",
    "df_gpt = pd.DataFrame([asdict(r) for r in results_gpt])\n",
    "\n",
    "claude_acc = df_claude['is_correct'].mean()\n",
    "gpt_acc = df_gpt['is_correct'].mean()\n",
    "\n",
    "print('='*70)\n",
    "print('A2 DIRECT BASELINE RESULTS')\n",
    "print('='*70)\n",
    "print(f'Claude Sonnet 4: {claude_acc:.1%} ({df_claude[\"is_correct\"].sum()}/{len(df_claude)})')\n",
    "print(f'GPT-4o:          {gpt_acc:.1%} ({df_gpt[\"is_correct\"].sum()}/{len(df_gpt)})')\n",
    "print(f'Difference:      {(claude_acc - gpt_acc)*100:+.1f} pp')\n",
    "print('='*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wilson confidence intervals\n",
    "def wilson_ci(successes, n, confidence=0.95):\n",
    "    from scipy.stats import norm\n",
    "    z = norm.ppf(1 - (1 - confidence) / 2)\n",
    "    p = successes / n\n",
    "    denominator = 1 + z**2 / n\n",
    "    center = (p + z**2 / (2*n)) / denominator\n",
    "    margin = z * np.sqrt((p * (1-p) + z**2 / (4*n)) / n) / denominator\n",
    "    return (center - margin) * 100, (center + margin) * 100\n",
    "\n",
    "claude_correct = df_claude['is_correct'].sum()\n",
    "gpt_correct = df_gpt['is_correct'].sum()\n",
    "N = len(problems)\n",
    "\n",
    "claude_ci = wilson_ci(claude_correct, N)\n",
    "gpt_ci = wilson_ci(gpt_correct, N)\n",
    "\n",
    "print('\\n95% Confidence Intervals:')\n",
    "print(f'Claude: {claude_acc*100:.1f}% [{claude_ci[0]:.1f}%, {claude_ci[1]:.1f}%]')\n",
    "print(f'GPT-4o: {gpt_acc*100:.1f}% [{gpt_ci[0]:.1f}%, {gpt_ci[1]:.1f}%]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem-level comparison\n",
    "both_correct = sum(1 for i in range(N) if results_claude[i].is_correct and results_gpt[i].is_correct)\n",
    "claude_only = sum(1 for i in range(N) if results_claude[i].is_correct and not results_gpt[i].is_correct)\n",
    "gpt_only = sum(1 for i in range(N) if not results_claude[i].is_correct and results_gpt[i].is_correct)\n",
    "both_wrong = sum(1 for i in range(N) if not results_claude[i].is_correct and not results_gpt[i].is_correct)\n",
    "\n",
    "print('\\nProblem-level Agreement:')\n",
    "print('                  GPT-4o')\n",
    "print('                  Correct  Wrong')\n",
    "print(f'Claude  Correct    {both_correct:3d}     {claude_only:3d}')\n",
    "print(f'        Wrong      {gpt_only:3d}     {both_wrong:3d}')\n",
    "print(f'\\nAgreement rate: {(both_correct + both_wrong) / N * 100:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison with A2 trace-provided conditions\n",
    "print('\\n' + '='*70)\n",
    "print('COMPARISON: DIRECT vs TRACE-PROVIDED')\n",
    "print('='*70)\n",
    "print('\\nClaude Sonnet 4:')\n",
    "print(f'  Direct (this exp):       {claude_acc*100:.1f}%')\n",
    "print(f'  E4\\' (cue protected):    92.0%  (from A2 main experiments)')\n",
    "print(f'  E2-Late (cue corrupted): 60.3%  (from A2 main experiments)')\n",
    "print('\\nGPT-4o:')\n",
    "print(f'  Direct (this exp):       {gpt_acc*100:.1f}%')\n",
    "print(f'  E4\\' (cue protected):    [RUN E4\\' NOTEBOOK]')\n",
    "print(f'  E2-Late (cue corrupted): [RUN E4\\' NOTEBOOK]')\n",
    "print('='*70)\n",
    "\n",
    "print('\\nINTERPRETATION:')\n",
    "print('  If Direct ≈ E4\\' (cue protected): Model may be solving independently')\n",
    "print('  If Direct > E2-Late: Corrupted traces are actively harmful')\n",
    "print('  If E4\\' > Direct: Clean traces provide useful information')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {\n",
    "    'experiment': 'A2_Direct_baseline',\n",
    "    'date': EXPERIMENT_DATE,\n",
    "    'n_problems': N,\n",
    "    'models': [CLAUDE_MODEL, GPT_MODEL],\n",
    "    'results': {\n",
    "        'claude': {\n",
    "            'model': CLAUDE_MODEL,\n",
    "            'accuracy': claude_acc,\n",
    "            'correct': int(claude_correct),\n",
    "            'ci_95': claude_ci\n",
    "        },\n",
    "        'gpt4o': {\n",
    "            'model': GPT_MODEL,\n",
    "            'accuracy': gpt_acc,\n",
    "            'correct': int(gpt_correct),\n",
    "            'ci_95': gpt_ci\n",
    "        }\n",
    "    },\n",
    "    'agreement': {\n",
    "        'both_correct': both_correct,\n",
    "        'claude_only': claude_only,\n",
    "        'gpt_only': gpt_only,\n",
    "        'both_wrong': both_wrong,\n",
    "        'agreement_rate': (both_correct + both_wrong) / N\n",
    "    }\n",
    "}\n",
    "\n",
    "save_json(summary, f'{SAVE_DIR}/results/A2_Direct_summary.json')\n",
    "\n",
    "print('='*70)\n",
    "print('A2 DIRECT BASELINE EXPERIMENT COMPLETE')\n",
    "print('='*70)\n",
    "print(f'Date: {EXPERIMENT_DATE}')\n",
    "print(f'Total experiments: {len(results_claude) + len(results_gpt)}')\n",
    "print(f'\\nResults:')\n",
    "print(f'  Claude Sonnet 4: {claude_acc:.1%}')\n",
    "print(f'  GPT-4o:          {gpt_acc:.1%}')\n",
    "print(f'\\nFiles saved to: {SAVE_DIR}')\n",
    "print('='*70)"
   ]
  }
 ]
}
